{"id": "2507.11582", "pdf": "https://arxiv.org/pdf/2507.11582", "abs": "https://arxiv.org/abs/2507.11582", "authors": ["Kazuyoshi Otsuka"], "title": "Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance", "categories": ["cs.CL"], "comment": "38 pages. Manuscript submitted for review to the Journal of\n  Computational Literary Studies (JCLS)", "summary": "This study positions large language models (LLMs) as \"subjective literary\ncritics\" to explore aesthetic preferences and evaluation patterns in literary\nassessment. Ten Japanese science fiction short stories were translated into\nEnglish and evaluated by six state-of-the-art LLMs across seven independent\nsessions. Principal component analysis and clustering techniques revealed\nsignificant variations in evaluation consistency ({\\alpha} ranging from 1.00 to\n0.35) and five distinct evaluation patterns. Additionally, evaluation variance\nacross stories differed by up to 4.5-fold, with TF-IDF analysis confirming\ndistinctive evaluation vocabularies for each model. Our seven-session\nwithin-day protocol using an original Science Fiction corpus strategically\nminimizes external biases, allowing us to observe implicit value systems shaped\nby RLHF and their influence on literary judgment. These findings suggest that\nLLMs may possess individual evaluation characteristics similar to human\ncritical schools, rather than functioning as neutral benchmarkers.", "AI": {"tldr": "\u7814\u7a76\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u201c\u4e3b\u89c2\u6587\u5b66\u8bc4\u8bba\u5bb6\u201d\uff0c\u63a2\u7d22\u5176\u5728\u6587\u5b66\u8bc4\u4f30\u4e2d\u7684\u5ba1\u7f8e\u504f\u597d\u548c\u8bc4\u4ef7\u6a21\u5f0f\uff0c\u53d1\u73b0LLMs\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u6279\u8bc4\u6d41\u6d3e\u7684\u4e2a\u4f53\u8bc4\u4ef7\u7279\u5f81\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u6587\u5b66\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u5176\u662f\u5426\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4e3b\u89c2\u8bc4\u4ef7\u7279\u5f81\uff0c\u800c\u975e\u4e2d\u6027\u57fa\u51c6\u3002", "method": "\u5c06\u5341\u7bc7\u65e5\u672c\u79d1\u5e7b\u77ed\u7bc7\u5c0f\u8bf4\u7ffb\u8bd1\u6210\u82f1\u6587\uff0c\u7531\u516d\u79cd\u5148\u8fdbLLMs\u5728\u4e03\u6b21\u72ec\u7acb\u4f1a\u8bdd\u4e2d\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e3b\u6210\u5206\u5206\u6790\u548c\u805a\u7c7b\u6280\u672f\u5206\u6790\u6570\u636e\u3002", "result": "\u53d1\u73b0\u8bc4\u4ef7\u4e00\u81f4\u6027\u5dee\u5f02\u663e\u8457\uff08\u03b1\u8303\u56f41.00\u81f30.35\uff09\uff0c\u4e94\u79cd\u8bc4\u4ef7\u6a21\u5f0f\uff0c\u4e0d\u540c\u6a21\u578b\u8bc4\u4ef7\u8bcd\u6c47\u72ec\u7279\u3002", "conclusion": "LLMs\u53ef\u80fd\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u6279\u8bc4\u6d41\u6d3e\u7684\u4e2a\u4f53\u8bc4\u4ef7\u7279\u5f81\uff0c\u800c\u975e\u4e2d\u6027\u57fa\u51c6\u3002"}}
{"id": "2507.11625", "pdf": "https://arxiv.org/pdf/2507.11625", "abs": "https://arxiv.org/abs/2507.11625", "authors": ["Varun Srivastava", "Fan Lei", "Srija Mukhopadhyay", "Vivek Gupta", "Ross Maciejewski"], "title": "MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Published as a conference paper at COLM 2025", "summary": "Recent advancements in multimodal large language models (MLLMs) have driven\nresearchers to explore how well these models read data visualizations, e.g.,\nbar charts, scatter plots. More recently, attention has shifted to visual\nquestion answering with maps (Map-VQA). However, Map-VQA research has primarily\nfocused on choropleth maps, which cover only a limited range of thematic\ncategories and visual analytical tasks. To address these gaps, we introduce\nMapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three\nmap types: choropleth maps, cartograms, and proportional symbol maps spanning\ntopics from six distinct themes (e.g., housing, crime). We evaluate multiple\nMLLMs using six visual analytical tasks, comparing their performance against\none another and a human baseline. An additional experiment examining the impact\nof map design changes (e.g., altered color schemes, modified legend designs,\nand removal of map elements) provides insights into the robustness and\nsensitivity of MLLMs, their reliance on internal geographic knowledge, and\npotential avenues for improving Map-VQA performance.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MapIQ\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5730\u56fe\u89c6\u89c9\u95ee\u7b54\uff08Map-VQA\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u6db5\u76d6\u591a\u79cd\u5730\u56fe\u7c7b\u578b\u548c\u4e3b\u9898\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u5bf9\u5730\u56fe\u8bbe\u8ba1\u53d8\u5316\u7684\u654f\u611f\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524dMap-VQA\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u7b49\u503c\u533a\u57df\u56fe\uff08choropleth maps\uff09\uff0c\u8986\u76d6\u7684\u4e3b\u9898\u548c\u89c6\u89c9\u5206\u6790\u4efb\u52a1\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86MapIQ\u6570\u636e\u96c6\uff0c\u5305\u542b14,706\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u8986\u76d6\u4e09\u79cd\u5730\u56fe\u7c7b\u578b\u548c\u516d\u4e2a\u4e3b\u9898\uff0c\u8bc4\u4f30\u4e86\u591a\u79cdMLLMs\u5728\u516d\u9879\u89c6\u89c9\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u5730\u56fe\u8bbe\u8ba1\u53d8\u5316\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u6bd4\u8f83\u4e86MLLMs\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u5730\u56fe\u8bbe\u8ba1\u53d8\u5316\u7684\u654f\u611f\u6027\u548c\u4f9d\u8d56\u5185\u90e8\u5730\u7406\u77e5\u8bc6\u7684\u7a0b\u5ea6\u3002", "conclusion": "MapIQ\u4e3aMap-VQA\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86MLLMs\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2507.11634", "pdf": "https://arxiv.org/pdf/2507.11634", "abs": "https://arxiv.org/abs/2507.11634", "authors": ["Farideh Majidi", "Ziaeddin Beheshtifard"], "title": "Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": "Proceedings of the First National Conference on Artificial\n  Intelligence and Emerging Research: Convergence of Humans and Intelligent\n  Systems", "summary": "This research examines cross-lingual sentiment analysis using few-shot\nlearning and incremental learning methods in Persian. The main objective is to\ndevelop a model capable of performing sentiment analysis in Persian using\nlimited data, while getting prior knowledge from high-resource languages. To\nachieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and\nDistilBERT) were employed, which were fine-tuned using few-shot and incremental\nlearning approaches on small samples of Persian data from diverse sources,\nincluding X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled\nthe models to learn from a broad range of contexts. Experimental results show\nthat the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%\naccuracy on Persian sentiment analysis. These findings highlight the\neffectiveness of combining few-shot learning and incremental learning with\nmultilingual pre-trained models.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u6ce2\u65af\u8bed\u4e2d\u4f7f\u7528\u5c11\u6837\u672c\u5b66\u4e60\u548c\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u8de8\u8bed\u8a00\u60c5\u611f\u5206\u6790\uff0c\u76ee\u6807\u662f\u5229\u7528\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u77e5\u8bc6\u5728\u6709\u9650\u6570\u636e\u4e0b\u5b9e\u73b0\u9ad8\u6548\u5206\u6790\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u6ce2\u65af\u8bed\u4e2d\u5229\u7528\u6709\u9650\u6570\u636e\u8fdb\u884c\u60c5\u611f\u5206\u6790\u7684\u6a21\u578b\uff0c\u540c\u65f6\u4ece\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e2d\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\uff08XLM-RoBERTa\u3001mDeBERTa\u548cDistilBERT\uff09\uff0c\u5e76\u901a\u8fc7\u5c11\u6837\u672c\u548c\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u5728\u6ce2\u65af\u8bed\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "mDeBERTa\u548cXLM-RoBERTa\u8868\u73b0\u51fa\u8272\uff0c\u5728\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u6790\u4e2d\u8fbe\u523096%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\u3001\u589e\u91cf\u5b66\u4e60\u548c\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u6709\u6548\u7684\u8de8\u8bed\u8a00\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2507.11661", "pdf": "https://arxiv.org/pdf/2507.11661", "abs": "https://arxiv.org/abs/2507.11661", "authors": ["Guimin Hu", "Yi Xin", "Lijie Hu", "Zhihong Zhu", "Hasti Seifi"], "title": "Partitioner Guided Modal Learning Framework", "categories": ["cs.CL", "cs.AI"], "comment": "acm multimedia 2025", "summary": "Multimodal learning benefits from multiple modal information, and each\nlearned modal representations can be divided into uni-modal that can be learned\nfrom uni-modal training and paired-modal features that can be learned from\ncross-modal interaction. Building on this perspective, we propose a\npartitioner-guided modal learning framework, PgM, which consists of the modal\npartitioner, uni-modal learner, paired-modal learner, and uni-paired modal\ndecoder. Modal partitioner segments the learned modal representation into\nuni-modal and paired-modal features. Modal learner incorporates two dedicated\ncomponents for uni-modal and paired-modal learning. Uni-paired modal decoder\nreconstructs modal representation based on uni-modal and paired-modal features.\nPgM offers three key benefits: 1) thorough learning of uni-modal and\npaired-modal features, 2) flexible distribution adjustment for uni-modal and\npaired-modal representations to suit diverse downstream tasks, and 3) different\nlearning rates across modalities and partitions. Extensive experiments\ndemonstrate the effectiveness of PgM across four multimodal tasks and further\nhighlight its transferability to existing models. Additionally, we visualize\nthe distribution of uni-modal and paired-modal features across modalities and\ntasks, offering insights into their respective contributions.", "AI": {"tldr": "PgM\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u6001\u5206\u5272\u5668\u548c\u5b66\u4e60\u5668\u5206\u79bb\u5355\u6a21\u6001\u548c\u914d\u5bf9\u6a21\u6001\u7279\u5f81\uff0c\u63d0\u4f9b\u7075\u6d3b\u7684\u5b66\u4e60\u548c\u8c03\u6574\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\uff0c\u5355\u6a21\u6001\u548c\u914d\u5bf9\u6a21\u6001\u7279\u5f81\u7684\u5b66\u4e60\u548c\u8c03\u6574\u5bf9\u4efb\u52a1\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "PgM\u6846\u67b6\u5305\u542b\u6a21\u6001\u5206\u5272\u5668\u3001\u5355\u6a21\u6001\u5b66\u4e60\u5668\u3001\u914d\u5bf9\u6a21\u6001\u5b66\u4e60\u5668\u548c\u89e3\u7801\u5668\uff0c\u5206\u522b\u5904\u7406\u7279\u5f81\u5206\u5272\u3001\u5b66\u4e60\u548c\u91cd\u6784\u3002", "result": "PgM\u5728\u56db\u4e2a\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u73b0\u6709\u6a21\u578b\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "PgM\u901a\u8fc7\u5206\u79bb\u548c\u4f18\u5316\u5355\u6a21\u6001\u4e0e\u914d\u5bf9\u6a21\u6001\u7279\u5f81\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2507.11549", "pdf": "https://arxiv.org/pdf/2507.11549", "abs": "https://arxiv.org/abs/2507.11549", "authors": ["Wendong Mao", "Mingfan Zhao", "Jianfeng Guan", "Qiwei Dong", "Zhongfeng Wang"], "title": "An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deformable Attention Transformers (DAT) have shown remarkable performance in\ncomputer vision tasks by adaptively focusing on informative image regions.\nHowever, their data-dependent sampling mechanism introduces irregular memory\naccess patterns, posing significant challenges for efficient hardware\ndeployment. Existing acceleration methods either incur high hardware overhead\nor compromise model accuracy. To address these issues, this paper proposes a\nhardware-friendly optimization framework for DAT. First, a neural architecture\nsearch (NAS)-based method with a new slicing strategy is proposed to\nautomatically divide the input feature into uniform patches during the\ninference process, avoiding memory conflicts without modifying model\narchitecture. The method explores the optimal slice configuration by jointly\noptimizing hardware cost and inference accuracy. Secondly, an FPGA-based\nverification system is designed to test the performance of this framework on\nedge-side hardware. Algorithm experiments on the ImageNet-1K dataset\ndemonstrate that our hardware-friendly framework can maintain have only 0.2%\naccuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA\nshow the proposed method reduces DRAM access times to 18% compared with\nexisting DAT acceleration methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6\u53cb\u597d\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u53d8\u6362\u5668\uff08DAT\uff09\u5728\u786c\u4ef6\u90e8\u7f72\u4e2d\u7684\u5185\u5b58\u8bbf\u95ee\u95ee\u9898\uff0c\u901a\u8fc7NAS\u548c\u5207\u7247\u7b56\u7565\u4f18\u5316\u6027\u80fd\u3002", "motivation": "DAT\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6570\u636e\u4f9d\u8d56\u7684\u91c7\u6837\u673a\u5236\u5bfc\u81f4\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff0c\u96be\u4ee5\u9ad8\u6548\u90e8\u7f72\u5230\u786c\u4ef6\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u786c\u4ef6\u5f00\u9500\u9ad8\uff0c\u8981\u4e48\u727a\u7272\u6a21\u578b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u7684\u5207\u7247\u7b56\u7565\uff0c\u81ea\u52a8\u5212\u5206\u8f93\u5165\u7279\u5f81\u4e3a\u5747\u5300\u5757\u4ee5\u907f\u514d\u5185\u5b58\u51b2\u7a81\uff1b\u8bbe\u8ba1FPGA\u9a8c\u8bc1\u7cfb\u7edf\u6d4b\u8bd5\u6027\u80fd\u3002", "result": "\u5728ImageNet-1K\u6570\u636e\u96c6\u4e0a\uff0c\u7cbe\u5ea6\u4ec5\u4e0b\u964d0.2%\uff1b\u5728Xilinx FPGA\u4e0a\uff0cDRAM\u8bbf\u95ee\u6b21\u6570\u51cf\u5c11\u81f3\u73b0\u6709\u65b9\u6cd5\u768418%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u786c\u4ef6\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u3002"}}
{"id": "2507.11694", "pdf": "https://arxiv.org/pdf/2507.11694", "abs": "https://arxiv.org/abs/2507.11694", "authors": ["Maximiliano Hormaz\u00e1bal Lagos", "\u00c1lvaro Bueno S\u00e1ez", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro", "H\u00e9ctor Cerezo-Costas"], "title": "ExpliCIT-QA: Explainable Code-Based Image Table Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "This work has been accepted for presentation at the 24nd Portuguese\n  Conference on Artificial Intelligence (EPIA 2025) and will be published in\n  the proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available", "summary": "We present ExpliCIT-QA, a system that extends our previous MRT approach for\ntabular question answering into a multimodal pipeline capable of handling\ncomplex table images and providing explainable answers. ExpliCIT-QA follows a\nmodular design, consisting of: (1) Multimodal Table Understanding, which uses a\nChain-of-Thought approach to extract and transform content from table images;\n(2) Language-based Reasoning, where a step-by-step explanation in natural\nlanguage is generated to solve the problem; (3) Automatic Code Generation,\nwhere Python/Pandas scripts are created based on the reasoning steps, with\nfeedback for handling errors; (4) Code Execution to compute the final answer;\nand (5) Natural Language Explanation that describes how the answer was\ncomputed. The system is built for transparency and auditability: all\nintermediate outputs, parsed tables, reasoning steps, generated code, and final\nanswers are available for inspection. This strategy works towards closing the\nexplainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on\nthe TableVQA-Bench benchmark, comparing it with existing baselines. We\ndemonstrated improvements in interpretability and transparency, which open the\ndoor for applications in sensitive domains like finance and healthcare where\nauditing results are critical.", "AI": {"tldr": "ExpliCIT-QA\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u8868\u683c\u95ee\u7b54\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7b54\u6848\uff0c\u5305\u62ec\u8868\u683c\u7406\u89e3\u3001\u8bed\u8a00\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u4e0e\u6267\u884c\uff0c\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u8868\u683c\u95ee\u7b54\u7cfb\u7edf\u4e2d\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u654f\u611f\u9886\u57df\uff08\u5982\u91d1\u878d\u548c\u533b\u7597\uff09\u4e2d\u9700\u8981\u5ba1\u8ba1\u7ed3\u679c\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5305\u62ec\u591a\u6a21\u6001\u8868\u683c\u7406\u89e3\u3001\u8bed\u8a00\u63a8\u7406\u3001\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u3001\u4ee3\u7801\u6267\u884c\u548c\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "\u5728TableVQA-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "ExpliCIT-QA\u901a\u8fc7\u900f\u660e\u548c\u53ef\u5ba1\u8ba1\u7684\u8bbe\u8ba1\uff0c\u586b\u8865\u4e86\u8868\u683c\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u7a7a\u767d\uff0c\u9002\u7528\u4e8e\u654f\u611f\u9886\u57df\u3002"}}
{"id": "2507.11550", "pdf": "https://arxiv.org/pdf/2507.11550", "abs": "https://arxiv.org/abs/2507.11550", "authors": ["Hyeonseok Jin", "Geonmin Kim", "Kyungbaek Kim"], "title": "Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction", "categories": ["cs.CV", "cs.AI"], "comment": "7 pages", "summary": "Spatio-temporal traffic prediction plays a key role in intelligent\ntransportation systems by enabling accurate prediction in complex urban areas.\nAlthough not only accuracy but also efficiency for scalability is important,\nsome previous methods struggle to capture heterogeneity such as varying traffic\npatterns across regions and time periods. Moreover, Graph Neural Networks\n(GNNs), which are the mainstream of traffic prediction, not only require\npredefined adjacency matrix, but also limit scalability to large-scale data\ncontaining many nodes due to their inherent complexity. To overcome these\nlimitations, we propose Deformable Dynamic Convolution Network (DDCN) for\naccurate yet efficient traffic prediction. Traditional Convolutional Neural\nNetworks (CNNs) are limited in modeling non-Euclidean spatial structures and\nspatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically\napplying deformable filters based on offset. Specifically, DDCN decomposes\ntransformer-style CNN to encoder-decoder structure, and applies proposed\napproaches to the spatial and spatio-temporal attention blocks of the encoder\nto emphasize important features. The decoder, composed of feed-forward module,\ncomplements the output of the encoder. This novel structure make DDCN can\nperform accurate yet efficient traffic prediction. In comprehensive experiments\non four real-world datasets, DDCN achieves competitive performance, emphasizing\nthe potential and effectiveness of CNN-based approaches for spatio-temporal\ntraffic prediction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDDCN\u7684\u52a8\u6001\u53ef\u53d8\u5f62\u5377\u79ef\u7f51\u7edc\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65f6\u7a7a\u4ea4\u901a\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfGNN\u548cCNN\u5728\u6355\u6349\u5f02\u8d28\u6027\u548c\u6269\u5c55\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u4ea4\u901a\u6570\u636e\u7684\u65f6\u7a7a\u5f02\u8d28\u6027\u53ca\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662fGNN\u9700\u8981\u9884\u5b9a\u4e49\u90bb\u63a5\u77e9\u9635\u4e14\u96be\u4ee5\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u3002", "method": "DDCN\u901a\u8fc7\u52a8\u6001\u5e94\u7528\u57fa\u4e8e\u504f\u79fb\u7684\u53ef\u53d8\u5f62\u6ee4\u6ce2\u5668\uff0c\u7ed3\u5408\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u5229\u7528\u7a7a\u95f4\u548c\u65f6\u7a7a\u6ce8\u610f\u529b\u5757\u5f3a\u8c03\u91cd\u8981\u7279\u5f81\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDDCN\u6027\u80fd\u4f18\u8d8a\uff0c\u9a8c\u8bc1\u4e86CNN\u65b9\u6cd5\u5728\u65f6\u7a7a\u4ea4\u901a\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "DDCN\u4e3a\u65f6\u7a7a\u4ea4\u901a\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.11742", "pdf": "https://arxiv.org/pdf/2507.11742", "abs": "https://arxiv.org/abs/2507.11742", "authors": ["Meng Li", "Timothy M. McPhillips", "Dingmin Wang", "Shin-Rong Tsai", "Bertram Lud\u00e4scher"], "title": "CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. Accepted to COLM 2025", "summary": "Recognizing the information flows and operations comprising data science and\nmachine learning Python notebooks is critical for evaluating, reusing, and\nadapting notebooks for new tasks. Investigating a notebook via re-execution\noften is impractical due to the challenges of resolving data and software\ndependencies. While Large Language Models (LLMs) pre-trained on large codebases\nhave demonstrated effectiveness in understanding code without running it, we\nobserve that they fail to understand some realistic notebooks due to\nhallucinations and long-context challenges. To address these issues, we propose\na notebook understanding task yielding an information flow graph and\ncorresponding cell execution dependency graph for a notebook, and demonstrate\nthe effectiveness of a pincer strategy that uses limited syntactic analysis to\nassist full comprehension of the notebook using an LLM. Our Capture and Resolve\nAssisted Bounding Strategy (CRABS) employs shallow syntactic parsing and\nanalysis of the abstract syntax tree (AST) to capture the correct\ninterpretation of a notebook between lower and upper estimates of the\ninter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via\ncell-by-cell zero-shot learning, thereby identifying the true data inputs and\noutputs of each cell. We evaluate and demonstrate the effectiveness of our\napproach using an annotated dataset of 50 representative, highly up-voted\nKaggle notebooks that together represent 3454 actual cell inputs and outputs.\nThe LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the\nsyntactic structure of these notebooks. Across 50 notebooks, CRABS achieves\naverage F1 scores of 98% identifying cell-to-cell information flows and 99%\nidentifying transitive cell execution dependencies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCRABS\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6d45\u5c42\u8bed\u6cd5\u5206\u6790\u548cLLM\uff0c\u89e3\u51b3\u4e86Python\u7b14\u8bb0\u672c\u4e2d\u4fe1\u606f\u6d41\u548c\u6267\u884c\u4f9d\u8d56\u7684\u8bc6\u522b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60Python\u7b14\u8bb0\u672c\u7684\u590d\u6742\u6027\u548c\u4f9d\u8d56\u95ee\u9898\uff0c\u76f4\u63a5\u91cd\u65b0\u6267\u884c\u7b14\u8bb0\u672c\u4ee5\u7406\u89e3\u5176\u4fe1\u606f\u6d41\u548c\u64cd\u4f5c\u5f80\u5f80\u4e0d\u5207\u5b9e\u9645\u3002\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7406\u89e3\u4ee3\u7801\u65f6\u5b58\u5728\u5e7b\u89c9\u548c\u957f\u4e0a\u4e0b\u6587\u6311\u6218\u3002", "method": "\u63d0\u51faCRABS\u65b9\u6cd5\uff0c\u7ed3\u5408\u6d45\u5c42\u8bed\u6cd5\u5206\u6790\uff08\u5982AST\uff09\u548cLLM\u7684\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u6355\u6349\u7b14\u8bb0\u672c\u7684\u51c6\u786e\u89e3\u91ca\uff0c\u5e76\u89e3\u51b3\u5269\u4f59\u6b67\u4e49\u3002", "result": "\u572850\u4e2aKaggle\u7b14\u8bb0\u672c\u7684\u8bc4\u4f30\u4e2d\uff0cLLM\u89e3\u51b3\u4e8698%\u7684\u6b67\u4e49\uff0cCRABS\u5728\u4fe1\u606f\u6d41\u548c\u6267\u884c\u4f9d\u8d56\u8bc6\u522b\u4e0a\u7684F1\u5206\u6570\u5206\u522b\u8fbe\u523098%\u548c99%\u3002", "conclusion": "CRABS\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u8bed\u6cd5\u5206\u6790\u548cLLM\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7b14\u8bb0\u672c\u7406\u89e3\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u7b14\u8bb0\u672c\u7684\u8bc4\u4f30\u548c\u91cd\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.11554", "pdf": "https://arxiv.org/pdf/2507.11554", "abs": "https://arxiv.org/abs/2507.11554", "authors": ["Zejian Li", "Yize Li", "Chenye Meng", "Zhongni Liu", "Yang Ling", "Shengyuan Zhang", "Guang Yang", "Changyuan Yang", "Zhiyuan Yang", "Lingyun Sun"], "title": "Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in diffusion models (DMs) have been propelled by\nalignment methods that post-train models to better conform to human\npreferences. However, these approaches typically require computation-intensive\ntraining of a base model and a reward model, which not only incurs substantial\ncomputational overhead but may also compromise model accuracy and training\nefficiency. To address these limitations, we propose Inversion-DPO, a novel\nalignment framework that circumvents reward modeling by reformulating Direct\nPreference Optimization (DPO) with DDIM inversion for DMs. Our method conducts\nintractable posterior sampling in Diffusion-DPO with the deterministic\ninversion from winning and losing samples to noise and thus derive a new\npost-training paradigm. This paradigm eliminates the need for auxiliary reward\nmodels or inaccurate appromixation, significantly enhancing both precision and\nefficiency of training. We apply Inversion-DPO to a basic task of text-to-image\ngeneration and a challenging task of compositional image generation. Extensive\nexperiments show substantial performance improvements achieved by Inversion-DPO\ncompared to existing post-training methods and highlight the ability of the\ntrained generative models to generate high-fidelity compositionally coherent\nimages. For the post-training of compostitional image geneation, we curate a\npaired dataset consisting of 11,140 images with complex structural annotations\nand comprehensive scores, designed to enhance the compositional capabilities of\ngenerative models. Inversion-DPO explores a new avenue for efficient,\nhigh-precision alignment in diffusion models, advancing their applicability to\ncomplex realistic generation tasks. Our code is available at\nhttps://github.com/MIGHTYEZ/Inversion-DPO", "AI": {"tldr": "Inversion-DPO\u662f\u4e00\u79cd\u65b0\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7DDIM\u53cd\u6f14\u4f18\u5316\u6269\u6563\u6a21\u578b\uff0c\u907f\u514d\u4e86\u5956\u52b1\u5efa\u6a21\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u9700\u8981\u8ba1\u7b97\u5bc6\u96c6\u578b\u8bad\u7ec3\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u7cbe\u5ea6\u548c\u6548\u7387\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7DDIM\u53cd\u6f14\u91cd\u65b0\u5b9a\u4e49\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\uff0c\u5728\u566a\u58f0\u4e2d\u91c7\u6837\uff0c\u907f\u514d\u8f85\u52a9\u5956\u52b1\u6a21\u578b\u6216\u4e0d\u51c6\u786e\u8fd1\u4f3c\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u7ec4\u5408\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cInversion-DPO\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u4e14\u7ec4\u5408\u4e00\u81f4\u7684\u56fe\u50cf\u3002", "conclusion": "Inversion-DPO\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u7cbe\u5ea6\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73b0\u5b9e\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2507.11764", "pdf": "https://arxiv.org/pdf/2507.11764", "abs": "https://arxiv.org/abs/2507.11764", "authors": ["Matteo Fasulo", "Luca Babboni", "Luca Tedeschini"], "title": "AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles", "categories": ["cs.CL", "cs.IR"], "comment": "14 pages, 6 figures, accepted at CLEF 2025 CheckThat! Lab", "summary": "This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\nTraining/development datasets were provided for Arabic, German, English,\nItalian, and Bulgarian; final evaluation included additional unseen languages\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\nprimary strategy enhanced transformer-based classifiers by integrating\nsentiment scores, derived from an auxiliary model, with sentence\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\nlanguages, we employed decision threshold calibration optimized on the\ndevelopment set. Our experiments show sentiment feature integration\nsignificantly boosts performance, especially subjective F1 score. This\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).", "AI": {"tldr": "AI Wizards\u56e2\u961f\u5728CLEF 2025 CheckThat! Lab\u4efb\u52a11\u4e2d\uff0c\u901a\u8fc7\u7ed3\u5408\u60c5\u611f\u5206\u6570\u589e\u5f3aTransformer\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u89c2\u6027\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u5e0c\u814a\u8bed\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6539\u8fdb\u65b0\u95fb\u6587\u7ae0\u4e2d\u53e5\u5b50\u4e3b\u89c2\u6027\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff08\u5982mDeBERTaV3-base\u3001ModernBERT-base\u548cLlama3.2-1B\uff09\uff0c\u7ed3\u5408\u60c5\u611f\u5206\u6570\u589e\u5f3a\u53e5\u5b50\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u51b3\u7b56\u9608\u503c\u6821\u51c6\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u60c5\u611f\u7279\u5f81\u96c6\u6210\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4e3b\u89c2F1\u5206\u6570\u4e0a\uff0c\u5e0c\u814a\u8bed\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u9ad8\u6392\u540d\uff08Macro F1 = 0.51\uff09\u3002", "conclusion": "\u901a\u8fc7\u60c5\u611f\u589e\u5f3a\u7684Transformer\u6a21\u578b\u5728\u591a\u8bed\u8a00\u4e3b\u89c2\u6027\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.11558", "pdf": "https://arxiv.org/pdf/2507.11558", "abs": "https://arxiv.org/abs/2507.11558", "authors": ["Changlu Chen", "Yanbin Liu", "Chaoxi Niu", "Ling Chen", "Tianqing Zhu"], "title": "Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models have achieved remarkable success in natural language\nprocessing and computer vision, demonstrating strong capabilities in modeling\ncomplex patterns. While recent efforts have explored adapting large language\nmodels (LLMs) for time-series forecasting, LLMs primarily capture\none-dimensional sequential dependencies and struggle to model the richer\nspatio-temporal (ST) correlations essential for accurate ST forecasting. In\nthis paper, we present \\textbf{ST-VFM}, a novel framework that systematically\nreprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal\nforecasting. While VFMs offer powerful spatial priors, two key challenges arise\nwhen applying them to ST tasks: (1) the lack of inherent temporal modeling\ncapacity and (2) the modality gap between visual and ST data. To address these,\nST-VFM adopts a \\emph{dual-branch architecture} that integrates raw ST inputs\nwith auxiliary ST flow inputs, where the flow encodes lightweight temporal\ndifference signals interpretable as dynamic spatial cues. To effectively\nprocess these dual-branch inputs, ST-VFM introduces two dedicated reprogramming\nstages. The \\emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token\nAdapter to embed temporal context and align both branches into VFM-compatible\nfeature spaces. The \\emph{post-VFM reprogramming} stage introduces a Bilateral\nCross-Prompt Coordination module, enabling dynamic interaction between branches\nthrough prompt-based conditioning, thus enriching joint representation learning\nwithout modifying the frozen VFM backbone. Extensive experiments on ten\nspatio-temporal datasets show that ST-VFM outperforms state-of-the-art\nbaselines, demonstrating effectiveness and robustness across VFM backbones\n(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong\ngeneral framework for spatio-temporal forecasting.", "AI": {"tldr": "ST-VFM\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u7f16\u7a0b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u6765\u89e3\u51b3\u65f6\u7a7a\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u5305\u62ec\u7f3a\u4e4f\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\u548c\u89c6\u89c9\u4e0e\u65f6\u7a7a\u6570\u636e\u7684\u6a21\u6001\u5dee\u8ddd\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65f6\u7a7a\u9884\u6d4b\u4e2d\u96be\u4ee5\u6355\u6349\u4e30\u5bcc\u7684\u65f6\u7a7a\u76f8\u5173\u6027\u3002ST-VFM\u65e8\u5728\u5229\u7528VFMs\u7684\u7a7a\u95f4\u5148\u9a8c\u77e5\u8bc6\uff0c\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "ST-VFM\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff0c\u7ed3\u5408\u539f\u59cb\u65f6\u7a7a\u8f93\u5165\u548c\u8f85\u52a9\u65f6\u7a7a\u6d41\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u91cd\u65b0\u7f16\u7a0b\u9636\u6bb5\uff08\u9884VFM\u548c\u540eVFM\uff09\u5904\u7406\u8fd9\u4e9b\u8f93\u5165\uff0c\u4ee5\u589e\u5f3a\u65f6\u7a7a\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728\u5341\u4e2a\u65f6\u7a7a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cST-VFM\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "ST-VFM\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u901a\u7528\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u65f6\u7a7a\u9884\u6d4b\u4efb\u52a1\uff0c\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7684VFM\u4e3b\u5e72\u7f51\u7edc\u3002"}}
{"id": "2507.11809", "pdf": "https://arxiv.org/pdf/2507.11809", "abs": "https://arxiv.org/abs/2507.11809", "authors": ["Dante Campregher", "Yanxu Chen", "Sander Hoffman", "Maria Heuss"], "title": "Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "18 Pages, 13 figures", "summary": "This paper presents a reproducibility study examining how Large Language\nModels (LLMs) manage competing factual and counterfactual information, focusing\non the role of attention heads in this process. We attempt to reproduce and\nreconcile findings from three recent studies by Ortu et al., Yu, Merullo, and\nPavlick and McDougall et al. that investigate the competition between\nmodel-learned facts and contradictory context information through Mechanistic\nInterpretability tools. Our study specifically examines the relationship\nbetween attention head strength and factual output ratios, evaluates competing\nhypotheses about attention heads' suppression mechanisms, and investigates the\ndomain specificity of these attention patterns. Our findings suggest that\nattention heads promoting factual output do so via general copy suppression\nrather than selective counterfactual suppression, as strengthening them can\nalso inhibit correct facts. Additionally, we show that attention head behavior\nis domain-dependent, with larger models exhibiting more specialized and\ncategory-sensitive patterns.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u53ef\u91cd\u590d\u6027\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u5904\u7406\u7ade\u4e89\u6027\u7684\u4e8b\u5b9e\u4e0e\u53cd\u4e8b\u5b9e\u4fe1\u606f\uff0c\u91cd\u70b9\u5173\u6ce8\u6ce8\u610f\u529b\u5934\u7684\u4f5c\u7528\u3002\u7814\u7a76\u590d\u73b0\u5e76\u6574\u5408\u4e86\u8fd1\u671f\u4e09\u9879\u7814\u7a76\u7684\u7ed3\u679c\uff0c\u53d1\u73b0\u6ce8\u610f\u529b\u5934\u901a\u8fc7\u901a\u7528\u590d\u5236\u6291\u5236\u800c\u975e\u9009\u62e9\u6027\u53cd\u4e8b\u5b9e\u6291\u5236\u6765\u4fc3\u8fdb\u4e8b\u5b9e\u8f93\u51fa\uff0c\u4e14\u5176\u884c\u4e3a\u5177\u6709\u9886\u57df\u4f9d\u8d56\u6027\u3002", "motivation": "\u63a2\u8ba8LLMs\u5982\u4f55\u7ba1\u7406\u7ade\u4e89\u6027\u7684\u4e8b\u5b9e\u4e0e\u53cd\u4e8b\u5b9e\u4fe1\u606f\uff0c\u7406\u89e3\u6ce8\u610f\u529b\u5934\u5728\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u673a\u5236\u3002", "method": "\u4f7f\u7528\u673a\u5236\u89e3\u91ca\u5de5\u5177\u590d\u73b0\u5e76\u6574\u5408\u4e09\u9879\u8fd1\u671f\u7814\u7a76\u7684\u7ed3\u679c\uff0c\u5206\u6790\u6ce8\u610f\u529b\u5934\u5f3a\u5ea6\u4e0e\u4e8b\u5b9e\u8f93\u51fa\u6bd4\u7387\u7684\u5173\u7cfb\uff0c\u8bc4\u4f30\u5176\u6291\u5236\u673a\u5236\u5047\u8bbe\uff0c\u5e76\u7814\u7a76\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u9886\u57df\u7279\u5f02\u6027\u3002", "result": "\u6ce8\u610f\u529b\u5934\u901a\u8fc7\u901a\u7528\u590d\u5236\u6291\u5236\u800c\u975e\u9009\u62e9\u6027\u53cd\u4e8b\u5b9e\u6291\u5236\u4fc3\u8fdb\u4e8b\u5b9e\u8f93\u51fa\uff0c\u4e14\u5176\u884c\u4e3a\u5177\u6709\u9886\u57df\u4f9d\u8d56\u6027\uff0c\u8f83\u5927\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u4e13\u4e1a\u548c\u7c7b\u522b\u654f\u611f\u7684\u6a21\u5f0f\u3002", "conclusion": "\u6ce8\u610f\u529b\u5934\u7684\u4f5c\u7528\u673a\u5236\u662f\u901a\u7528\u800c\u975e\u9009\u62e9\u6027\u7684\uff0c\u4e14\u5176\u884c\u4e3a\u53d7\u9886\u57df\u5f71\u54cd\uff0c\u8fd9\u5bf9\u7406\u89e3LLMs\u7684\u4fe1\u606f\u5904\u7406\u673a\u5236\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.11562", "pdf": "https://arxiv.org/pdf/2507.11562", "abs": "https://arxiv.org/abs/2507.11562", "authors": ["Ozer Can Devecioglu", "Serkan Kiranyaz", "Mehmet Yamac", "Moncef Gabbouj"], "title": "Expert Operational GANS: Towards Real-Color Underwater Image Restoration", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "6 pages", "summary": "The wide range of deformation artifacts that arise from complex light\npropagation, scattering, and depth-dependent attenuation makes the underwater\nimage restoration to remain a challenging problem. Like other single deep\nregressor networks, conventional GAN-based restoration methods struggle to\nperform well across this heterogeneous domain, since a single generator network\nis typically insufficient to capture the full range of visual degradations. In\norder to overcome this limitation, we propose xOp-GAN, a novel GAN model with\nseveral expert generator networks, each trained solely on a particular subset\nwith a certain image quality. Thus, each generator can learn to maximize its\nrestoration performance for a particular quality range. Once a xOp-GAN is\ntrained, each generator can restore the input image and the best restored image\ncan then be selected by the discriminator based on its perceptual confidence\nscore. As a result, xOP-GAN is the first GAN model with multiple generators\nwhere the discriminator is being used during the inference of the regression\ntask. Experimental results on benchmark Large Scale Underwater Image (LSUI)\ndataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,\nsurpassing all single-regressor models by a large margin even, with reduced\ncomplexity.", "AI": {"tldr": "\u63d0\u51faxOp-GAN\uff0c\u4e00\u79cd\u591a\u751f\u6210\u5668\u7684GAN\u6a21\u578b\uff0c\u901a\u8fc7\u4e13\u5bb6\u751f\u6210\u5668\u5206\u522b\u5904\u7406\u4e0d\u540c\u8d28\u91cf\u8303\u56f4\u7684\u56fe\u50cf\uff0c\u63d0\u5347\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5355\u751f\u6210\u5668GAN\u96be\u4ee5\u5e94\u5bf9\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u4e2d\u590d\u6742\u7684\u89c6\u89c9\u9000\u5316\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8986\u76d6\u5f02\u6784\u57df\u7684\u65b9\u6cd5\u3002", "method": "xOp-GAN\u91c7\u7528\u591a\u4e2a\u4e13\u5bb6\u751f\u6210\u5668\uff0c\u6bcf\u4e2a\u751f\u6210\u5668\u4e13\u6ce8\u4e8e\u7279\u5b9a\u8d28\u91cf\u8303\u56f4\u7684\u56fe\u50cf\u6062\u590d\uff0c\u5e76\u901a\u8fc7\u5224\u522b\u5668\u9009\u62e9\u6700\u4f73\u6062\u590d\u7ed3\u679c\u3002", "result": "\u5728LSUI\u6570\u636e\u96c6\u4e0a\uff0cxOp-GAN\u7684PSNR\u8fbe\u523025.16 dB\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u56de\u5f52\u5668\u6a21\u578b\u3002", "conclusion": "xOp-GAN\u901a\u8fc7\u591a\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.11832", "pdf": "https://arxiv.org/pdf/2507.11832", "abs": "https://arxiv.org/abs/2507.11832", "authors": ["Yash Ingle", "Pruthwik Mishra"], "title": "ILID: Native Script Language Identification for Indian Languages", "categories": ["cs.CL"], "comment": "8 pages, 1 figure, 7 tables, Paper accepted in RANLP 2025", "summary": "The language identification task is a crucial fundamental step in NLP. Often\nit serves as a pre-processing step for widely used NLP applications such as\nmultilingual machine translation, information retrieval, question and\nanswering, and text summarization. The core challenge of language\nidentification lies in distinguishing languages in noisy, short, and code-mixed\nenvironments. This becomes even harder in case of diverse Indian languages that\nexhibit lexical and phonetic similarities, but have distinct differences. Many\nIndian languages share the same script making the task even more challenging.\nIn this paper, we release a dataset of 230K sentences consisting of English and\nall 22 official Indian languages labeled with their language identifiers where\ndata in most languages are newly created. We also develop and release robust\nbaseline models using state-of-the-art approaches in machine learning and deep\nlearning that can aid the research in this field. Our baseline models are\ncomparable to the state-of-the-art models for the language identification task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u7684\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u7279\u522b\u9488\u5bf9\u5370\u5ea6\u8bed\u8a00\uff0c\u89e3\u51b3\u4e86\u566a\u58f0\u3001\u77ed\u6587\u672c\u548c\u6df7\u5408\u4ee3\u7801\u73af\u5883\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u8bed\u8a00\u8bc6\u522b\u662fNLP\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u5c24\u5176\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u5c24\u4e3a\u91cd\u8981\u3002\u5370\u5ea6\u8bed\u8a00\u56e0\u5171\u4eab\u811a\u672c\u548c\u76f8\u4f3c\u6027\u800c\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b23\u4e07\u53e5\u5b50\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u82f1\u8bed\u548c22\u79cd\u5370\u5ea6\u8bed\u8a00\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u57fa\u7ebf\u6a21\u578b\u5728\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u5f53\u524d\u6700\u4f18\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5370\u5ea6\u8bed\u8a00\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u6570\u636e\u96c6\u548c\u5f3a\u5927\u57fa\u7ebf\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.11571", "pdf": "https://arxiv.org/pdf/2507.11571", "abs": "https://arxiv.org/abs/2507.11571", "authors": ["Varun Velankar"], "title": "Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Estimating a person's age from their gait has important applications in\nhealthcare, security and human-computer interaction. In this work, we review\nfifty-nine studies involving over seventy-five thousand subjects recorded with\nvideo, wearable and radar sensors. We observe that convolutional neural\nnetworks produce an average error of about 4.2 years, inertial-sensor models\nabout 4.5 years and multi-sensor fusion as low as 3.4 years, with notable\ndifferences between lab and real-world data. We then analyse sixty-three\nthousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population\ndataset to quantify correlations between age and five key metrics: stride\nlength, walking speed, step cadence, step-time variability and joint-angle\nentropy, with correlation coefficients of at least 0.27. Next, we fine-tune a\nResNet34 model and apply Grad-CAM to reveal that the network attends to the\nknee and pelvic regions, consistent with known age-related gait changes.\nFinally, on a one hundred thousand sample subset of the VersatileGait database,\nwe compare support vector machines, decision trees, random forests, multilayer\nperceptrons and convolutional neural networks, finding that deep networks\nachieve up to 96 percent accuracy while processing each sample in under 0.1\nseconds. By combining a broad meta-analysis with new large-scale experiments\nand interpretable visualizations, we establish solid performance baselines and\npractical guidelines for reducing gait-age error below three years in\nreal-world scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5143\u5206\u6790\u548c\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86\u57fa\u4e8e\u6b65\u6001\u4f30\u8ba1\u5e74\u9f84\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u591a\u4f20\u611f\u5668\u878d\u5408\u8bef\u5dee\u6700\u4f4e\uff083.4\u5e74\uff09\uff0c\u5e76\u5206\u6790\u4e86\u6b65\u6001\u7279\u5f81\u4e0e\u5e74\u9f84\u7684\u76f8\u5173\u6027\u3002", "motivation": "\u6b65\u6001\u5e74\u9f84\u4f30\u8ba1\u5728\u533b\u7597\u3001\u5b89\u5168\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u7684\u6027\u80fd\u57fa\u51c6\u548c\u5b9e\u7528\u6307\u5357\u3002", "method": "\u7ed3\u5408\u5143\u5206\u6790\uff0859\u9879\u7814\u7a76\uff09\u548c\u5927\u89c4\u6a21\u5b9e\u9a8c\uff08OU-ISIR\u548cVersatileGait\u6570\u636e\u96c6\uff09\uff0c\u4f7f\u7528CNN\u3001SVM\u7b49\u65b9\u6cd5\u8bc4\u4f30\u6027\u80fd\uff0c\u5e76\u901a\u8fc7Grad-CAM\u89e3\u91ca\u6a21\u578b\u5173\u6ce8\u533a\u57df\u3002", "result": "\u591a\u4f20\u611f\u5668\u878d\u5408\u8bef\u5dee\u6700\u4f4e\uff083.4\u5e74\uff09\uff0cCNN\u51c6\u786e\u7387\u9ad8\u8fbe96%\uff0c\u6b65\u6001\u7279\u5f81\u4e0e\u5e74\u9f84\u76f8\u5173\u6027\u663e\u8457\uff08\u76f8\u5173\u7cfb\u6570\u22650.27\uff09\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6b65\u6001\u5e74\u9f84\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6027\u80fd\u57fa\u51c6\u548c\u5b9e\u7528\u6307\u5357\uff0c\u76ee\u6807\u662f\u5c06\u8bef\u5dee\u964d\u81f33\u5e74\u4ee5\u4e0b\u3002"}}
{"id": "2507.11851", "pdf": "https://arxiv.org/pdf/2507.11851", "abs": "https://arxiv.org/abs/2507.11851", "authors": ["Mohammad Samragh", "Arnav Kundu", "David Harrison", "Kumari Nishu", "Devang Naik", "Minsik Cho", "Mehrdad Farajtabar"], "title": "Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Autoregressive language models are constrained by their inherently sequential\nnature, generating one token at a time. This paradigm limits inference speed\nand parallelism, especially during later stages of generation when the\ndirection and semantics of text are relatively certain. In this work, we\npropose a novel framework that leverages the inherent knowledge of vanilla\nautoregressive language models about future tokens, combining techniques to\nrealize this potential and enable simultaneous prediction of multiple\nsubsequent tokens. Our approach introduces several key innovations: (1) a\nmasked-input formulation where multiple future tokens are jointly predicted\nfrom a common prefix; (2) a gated LoRA formulation that preserves the original\nLLM's functionality, while equipping it for multi-token prediction; (3) a\nlightweight, learnable sampler module that generates coherent sequences from\nthe predicted future tokens; (4) a set of auxiliary training losses, including\na consistency loss, to enhance the coherence and accuracy of jointly generated\ntokens; and (5) a speculative generation strategy that expands tokens\nquadratically in the future while maintaining high fidelity. Our method\nachieves significant speedups through supervised fine-tuning on pretrained\nmodels. For example, it generates code and math nearly 5x faster, and improves\ngeneral chat and knowledge tasks by almost 2.5x. These gains come without any\nloss in quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4ee4\u724c\u9884\u6d4b\u63d0\u5347\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u548c\u5e76\u884c\u6027\uff0c\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u4e14\u4e0d\u635f\u5931\u8d28\u91cf\u3002", "motivation": "\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u7684\u987a\u5e8f\u751f\u6210\u7279\u6027\u9650\u5236\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u5e76\u884c\u6027\uff0c\u5c24\u5176\u5728\u751f\u6210\u540e\u671f\u6587\u672c\u65b9\u5411\u548c\u8bed\u4e49\u5df2\u786e\u5b9a\u65f6\u3002", "method": "\u7ed3\u5408\u63a9\u7801\u8f93\u5165\u3001\u95e8\u63a7LoRA\u3001\u8f7b\u91cf\u91c7\u6837\u5668\u3001\u8f85\u52a9\u8bad\u7ec3\u635f\u5931\u548c\u63a8\u6d4b\u751f\u6210\u7b56\u7565\uff0c\u5b9e\u73b0\u591a\u4ee4\u724c\u9884\u6d4b\u3002", "result": "\u4ee3\u7801\u548c\u6570\u5b66\u751f\u6210\u901f\u5ea6\u63d0\u5347\u8fd15\u500d\uff0c\u901a\u7528\u804a\u5929\u548c\u77e5\u8bc6\u4efb\u52a1\u63d0\u5347\u7ea62.5\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u52a0\u901f\u751f\u6210\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u5e76\u884c\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11575", "pdf": "https://arxiv.org/pdf/2507.11575", "abs": "https://arxiv.org/abs/2507.11575", "authors": ["Victor Caquilpan"], "title": "What cat is that? A re-id model for feral cats", "categories": ["cs.CV", "cs.AI"], "comment": "Master's project", "summary": "Feral cats exert a substantial and detrimental impact on Australian wildlife,\nplacing them among the most dangerous invasive species worldwide. Therefore,\nclosely monitoring these cats is essential labour in minimising their effects.\nIn this context, the potential application of Re-Identification (re-ID) emerges\nto enhance monitoring activities for these animals, utilising images captured\nby camera traps. This project explores different CV approaches to create a\nre-ID model able to identify individual feral cats in the wild. The main\napproach consists of modifying a part-pose guided network (PPGNet) model,\ninitially used in the re-ID of Amur tigers, to be applicable for feral cats.\nThis adaptation, resulting in PPGNet-Cat, which incorporates specific\nmodifications to suit the characteristics of feral cats images. Additionally,\nvarious experiments were conducted, particularly exploring contrastive learning\napproaches such as ArcFace loss. The main results indicate that PPGNet-Cat\nexcels in identifying feral cats, achieving high performance with a mean\nAverage Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes\nestablish PPGNet-Cat as a competitive model within the realm of re-ID.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u6539\u8fdb\u7684PPGNet\u6a21\u578b\uff08PPGNet-Cat\uff09\u901a\u8fc7\u56fe\u50cf\u91cd\u8bc6\u522b\u6280\u672f\u76d1\u63a7\u91ce\u732b\uff0c\u53d6\u5f97\u4e86\u9ad8\u7cbe\u5ea6\u7ed3\u679c\u3002", "motivation": "\u91ce\u732b\u5bf9\u6fb3\u5927\u5229\u4e9a\u91ce\u751f\u52a8\u7269\u9020\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u9700\u8981\u9ad8\u6548\u76d1\u63a7\u4ee5\u51cf\u5c11\u5176\u5f71\u54cd\u3002", "method": "\u6539\u8fdbPPGNet\u6a21\u578b\u4e3aPPGNet-Cat\uff0c\u5e76\u63a2\u7d22\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5982ArcFace\u635f\u5931\u3002", "result": "PPGNet-Cat\u8868\u73b0\u4f18\u5f02\uff0cmAP\u8fbe0.86\uff0crank-1\u51c6\u786e\u73870.95\u3002", "conclusion": "PPGNet-Cat\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u91ce\u732b\u91cd\u8bc6\u522b\u6a21\u578b\u3002"}}
{"id": "2507.11862", "pdf": "https://arxiv.org/pdf/2507.11862", "abs": "https://arxiv.org/abs/2507.11862", "authors": ["Junhong Ye", "Xu Yuan", "Xinying Qiu"], "title": "Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition", "categories": ["cs.CL"], "comment": "Accepted to CLNLP 2025", "summary": "Accurate recognition of personally identifiable information (PII) is central\nto automated text anonymization. This paper investigates the effectiveness of\ncross-domain model transfer, multi-domain data fusion, and sample-efficient\nlearning for PII recognition. Using annotated corpora from healthcare (I2B2),\nlegal (TAB), and biography (Wikipedia), we evaluate models across four\ndimensions: in-domain performance, cross-domain transferability, fusion, and\nfew-shot learning. Results show legal-domain data transfers well to\nbiographical texts, while medical domains resist incoming transfer. Fusion\nbenefits are domain-specific, and high-quality recognition is achievable with\nonly 10% of training data in low-specialization domains.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8de8\u9886\u57df\u6a21\u578b\u8fc1\u79fb\u3001\u591a\u9886\u57df\u6570\u636e\u878d\u5408\u548c\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u5728PII\u8bc6\u522b\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u6cd5\u5f8b\u9886\u57df\u6570\u636e\u80fd\u5f88\u597d\u5730\u8fc1\u79fb\u5230\u4f20\u8bb0\u6587\u672c\uff0c\u800c\u533b\u7597\u9886\u57df\u6570\u636e\u8fc1\u79fb\u6548\u679c\u8f83\u5dee\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u8de8\u9886\u57df\u6a21\u578b\u8fc1\u79fb\u3001\u6570\u636e\u878d\u5408\u548c\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u63d0\u9ad8PII\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u533b\u7597\u3001\u6cd5\u5f8b\u548c\u4f20\u8bb0\u9886\u57df\u7684\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u9886\u57df\u5185\u6027\u80fd\u3001\u8de8\u9886\u57df\u8fc1\u79fb\u6027\u3001\u6570\u636e\u878d\u5408\u548c\u5c11\u6837\u672c\u5b66\u4e60\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6cd5\u5f8b\u9886\u57df\u6570\u636e\u5728\u4f20\u8bb0\u6587\u672c\u4e2d\u8fc1\u79fb\u6548\u679c\u826f\u597d\uff0c\u533b\u7597\u9886\u57df\u6570\u636e\u8fc1\u79fb\u6548\u679c\u5dee\uff1b\u6570\u636e\u878d\u5408\u7684\u6548\u679c\u56e0\u9886\u57df\u800c\u5f02\uff1b\u5728\u4f4e\u4e13\u4e1a\u5316\u9886\u57df\u4e2d\uff0c\u4ec5\u970010%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8bc6\u522b\u3002", "conclusion": "\u8de8\u9886\u57df\u8fc1\u79fb\u548c\u6570\u636e\u878d\u5408\u7684\u6548\u679c\u5177\u6709\u9886\u57df\u4f9d\u8d56\u6027\uff0c\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u5728\u7279\u5b9a\u9886\u57df\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.11579", "pdf": "https://arxiv.org/pdf/2507.11579", "abs": "https://arxiv.org/abs/2507.11579", "authors": ["Sathvik Chereddy", "John Femiani"], "title": "SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation", "categories": ["cs.CV", "cs.LG"], "comment": "17 pages, 63 figures, Proceedings of the 42nd International\n  Conference on Machine Learning (ICML2025)", "summary": "We present SketchDNN, a generative model for synthesizing CAD sketches that\njointly models both continuous parameters and discrete class labels through a\nunified continuous-discrete diffusion process. Our core innovation is\nGaussian-Softmax diffusion, where logits perturbed with Gaussian noise are\nprojected onto the probability simplex via a softmax transformation,\nfacilitating blended class labels for discrete variables. This formulation\naddresses 2 key challenges, namely, the heterogeneity of primitive\nparameterizations and the permutation invariance of primitives in CAD sketches.\nOur approach significantly improves generation quality, reducing Fr\\'echet\nInception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)\nfrom 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch\ngeneration on the SketchGraphs dataset.", "AI": {"tldr": "SketchDNN\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210CAD\u8349\u56fe\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u8fde\u7eed-\u79bb\u6563\u6269\u6563\u8fc7\u7a0b\u8054\u5408\u5efa\u6a21\u8fde\u7eed\u53c2\u6570\u548c\u79bb\u6563\u7c7b\u522b\u6807\u7b7e\u3002", "motivation": "\u89e3\u51b3CAD\u8349\u56fe\u4e2d\u539f\u59cb\u53c2\u6570\u5316\u7684\u5f02\u8d28\u6027\u548c\u539f\u59cb\u5143\u7d20\u7684\u6392\u5217\u4e0d\u53d8\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528Gaussian-Softmax\u6269\u6563\uff0c\u901a\u8fc7\u9ad8\u65af\u566a\u58f0\u6270\u52a8logits\u5e76\u901a\u8fc7softmax\u53d8\u6362\u6295\u5f71\u5230\u6982\u7387\u5355\u7eaf\u5f62\u4e0a\u3002", "result": "\u751f\u6210\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0cFID\u4ece16.04\u964d\u81f37.80\uff0cNLL\u4ece84.8\u964d\u81f381.33\u3002", "conclusion": "\u5728SketchGraphs\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86CAD\u8349\u56fe\u751f\u6210\u7684\u6700\u65b0\u6210\u679c\u3002"}}
{"id": "2507.11867", "pdf": "https://arxiv.org/pdf/2507.11867", "abs": "https://arxiv.org/abs/2507.11867", "authors": ["Xiangyu Yang", "Xinying Qiu"], "title": "COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction", "categories": ["cs.CL"], "comment": "Accepted to CLNLP 2025", "summary": "Grammatical Error Correction (GEC) and grammatical acceptability judgment\n(COLA) are core tasks in natural language processing, sharing foundational\ngrammatical knowledge yet typically evolving independently. This paper\nintroduces COLA-GEC, a novel bidirectional framework that enhances both tasks\nthrough mutual knowledge transfer. First, we augment grammatical acceptability\nmodels using GEC datasets, significantly improving their performance across\nmultiple languages. Second, we integrate grammatical acceptability signals into\nGEC model training via a dynamic loss function, effectively guiding corrections\ntoward grammatically acceptable outputs. Our approach achieves state-of-the-art\nresults on several multilingual benchmarks. Comprehensive error analysis\nhighlights remaining challenges, particularly in punctuation error correction,\nproviding insights for future improvements in grammatical modeling.", "AI": {"tldr": "COLA-GEC\u6846\u67b6\u901a\u8fc7\u53cc\u5411\u77e5\u8bc6\u8f6c\u79fb\u63d0\u5347\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63\uff08GEC\uff09\u548c\u8bed\u6cd5\u53ef\u63a5\u53d7\u6027\u5224\u65ad\uff08COLA\uff09\u4efb\u52a1\uff0c\u5728\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\u3002", "motivation": "GEC\u548cCOLA\u4efb\u52a1\u5171\u4eab\u8bed\u6cd5\u77e5\u8bc6\u4f46\u72ec\u7acb\u53d1\u5c55\uff0c\u901a\u8fc7\u53cc\u5411\u6846\u67b6\u5b9e\u73b0\u77e5\u8bc6\u4e92\u8865\u3002", "method": "1. \u4f7f\u7528GEC\u6570\u636e\u589e\u5f3a\u8bed\u6cd5\u53ef\u63a5\u53d7\u6027\u6a21\u578b\uff1b2. \u901a\u8fc7\u52a8\u6001\u635f\u5931\u51fd\u6570\u5c06\u8bed\u6cd5\u53ef\u63a5\u53d7\u6027\u4fe1\u53f7\u878d\u5165GEC\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u4f46\u4ecd\u5b58\u5728\u6807\u70b9\u9519\u8bef\u7ea0\u6b63\u7b49\u6311\u6218\u3002", "conclusion": "COLA-GEC\u6846\u67b6\u6709\u6548\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u8bed\u6cd5\u5efa\u6a21\u3002"}}
{"id": "2507.11638", "pdf": "https://arxiv.org/pdf/2507.11638", "abs": "https://arxiv.org/abs/2507.11638", "authors": ["Benjamin Keel", "Aaron Quyn", "David Jayne", "Maryam Mohsin", "Samuel D. Relton"], "title": "Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Published in Medical Image Understanding and Analysis (MIUA) 2025", "summary": "Effective treatment for rectal cancer relies on accurate lymph node\nmetastasis (LNM) staging. However, radiological criteria based on lymph node\n(LN) size, shape and texture morphology have limited diagnostic accuracy. In\nthis work, we investigate applying a Variational Autoencoder (VAE) as a feature\nencoder model to replace the large pre-trained Convolutional Neural Network\n(CNN) used in existing approaches. The motivation for using a VAE is that the\ngenerative model aims to reconstruct the images, so it directly encodes visual\nfeatures and meaningful patterns across the data. This leads to a disentangled\nand structured latent space which can be more interpretable than a CNN. Models\nare deployed on an in-house MRI dataset with 168 patients who did not undergo\nneo-adjuvant treatment. The post-operative pathological N stage was used as the\nground truth to evaluate model predictions. Our proposed model 'VAE-MLP'\nachieved state-of-the-art performance on the MRI dataset, with cross-validated\nmetrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85\n+/- 0.05. Code is available at:\nhttps://github.com/benkeel/Lymph_Node_Classification_MIUA.", "AI": {"tldr": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u66ff\u4ee3\u4f20\u7edfCNN\uff0c\u63d0\u9ad8\u76f4\u80a0\u764c\u6dcb\u5df4\u7ed3\u8f6c\u79fb\uff08LNM\uff09\u5206\u671f\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6dcb\u5df4\u7ed3\u5927\u5c0f\u3001\u5f62\u72b6\u548c\u7eb9\u7406\u7684\u653e\u5c04\u5b66\u6807\u51c6\u8bca\u65ad\u51c6\u786e\u6027\u6709\u9650\uff0cVAE\u80fd\u76f4\u63a5\u7f16\u7801\u89c6\u89c9\u7279\u5f81\uff0c\u751f\u6210\u89e3\u8026\u4e14\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u7a7a\u95f4\u3002", "method": "\u63d0\u51faVAE-MLP\u6a21\u578b\uff0c\u5e94\u7528\u4e8e168\u4f8b\u672a\u63a5\u53d7\u65b0\u8f85\u52a9\u6cbb\u7597\u7684\u60a3\u8005\u7684MRI\u6570\u636e\u96c6\uff0c\u4ee5\u672f\u540e\u75c5\u7406N\u5206\u671f\u4e3a\u771f\u5b9e\u6807\u7b7e\u3002", "result": "VAE-MLP\u5728MRI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cAUC\u4e3a0.86 +/- 0.05\uff0c\u654f\u611f\u60270.79 +/- 0.06\uff0c\u7279\u5f02\u60270.85 +/- 0.05\u3002", "conclusion": "VAE-MLP\u5728LNM\u5206\u671f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5177\u6709\u6f5c\u5728\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.11875", "pdf": "https://arxiv.org/pdf/2507.11875", "abs": "https://arxiv.org/abs/2507.11875", "authors": ["Tianyou Huang", "Xinglu Chen", "Jingshen Zhang", "Xinying Qiu", "Ruiying Niu"], "title": "DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation", "categories": ["cs.CL"], "comment": "Accepted to CCL 2025", "summary": "This paper introduces DualReward, a novel reinforcement learning framework\nfor automatic distractor generation in cloze tests. Unlike conventional\napproaches that rely primarily on supervised learning or static generative\nmodels, our method employs a dual reward structure with adaptive scaling that\ndifferentiates between human-created gold standard distractors and\nmodel-generated candidates. The framework dynamically adjusts reward signal\nintensity based on model performance and confidence. We evaluate our approach\non both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,\ndemonstrating consistent improvements over state-of-the-art baselines.\nExperimental results show that our adaptive reward scaling mechanism provides\nmodest but consistent benefits on homogeneous datasets (CLOTH-F) and more\nsubstantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data\n(MCQ), suggesting its particular effectiveness for handling varied question\ntypes and domains. Our work offers a flexible framework that effectively\nbalances learning from reliable human examples while exploring novel,\nhigh-quality distractors for automated test generation.", "AI": {"tldr": "DualReward\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u5b8c\u5f62\u586b\u7a7a\u6d4b\u8bd5\u4e2d\u7684\u5e72\u6270\u9879\uff0c\u901a\u8fc7\u53cc\u5956\u52b1\u7ed3\u6784\u548c\u81ea\u9002\u5e94\u7f29\u653e\u673a\u5236\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\u6216\u9759\u6001\u751f\u6210\u6a21\u578b\uff0c\u65e0\u6cd5\u52a8\u6001\u533a\u5206\u4eba\u5de5\u6807\u51c6\u5e72\u6270\u9879\u548c\u6a21\u578b\u751f\u6210\u5019\u9009\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u6846\u67b6\u3002", "method": "\u91c7\u7528\u53cc\u5956\u52b1\u7ed3\u6784\uff0c\u52a8\u6001\u8c03\u6574\u5956\u52b1\u4fe1\u53f7\u5f3a\u5ea6\uff0c\u57fa\u4e8e\u6a21\u578b\u8868\u73b0\u548c\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5728CLOTH-F\u548cMCQ\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u8de8\u57df\u6570\u636e\u4e0a\u63d0\u5347\u663e\u8457\uff08P@1\u63d0\u9ad83.48-3.86%\uff09\u3002", "conclusion": "DualReward\u6846\u67b6\u80fd\u6709\u6548\u5e73\u8861\u4ece\u53ef\u9760\u4eba\u5de5\u793a\u4f8b\u4e2d\u5b66\u4e60\u4e0e\u63a2\u7d22\u9ad8\u8d28\u91cf\u5e72\u6270\u9879\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u6d4b\u8bd5\u751f\u6210\u3002"}}
{"id": "2507.11642", "pdf": "https://arxiv.org/pdf/2507.11642", "abs": "https://arxiv.org/abs/2507.11642", "authors": ["Abhishek Jaiswal", "Nisheeth Srivastava"], "title": "Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Posture-based mental state inference has significant potential in diagnosing\nfatigue, preventing injury, and enhancing performance across various domains.\nSuch tools must be research-validated with large datasets before being\ntranslated into practice. Unfortunately, such vision diagnosis faces serious\nchallenges due to the sensitivity of human subject data. To address this, we\nidentify sports settings as a viable alternative for accumulating data from\nhuman subjects experiencing diverse emotional states. We test our hypothesis in\nthe game of cricket and present a posture-based solution to identify human\nintent from activity videos. Our method achieves over 75\\% F1 score and over\n80\\% AUC-ROC in discriminating aggressive and defensive shot intent through\nmotion analysis. These findings indicate that posture leaks out strong signals\nfor intent inference, even with inherent noise in the data pipeline.\nFurthermore, we utilize existing data statistics as weak supervision to\nvalidate our findings, offering a potential solution for overcoming data\nlabelling limitations. This research contributes to generalizable techniques\nfor sports analytics and also opens possibilities for applying human behavior\nanalysis across various fields.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u59ff\u52bf\u7684\u5fc3\u7406\u72b6\u6001\u63a8\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u677f\u7403\u8fd0\u52a8\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u901a\u8fc7\u59ff\u52bf\u63a8\u65ad\u5fc3\u7406\u72b6\u6001\u5728\u8bca\u65ad\u75b2\u52b3\u3001\u9884\u9632\u4f24\u5bb3\u548c\u63d0\u5347\u8868\u73b0\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u6570\u636e\u654f\u611f\u6027\u95ee\u9898\u3002", "method": "\u5229\u7528\u677f\u7403\u8fd0\u52a8\u4e2d\u7684\u6d3b\u52a8\u89c6\u9891\uff0c\u901a\u8fc7\u8fd0\u52a8\u5206\u6790\u8bc6\u522b\u653b\u51fb\u6027\u548c\u9632\u5fa1\u6027\u51fb\u7403\u610f\u56fe\u3002", "result": "\u65b9\u6cd5\u5728\u533a\u5206\u610f\u56fe\u4e0a\u8fbe\u523075% F1\u5206\u6570\u548c80% AUC-ROC\u3002", "conclusion": "\u59ff\u52bf\u5206\u6790\u53ef\u7528\u4e8e\u610f\u56fe\u63a8\u65ad\uff0c\u4e14\u5f31\u76d1\u7763\u65b9\u6cd5\u80fd\u514b\u670d\u6570\u636e\u6807\u6ce8\u9650\u5236\uff0c\u4e3a\u4f53\u80b2\u5206\u6790\u548c\u5176\u4ed6\u9886\u57df\u7684\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2507.11878", "pdf": "https://arxiv.org/pdf/2507.11878", "abs": "https://arxiv.org/abs/2507.11878", "authors": ["Jiachen Zhao", "Jing Huang", "Zhengxuan Wu", "David Bau", "Weiyan Shi"], "title": "LLMs Encode Harmfulness and Refusal Separately", "categories": ["cs.CL"], "comment": null, "summary": "LLMs are trained to refuse harmful instructions, but do they truly understand\nharmfulness beyond just refusing? Prior work has shown that LLMs' refusal\nbehaviors can be mediated by a one-dimensional subspace, i.e., a refusal\ndirection. In this work, we identify a new dimension to analyze safety\nmechanisms in LLMs, i.e., harmfulness, which is encoded internally as a\nseparate concept from refusal. There exists a harmfulness direction that is\ndistinct from the refusal direction. As causal evidence, steering along the\nharmfulness direction can lead LLMs to interpret harmless instructions as\nharmful, but steering along the refusal direction tends to elicit refusal\nresponses directly without reversing the model's judgment on harmfulness.\nFurthermore, using our identified harmfulness concept, we find that certain\njailbreak methods work by reducing the refusal signals without reversing the\nmodel's internal belief of harmfulness. We also find that adversarially\nfinetuning models to accept harmful instructions has minimal impact on the\nmodel's internal belief of harmfulness. These insights lead to a practical\nsafety application: The model's latent harmfulness representation can serve as\nan intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing\nover-refusals that is robust to finetuning attacks. For instance, our Latent\nGuard achieves performance comparable to or better than Llama Guard 3 8B, a\ndedicated finetuned safeguard model, across different jailbreak methods. Our\nfindings suggest that LLMs' internal understanding of harmfulness is more\nrobust than their refusal decision to diverse input instructions, offering a\nnew perspective to study AI safety", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5185\u90e8\u5b58\u5728\u72ec\u7acb\u7684\u2018\u6709\u5bb3\u6027\u2019\u6982\u5ff5\uff0c\u4e0e\u2018\u62d2\u7edd\u2019\u884c\u4e3a\u5206\u79bb\uff0c\u53ef\u7528\u4e8e\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u5b89\u5168\u673a\u5236\u3002", "motivation": "\u63a2\u7a76LLMs\u662f\u5426\u771f\u6b63\u7406\u89e3\u2018\u6709\u5bb3\u6027\u2019\uff0c\u800c\u975e\u4ec5\u673a\u68b0\u62d2\u7edd\u6307\u4ee4\u3002", "method": "\u8bc6\u522b\u2018\u6709\u5bb3\u6027\u2019\u65b9\u5411\uff0c\u5206\u6790\u5176\u4e0e\u2018\u62d2\u7edd\u2019\u65b9\u5411\u7684\u5dee\u5f02\uff0c\u5e76\u9a8c\u8bc1\u5176\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u2018\u6709\u5bb3\u6027\u2019\u65b9\u5411\u72ec\u7acb\u5b58\u5728\uff0c\u67d0\u4e9b\u8d8a\u72f1\u65b9\u6cd5\u4ec5\u524a\u5f31\u62d2\u7edd\u4fe1\u53f7\u800c\u4e0d\u6539\u53d8\u6a21\u578b\u5bf9\u6709\u5bb3\u6027\u7684\u5224\u65ad\u3002", "conclusion": "LLMs\u5bf9\u6709\u5bb3\u6027\u7684\u5185\u90e8\u7406\u89e3\u6bd4\u62d2\u7edd\u884c\u4e3a\u66f4\u9c81\u68d2\uff0c\u4e3aAI\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.11653", "pdf": "https://arxiv.org/pdf/2507.11653", "abs": "https://arxiv.org/abs/2507.11653", "authors": ["Hannah Shafferman", "Annika Thomas", "Jouko Kinnari", "Michael Ricard", "Jose Nino", "Jonathan How"], "title": "VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization", "categories": ["cs.CV", "cs.RO"], "comment": "9 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Global localization is critical for autonomous navigation, particularly in\nscenarios where an agent must localize within a map generated in a different\nsession or by another agent, as agents often have no prior knowledge about the\ncorrelation between reference frames. However, this task remains challenging in\nunstructured environments due to appearance changes induced by viewpoint\nvariation, seasonal changes, spatial aliasing, and occlusions -- known failure\nmodes for traditional place recognition methods. To address these challenges,\nwe propose VISTA (View-Invariant Segmentation-Based Tracking for Frame\nAlignment), a novel open-set, monocular global localization framework that\ncombines: 1) a front-end, object-based, segmentation and tracking pipeline,\nfollowed by 2) a submap correspondence search, which exploits geometric\nconsistencies between environment maps to align vehicle reference frames. VISTA\nenables consistent localization across diverse camera viewpoints and seasonal\nchanges, without requiring any domain-specific training or finetuning. We\nevaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a\n69% improvement in recall over baseline methods. Furthermore, we maintain a\ncompact object-based map that is only 0.6% the size of the most\nmemory-conservative baseline, making our approach capable of real-time\nimplementation on resource-constrained platforms.", "AI": {"tldr": "VISTA\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5355\u76ee\u5168\u5c40\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8c61\u5206\u5272\u548c\u8ddf\u8e2a\u7ed3\u5408\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e86\u65e0\u7ed3\u6784\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u65e0\u7ed3\u6784\u73af\u5883\u4e2d\u56e0\u89c6\u89d2\u53d8\u5316\u3001\u5b63\u8282\u53d8\u5316\u7b49\u56e0\u7d20\u5bfc\u81f4\u7684\u5b9a\u4f4d\u5931\u8d25\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5bf9\u8c61\u5206\u5272\u3001\u8ddf\u8e2a\u548c\u5b50\u56fe\u5bf9\u5e94\u641c\u7d22\uff0c\u5229\u7528\u51e0\u4f55\u4e00\u81f4\u6027\u5bf9\u9f50\u53c2\u8003\u5e27\u3002", "result": "\u5728\u5b63\u8282\u548c\u659c\u89d2\u822a\u7a7a\u6570\u636e\u96c6\u4e0a\uff0c\u53ec\u56de\u7387\u63d0\u534769%\uff0c\u5730\u56fe\u5927\u5c0f\u4ec5\u4e3a\u57fa\u7ebf\u65b9\u6cd5\u76840.6%\u3002", "conclusion": "VISTA\u65e0\u9700\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e86\u8de8\u89c6\u89d2\u548c\u5b63\u8282\u7684\u7a33\u5b9a\u5b9a\u4f4d\u3002"}}
{"id": "2507.11882", "pdf": "https://arxiv.org/pdf/2507.11882", "abs": "https://arxiv.org/abs/2507.11882", "authors": ["Bo Zeng", "Chenyang Lyu", "Sinuo Liu", "Mingyan Zeng", "Minghao Wu", "Xuanfan Ni", "Tianqi Shi", "Yu Zhao", "Yefeng Liu", "Chenyu Zhu", "Ruizhe Li", "Jiahui Geng", "Qing Li", "Yu Tong", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference paper", "summary": "Instruction-following capability has become a major ability to be evaluated\nfor Large Language Models (LLMs). However, existing datasets, such as IFEval,\nare either predominantly monolingual and centered on English or simply machine\ntranslated to other languages, limiting their applicability in multilingual\ncontexts. In this paper, we present an carefully-curated extension of IFEval to\na localized multilingual version named Marco-Bench-MIF, covering 30 languages\nwith varying levels of localization. Our benchmark addresses linguistic\nconstraints (e.g., modifying capitalization requirements for Chinese) and\ncultural references (e.g., substituting region-specific company names in\nprompts) via a hybrid pipeline combining translation with verification. Through\ncomprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)\n25-35% accuracy gap between high/low-resource languages, (2) model scales\nlargely impact performance by 45-60% yet persists script-specific challenges,\nand (3) machine-translated data underestimates accuracy by7-22% versus\nlocalized data. Our analysis identifies challenges in multilingual instruction\nfollowing, including keyword consistency preservation and compositional\nconstraint adherence across languages. Our Marco-Bench-MIF is available at\nhttps://github.com/AIDC-AI/Marco-Bench-MIF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6Marco-Bench-MIF\uff0c\u8986\u76d630\u79cd\u8bed\u8a00\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u8bed\u8a00\u548c\u6587\u5316\u9002\u5e94\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6307\u4ee4\u8ddf\u968f\u8bc4\u4f30\u6570\u636e\u96c6\u591a\u4e3a\u5355\u8bed\uff08\u82f1\u8bed\uff09\u6216\u7b80\u5355\u673a\u5668\u7ffb\u8bd1\uff0c\u9650\u5236\u4e86\u591a\u8bed\u8a00\u573a\u666f\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u7ffb\u8bd1\u4e0e\u9a8c\u8bc1\u7684\u6df7\u5408\u6d41\u7a0b\uff0c\u521b\u5efa\u4e86\u672c\u5730\u5316\u7684\u591a\u8bed\u8a00\u7248\u672cMarco-Bench-MIF\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u95f4\u5b58\u572825-35%\u7684\u51c6\u786e\u7387\u5dee\u8ddd\uff0c\u6a21\u578b\u89c4\u6a21\u5f71\u54cd\u663e\u8457\uff0845-60%\uff09\uff0c\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u4f4e\u4f30\u4e867-22%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u7684\u6311\u6218\uff0c\u5982\u5173\u952e\u8bcd\u4e00\u81f4\u6027\u548c\u8de8\u8bed\u8a00\u7ea6\u675f\u9075\u5faa\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5f00\u53ef\u7528\u7684\u57fa\u51c6\u3002"}}
{"id": "2507.11730", "pdf": "https://arxiv.org/pdf/2507.11730", "abs": "https://arxiv.org/abs/2507.11730", "authors": ["Maciej Szankin", "Vidhyananth Venkatasamy", "Lihang Ying"], "title": "Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Outdoor advertisements remain a critical medium for modern marketing, yet\naccurately verifying billboard text visibility under real-world conditions is\nstill challenging. Traditional Optical Character Recognition (OCR) pipelines\nexcel at cropped text recognition but often struggle with complex outdoor\nscenes, varying fonts, and weather-induced visual noise. Recently, multimodal\nVision-Language Models (VLMs) have emerged as promising alternatives, offering\nend-to-end scene understanding with no explicit detection step. This work\nsystematically benchmarks representative VLMs - including Qwen 2.5 VL 3B,\nInternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline\n(PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with\nsynthetic weather distortions to simulate realistic degradation. Our results\nreveal that while selected VLMs excel at holistic scene reasoning, lightweight\nCNN pipelines still achieve competitive accuracy for cropped text at a fraction\nof the computational cost-an important consideration for edge deployment. To\nfoster future research, we release our weather-augmented benchmark and\nevaluation code publicly.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e0e\u4f20\u7edfCNN-based OCR\u5728\u6237\u5916\u5e7f\u544a\u6587\u672c\u8bc6\u522b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0VLMs\u5728\u6574\u4f53\u573a\u666f\u7406\u89e3\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8f7b\u91cf\u7ea7CNN\u5728\u88c1\u526a\u6587\u672c\u8bc6\u522b\u4e0a\u4ecd\u5177\u7ade\u4e89\u529b\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u6237\u5916\u5e7f\u544a\u6587\u672c\u8bc6\u522b\u5728\u590d\u6742\u573a\u666f\u4e2d\u4ecd\u5177\u6311\u6218\u6027\uff0c\u4f20\u7edfOCR\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u65b0\u5174\u7684VLMs\u53ef\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86Qwen 2.5 VL 3B\u3001InternVL3\u548cSmolVLM2\u7b49VLMs\u4e0ePaddleOCRv4\u5728ICDAR 2015\u548cSVT\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u52a0\u5165\u5408\u6210\u5929\u6c14\u566a\u58f0\u6a21\u62df\u771f\u5b9e\u573a\u666f\u3002", "result": "VLMs\u5728\u6574\u4f53\u573a\u666f\u7406\u89e3\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u8f7b\u91cf\u7ea7CNN\u5728\u88c1\u526a\u6587\u672c\u8bc6\u522b\u4e0a\u4ecd\u5177\u7ade\u4e89\u529b\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "VLMs\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46CNN\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u4ecd\u5177\u4f18\u52bf\uff0c\u9002\u5408\u8fb9\u7f18\u90e8\u7f72\u3002\u7814\u7a76\u516c\u5f00\u4e86\u5929\u6c14\u589e\u5f3a\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2507.11936", "pdf": "https://arxiv.org/pdf/2507.11936", "abs": "https://arxiv.org/abs/2507.11936", "authors": ["Jianzhe Ma", "Wenxuan Wang", "Qin Jin"], "title": "A Survey of Deep Learning for Geometry Problem Solving", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Work in progress", "summary": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u4efb\u52a1\u603b\u7ed3\u3001\u65b9\u6cd5\u56de\u987e\u3001\u8bc4\u4f30\u6307\u6807\u5206\u6790\u53ca\u672a\u6765\u65b9\u5411\u63a2\u8ba8\u3002", "motivation": "\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u662f\u6570\u5b66\u63a8\u7406\u7684\u5173\u952e\u9886\u57df\uff0c\u6d89\u53ca\u6559\u80b2\u548c\u4eba\u5de5\u667a\u80fd\u80fd\u529b\u8bc4\u4f30\u7b49\u591a\u4e2a\u91cd\u8981\u9886\u57df\u3002\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u7684\u53d1\u5c55\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u70ed\u6f6e\u3002", "method": "\u7efc\u8ff0\u4e86\u76f8\u5173\u4efb\u52a1\u3001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3001\u8bc4\u4f30\u6307\u6807\u548c\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "result": "\u63d0\u4f9b\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u5168\u9762\u53c2\u8003\uff0c\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "conclusion": "\u672c\u6587\u4e3a\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.11761", "pdf": "https://arxiv.org/pdf/2507.11761", "abs": "https://arxiv.org/abs/2507.11761", "authors": ["Fan Shi", "Bin Li", "Xiangyang Xue"], "title": "Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Abstract visual reasoning (AVR) enables humans to quickly discover and\ngeneralize abstract rules to new scenarios. Designing intelligent systems with\nhuman-like AVR abilities has been a long-standing topic in the artificial\nintelligence community. Deep AVR solvers have recently achieved remarkable\nsuccess in various AVR tasks. However, they usually use task-specific designs\nor parameters in different tasks. In such a paradigm, solving new tasks often\nmeans retraining the model, and sometimes retuning the model architectures,\nwhich increases the cost of solving AVR problems. In contrast to task-specific\napproaches, this paper proposes a novel Unified Conditional Generative Solver\n(UCGS), aiming to address multiple AVR tasks in a unified framework. First, we\nprove that some well-known AVR tasks can be reformulated as the problem of\nestimating the predictability of target images in problem panels. Then, we\nillustrate that, under the proposed framework, training one conditional\ngenerative model can solve various AVR tasks. The experiments show that with a\nsingle round of multi-task training, UCGS demonstrates abstract reasoning\nability across various AVR tasks. Especially, UCGS exhibits the ability of\nzero-shot reasoning, enabling it to perform abstract reasoning on problems from\nunseen AVR tasks in the testing phase.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6761\u4ef6\u751f\u6210\u6c42\u89e3\u5668\uff08UCGS\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u79cd\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\uff08AVR\uff09\u4efb\u52a1\uff0c\u907f\u514d\u4e86\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\u6216\u53c2\u6570\u8c03\u6574\u7684\u9700\u6c42\u3002", "motivation": "\u8bbe\u8ba1\u5177\u6709\u4eba\u7c7b\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u7684\u667a\u80fd\u7cfb\u7edf\u662f\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u957f\u671f\u8bfe\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u8fdb\u884c\u7279\u5b9a\u8bbe\u8ba1\u6216\u91cd\u65b0\u8bad\u7ec3\uff0c\u589e\u52a0\u4e86\u6210\u672c\u3002", "method": "\u5c06\u4e00\u4e9b\u77e5\u540dAVR\u4efb\u52a1\u91cd\u65b0\u8868\u8ff0\u4e3a\u76ee\u6807\u56fe\u50cf\u5728\u95ee\u9898\u9762\u677f\u4e2d\u7684\u53ef\u9884\u6d4b\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u6761\u4ef6\u751f\u6210\u6a21\u578b\u6765\u89e3\u51b3\u591a\u79cd\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUCGS\u901a\u8fc7\u591a\u4efb\u52a1\u8bad\u7ec3\uff0c\u80fd\u591f\u5728\u591a\u79cdAVR\u4efb\u52a1\u4e2d\u5c55\u793a\u62bd\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5177\u5907\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "UCGS\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u89e3\u51b3\u591a\u79cdAVR\u4efb\u52a1\uff0c\u4e14\u5177\u5907\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\uff0c\u964d\u4f4e\u4e86\u89e3\u51b3AVR\u95ee\u9898\u7684\u6210\u672c\u3002"}}
{"id": "2507.11939", "pdf": "https://arxiv.org/pdf/2507.11939", "abs": "https://arxiv.org/abs/2507.11939", "authors": ["Yichen Xu", "Liangyu Chen", "Liang Zhang", "Wenxuan Wang", "Qin Jin"], "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "comment": "Work in Progress", "summary": "Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.", "AI": {"tldr": "PolyChartQA\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u56fe\u8868\u95ee\u7b54\u57fa\u51c6\uff0c\u8986\u76d610\u79cd\u8bed\u8a00\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u56fe\u8868\u7406\u89e3\u57fa\u51c6\u7684\u82f1\u8bed\u4e2d\u5fc3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u8868\u7406\u89e3\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\uff0c\u9650\u5236\u4e86\u5176\u5168\u7403\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u89e3\u8026\u7ba1\u9053\uff0c\u5206\u79bb\u56fe\u8868\u6570\u636e\u4e0e\u6e32\u67d3\u4ee3\u7801\uff0c\u5229\u7528LLM\u7ffb\u8bd1\u548c\u4e25\u683c\u8d28\u91cf\u63a7\u5236\u751f\u6210\u591a\u8bed\u8a00\u56fe\u8868\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u82f1\u8bed\u4e0e\u5176\u4ed6\u8bed\u8a00\uff08\u5c24\u5176\u662f\u975e\u62c9\u4e01\u6587\u5b57\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "PolyChartQA\u4e3a\u63a8\u8fdb\u5168\u7403\u5305\u5bb9\u6027\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.11834", "pdf": "https://arxiv.org/pdf/2507.11834", "abs": "https://arxiv.org/abs/2507.11834", "authors": ["Peiwen Xia", "Tangfei Liao", "Wei Zhu", "Danhuai Zhao", "Jianjun Ke", "Kaihao Zhang", "Tong Lu", "Tao Wang"], "title": "CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning", "categories": ["cs.CV"], "comment": "Accepted by ECAI 2025", "summary": "Establishing reliable correspondences between image pairs is a fundamental\ntask in computer vision, underpinning applications such as 3D reconstruction\nand visual localization. Although recent methods have made progress in pruning\noutliers from dense correspondence sets, they often hypothesize consistent\nvisual domains and overlook the challenges posed by diverse scene structures.\nIn this paper, we propose CorrMoE, a novel correspondence pruning framework\nthat enhances robustness under cross-domain and cross-scene variations. To\naddress domain shift, we introduce a De-stylization Dual Branch, performing\nstyle mixing on both implicit and explicit graph features to mitigate the\nadverse influence of domain-specific representations. For scene diversity, we\ndesign a Bi-Fusion Mixture of Experts module that adaptively integrates\nmulti-perspective features through linear-complexity attention and dynamic\nexpert routing. Extensive experiments on benchmark datasets demonstrate that\nCorrMoE achieves superior accuracy and generalization compared to\nstate-of-the-art methods. The code and pre-trained models are available at\nhttps://github.com/peiwenxia/CorrMoE.", "AI": {"tldr": "CorrMoE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u5e94\u70b9\u4fee\u526a\u6846\u67b6\uff0c\u901a\u8fc7\u53bb\u98ce\u683c\u5316\u53cc\u5206\u652f\u548c\u53cc\u878d\u5408\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff0c\u589e\u5f3a\u4e86\u8de8\u57df\u548c\u8de8\u573a\u666f\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6837\u573a\u666f\u7ed3\u6784\u548c\u8de8\u57df\u53d8\u5316\u4e2d\u8868\u73b0\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53bb\u98ce\u683c\u5316\u53cc\u5206\u652f\u5904\u7406\u57df\u504f\u79fb\uff0c\u53cc\u878d\u5408\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u9002\u5e94\u573a\u666f\u591a\u6837\u6027\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CorrMoE\u5728\u8de8\u57df\u548c\u8de8\u573a\u666f\u53d8\u5316\u4e0b\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2507.11941", "pdf": "https://arxiv.org/pdf/2507.11941", "abs": "https://arxiv.org/abs/2507.11941", "authors": ["Amos You"], "title": "BlockBPE: Parallel BPE Tokenization", "categories": ["cs.CL", "cs.DC"], "comment": "ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models\n  (ICML 2025)", "summary": "Tokenization is a critical preprocessing step in large language model\npipelines, yet widely-used implementations remain CPU-bound and suboptimal for\nbatch inference workflows on GPU. We present BlockBPE, a parallel GPU\nimplementation of byte-pair encoding (BPE) that achieves near linear-time\ncomplexity under realistic assumptions and is optimized for high-throughput,\nbatch inference. Unlike existing Rust-based tokenizers such as HuggingFace\nTokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex\npre-tokenization and exhibit $O(n \\log n)$ runtime-BlockBPE eliminates the\nRegex pre-tokenization which leads to small loss in generation quality, but\nenables highly parallelized token merges within thread blocks, reducing overall\ncomplexity to $O(nd)$ where $d \\ll n$. On high-batch inference workloads,\nBlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over\nHuggingFace Tokenizers.", "AI": {"tldr": "BlockBPE\u662f\u4e00\u79cd\u5e76\u884cGPU\u5b9e\u73b0\u7684BPE\u7b97\u6cd5\uff0c\u4f18\u5316\u4e86\u9ad8\u541e\u5410\u91cf\u6279\u91cf\u63a8\u7406\uff0c\u76f8\u6bd4\u73b0\u6709CPU\u7ed1\u5b9a\u7684\u5b9e\u73b0\uff08\u5982tiktoken\u548cHuggingFace Tokenizers\uff09\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u73b0\u6709BPE\u5b9e\u73b0\uff08\u5982tiktoken\u548cHuggingFace Tokenizers\uff09\u5728CPU\u4e0a\u8fd0\u884c\u6548\u7387\u4f4e\uff0c\u4e14\u4e0d\u9002\u5408GPU\u6279\u91cf\u63a8\u7406\uff0cBlockBPE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "BlockBPE\u901a\u8fc7\u6d88\u9664Regex\u9884\u5206\u8bcd\u6b65\u9aa4\uff0c\u5b9e\u73b0\u9ad8\u5ea6\u5e76\u884c\u5316\u7684\u5206\u8bcd\u5408\u5e76\uff0c\u5c06\u590d\u6742\u5ea6\u4eceO(n log n)\u964d\u4f4e\u5230O(nd)\u3002", "result": "\u5728\u9ad8\u6279\u91cf\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cBlockBPE\u7684\u541e\u5410\u91cf\u6bd4tiktoken\u9ad82\u500d\uff0c\u6bd4HuggingFace Tokenizers\u9ad82.5\u500d\u3002", "conclusion": "BlockBPE\u4e3aGPU\u4e0a\u7684\u9ad8\u6548\u5206\u8bcd\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\uff0c\u9002\u5408\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002"}}
{"id": "2507.11845", "pdf": "https://arxiv.org/pdf/2507.11845", "abs": "https://arxiv.org/abs/2507.11845", "authors": ["Kexuan Shi", "Zhuang Qi", "Jingjing Zhu", "Lei Meng", "Yaochen Zhang", "Haibei Huang", "Xiangxu Meng"], "title": "ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification", "categories": ["cs.CV"], "comment": "Accepted in ChinaMM and recommended to Displays", "summary": "Open-set few-shot image classification aims to train models using a small\namount of labeled data, enabling them to achieve good generalization when\nconfronted with unknown environments. Existing methods mainly use visual\ninformation from a single image to learn class representations to distinguish\nknown from unknown categories. However, these methods often overlook the\nbenefits of integrating rich contextual information. To address this issue,\nthis paper proposes a prototypical augmentation and alignment method, termed\nProtoConNet, which incorporates background information from different samples\nto enhance the diversity of the feature space, breaking the spurious\nassociations between context and image subjects in few-shot scenarios.\nSpecifically, it consists of three main modules: the clustering-based data\nselection (CDS) module mines diverse data patterns while preserving core\nfeatures; the contextual-enhanced semantic refinement (CSR) module builds a\ncontext dictionary to integrate into image representations, which boosts the\nmodel's robustness in various scenarios; and the prototypical alignment (PA)\nmodule reduces the gap between image representations and class prototypes,\namplifying feature distances for known and unknown classes. Experimental\nresults from two datasets verified that ProtoConNet enhances the effectiveness\nof representation learning in few-shot scenarios and identifies open-set\nsamples, making it superior to existing methods.", "AI": {"tldr": "ProtoConNet\u901a\u8fc7\u6574\u5408\u80cc\u666f\u4fe1\u606f\u589e\u5f3a\u7279\u5f81\u7a7a\u95f4\u591a\u6837\u6027\uff0c\u63d0\u5347\u5c11\u6837\u672c\u5f00\u653e\u96c6\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5355\u5f20\u56fe\u50cf\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6574\u5408\u3002", "method": "\u63d0\u51faProtoConNet\uff0c\u5305\u542b\u805a\u7c7b\u6570\u636e\u9009\u62e9\uff08CDS\uff09\u3001\u4e0a\u4e0b\u6587\u589e\u5f3a\u8bed\u4e49\u7ec6\u5316\uff08CSR\uff09\u548c\u539f\u578b\u5bf9\u9f50\uff08PA\uff09\u4e09\u4e2a\u6a21\u5757\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86ProtoConNet\u5728\u5c11\u6837\u672c\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "ProtoConNet\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u5f00\u653e\u96c6\u6837\u672c\u3002"}}
{"id": "2507.11942", "pdf": "https://arxiv.org/pdf/2507.11942", "abs": "https://arxiv.org/abs/2507.11942", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao", "Baoyuan Qi", "Guoming Liu"], "title": "DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Task-agnostic prompt compression leverages the redundancy in natural language\nto reduce computational overhead and enhance information density within\nprompts, especially in long-context scenarios. Existing methods predominantly\nrely on information entropy as the metric to compress lexical units, aiming to\nachieve minimal information loss. However, these approaches overlook two\ncritical aspects: (i) the importance of attention-critical tokens at the\nalgorithmic level, and (ii) shifts in information entropy during the\ncompression process. Motivated by these challenges, we propose a dynamic\nattention-aware approach for task-agnostic prompt compression (DAC). This\napproach effectively integrates entropy and attention information, dynamically\nsensing entropy shifts during compression to achieve fine-grained prompt\ncompression. Extensive experiments across various domains, including LongBench,\nGSM8K, and BBH, show that DAC consistently yields robust and substantial\nimprovements across a diverse range of tasks and LLMs, offering compelling\nevidence of its efficacy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6ce8\u610f\u529b\u611f\u77e5\u7684\u4efb\u52a1\u65e0\u5173\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\uff08DAC\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u71b5\u548c\u6ce8\u610f\u529b\u4fe1\u606f\uff0c\u52a8\u6001\u611f\u77e5\u538b\u7f29\u8fc7\u7a0b\u4e2d\u7684\u71b5\u53d8\u5316\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u7684\u63d0\u793a\u538b\u7f29\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4fe1\u606f\u71b5\u4f5c\u4e3a\u538b\u7f29\u6307\u6807\uff0c\u5ffd\u7565\u4e86\u6ce8\u610f\u529b\u5173\u952e\u4ee4\u724c\u548c\u538b\u7f29\u8fc7\u7a0b\u4e2d\u71b5\u7684\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6ce8\u610f\u529b\u611f\u77e5\u65b9\u6cd5\uff08DAC\uff09\uff0c\u6574\u5408\u71b5\u548c\u6ce8\u610f\u529b\u4fe1\u606f\uff0c\u52a8\u6001\u611f\u77e5\u71b5\u53d8\u5316\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982LongBench\u3001GSM8K\u3001BBH\uff09\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDAC\u5728\u4e0d\u540c\u4efb\u52a1\u548cLLMs\u4e2d\u5747\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "DAC\u65b9\u6cd5\u5728\u4efb\u52a1\u65e0\u5173\u63d0\u793a\u538b\u7f29\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.11892", "pdf": "https://arxiv.org/pdf/2507.11892", "abs": "https://arxiv.org/abs/2507.11892", "authors": ["Yu Liu", "Leyuan Qu", "Hanlei Shi", "Di Gao", "Yuhua Zheng", "Taihao Li"], "title": "From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Dynamic Facial Expression Recognition (DFER) aims to identify human emotions\nfrom temporally evolving facial movements and plays a critical role in\naffective computing. While recent vision-language approaches have introduced\nsemantic textual descriptions to guide expression recognition, existing methods\nstill face two key limitations: they often underutilize the subtle emotional\ncues embedded in generated text, and they have yet to incorporate sufficiently\neffective mechanisms for filtering out facial dynamics that are irrelevant to\nemotional expression. To address these gaps, We propose GRACE, Granular\nRepresentation Alignment for Cross-modal Emotion recognition that integrates\ndynamic motion modeling, semantic text refinement, and token-level cross-modal\nalignment to facilitate the precise localization of emotionally salient\nspatiotemporal features. Our method constructs emotion-aware textual\ndescriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and\nhighlights expression-relevant facial motion through a motion-difference\nweighting mechanism. These refined semantic and visual signals are aligned at\nthe token level using entropy-regularized optimal transport. Experiments on\nthree benchmark datasets demonstrate that our method significantly improves\nrecognition performance, particularly in challenging settings with ambiguous or\nimbalanced emotion classes, establishing new state-of-the-art (SOTA) results in\nterms of both UAR and WAR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGRACE\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8fd0\u52a8\u5efa\u6a21\u3001\u8bed\u4e49\u6587\u672c\u7ec6\u5316\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u63d0\u5347\u52a8\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6587\u672c\u4e2d\u7684\u60c5\u611f\u7ebf\u7d22\uff0c\u4e14\u7f3a\u4e4f\u8fc7\u6ee4\u65e0\u5173\u9762\u90e8\u52a8\u6001\u7684\u6709\u6548\u673a\u5236\u3002", "method": "GRACE\u7ed3\u5408\u52a8\u6001\u8fd0\u52a8\u5efa\u6a21\u3001\u8bed\u4e49\u6587\u672c\u7ec6\u5316\uff08CATE\u6a21\u5757\uff09\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u901a\u8fc7\u8fd0\u52a8\u5dee\u5f02\u52a0\u6743\u548c\u71b5\u6b63\u5219\u5316\u6700\u4f18\u4f20\u8f93\u5b9e\u73b0\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u8bc6\u522b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6a21\u7cca\u6216\u4e0d\u5e73\u8861\u60c5\u611f\u7c7b\u522b\u4e2d\uff0c\u8fbe\u5230\u65b0\u7684SOTA\u7ed3\u679c\u3002", "conclusion": "GRACE\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u60c5\u611f\u611f\u77e5\u6587\u672c\u589e\u5f3a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2507.11953", "pdf": "https://arxiv.org/pdf/2507.11953", "abs": "https://arxiv.org/abs/2507.11953", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao"], "title": "IAM: Efficient Inference through Attention Mapping between Different-scale LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "ACL 2025", "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.", "AI": {"tldr": "IAM\u6846\u67b6\u901a\u8fc7\u5229\u7528\u4e0d\u540c\u89c4\u6a21LLMs\u95f4\u6ce8\u610f\u529b\u77e9\u9635\u7684\u9ad8\u76f8\u4f3c\u6027\uff0c\u4f18\u5316\u6ce8\u610f\u529b\u8ba1\u7b97\u548cKV\u7f13\u5b58\u4f7f\u7528\uff0c\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5229\u7528\u6a21\u578b\u5185\u90e8\u7a00\u758f\u6027\uff0c\u672a\u5145\u5206\u5229\u7528\u5916\u90e8\u4fe1\u606f\u4f18\u5316\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u63d0\u51faIAM\u6846\u67b6\uff0c\u901a\u8fc7\u5c0f\u89c4\u6a21\u4e0e\u5927\u89c4\u6a21LLMs\u95f4\u7684\u6ce8\u610f\u529b\u6620\u5c04\uff0c\u52a0\u901f\u6ce8\u610f\u529b\u8ba1\u7b97\u5e76\u51cf\u5c11KV\u7f13\u5b58\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u663e\u793aIAM\u53ef\u52a0\u901f\u9884\u586b\u514515%\uff0c\u51cf\u5c11KV\u7f13\u5b5822.1%\uff0c\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "IAM\u5177\u6709\u901a\u7528\u6027\uff0c\u5e76\u4e0e\u73b0\u6709KV\u7f13\u5b58\u4f18\u5316\u65b9\u6cd5\u6b63\u4ea4\uff0c\u662f\u63d0\u5347LLM\u6548\u7387\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.11893", "pdf": "https://arxiv.org/pdf/2507.11893", "abs": "https://arxiv.org/abs/2507.11893", "authors": ["Linwei Chen", "Ying Fu", "Lin Gu", "Dezhi Zheng", "Jifeng Dai"], "title": "Spatial Frequency Modulation for Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accept by TPAMI 2025", "summary": "High spatial frequency information, including fine details like textures,\nsignificantly contributes to the accuracy of semantic segmentation. However,\naccording to the Nyquist-Shannon Sampling Theorem, high-frequency components\nare vulnerable to aliasing or distortion when propagating through downsampling\nlayers such as strided-convolution. Here, we propose a novel Spatial Frequency\nModulation (SFM) that modulates high-frequency features to a lower frequency\nbefore downsampling and then demodulates them back during upsampling.\nSpecifically, we implement modulation through adaptive resampling (ARS) and\ndesign a lightweight add-on that can densely sample the high-frequency areas to\nscale up the signal, thereby lowering its frequency in accordance with the\nFrequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling\n(MSAU) to demodulate the modulated feature and recover high-frequency\ninformation through non-uniform upsampling This module further improves\nsegmentation by explicitly exploiting information interaction between densely\nand sparsely resampled areas at multiple scales. Both modules can seamlessly\nintegrate with various architectures, extending from convolutional neural\nnetworks to transformers. Feature visualization and analysis confirm that our\nmethod effectively alleviates aliasing while successfully retaining details\nafter demodulation. Finally, we validate the broad applicability and\neffectiveness of SFM by extending it to image classification, adversarial\nrobustness, instance segmentation, and panoptic segmentation tasks. The code is\navailable at\n\\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u9891\u7387\u8c03\u5236\uff08SFM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u5236\u9ad8\u9891\u7279\u5f81\u5230\u4e0b\u91c7\u6837\u524d\u548c\u89e3\u8c03\u4e0a\u91c7\u6837\u540e\uff0c\u6709\u6548\u51cf\u5c11\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6df7\u53e0\u95ee\u9898\u3002", "motivation": "\u9ad8\u9891\u4fe1\u606f\u5bf9\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e0b\u91c7\u6837\u5c42\u53ef\u80fd\u5bfc\u81f4\u5176\u6df7\u53e0\u6216\u5931\u771f\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u91cd\u91c7\u6837\uff08ARS\uff09\u8c03\u5236\u9ad8\u9891\u7279\u5f81\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u6a21\u5757\u548c\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u4e0a\u91c7\u6837\uff08MSAU\uff09\u89e3\u8c03\u3002", "result": "\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u6df7\u53e0\u5e76\u4fdd\u7559\u7ec6\u8282\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u548c\u67b6\u6784\u3002", "conclusion": "SFM\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.11954", "pdf": "https://arxiv.org/pdf/2507.11954", "abs": "https://arxiv.org/abs/2507.11954", "authors": ["Artem Alekseev", "Mikhail Chaichuk", "Miron Butko", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "title": "The benefits of query-based KGQA systems for complex and temporal questions in LLM era", "categories": ["cs.CL", "cs.LG"], "comment": "15 pages, 3 figures, 7 tables", "summary": "Large language models excel in question-answering (QA) yet still struggle\nwith multi-hop reasoning and temporal questions. Query-based knowledge graph QA\n(KGQA) offers a modular alternative by generating executable queries instead of\ndirect answers. We explore multi-stage query-based framework for WikiData QA,\nproposing multi-stage approach that enhances performance on challenging\nmulti-hop and temporal benchmarks. Through generalization and rejection\nstudies, we evaluate robustness across multi-hop and temporal QA datasets.\nAdditionally, we introduce a novel entity linking and predicate matching method\nusing CoT reasoning. Our results demonstrate the potential of query-based\nmulti-stage KGQA framework for improving multi-hop and temporal QA with small\nlanguage models. Code and data: https://github.com/ar2max/NLDB-KGQA-System", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u9636\u6bb5\u67e5\u8be2\u7684\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u8df3\u548c\u65f6\u95f4\u95ee\u9898\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u63a8\u7406\u548c\u65f6\u95f4\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u57fa\u4e8e\u67e5\u8be2\u7684\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\uff08KGQA\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u67e5\u8be2\u6846\u67b6\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u5b9e\u4f53\u94fe\u63a5\u548c\u8c13\u8bcd\u5339\u914d\u65b9\u6cd5\uff08\u4f7f\u7528CoT\u63a8\u7406\uff09\u3002", "result": "\u5728\u591a\u8df3\u548c\u65f6\u95f4\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8e\u67e5\u8be2\u7684\u591a\u9636\u6bb5KGQA\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u591a\u8df3\u548c\u65f6\u95f4\u95ee\u9898\u7684\u6027\u80fd\u3002"}}
{"id": "2507.11900", "pdf": "https://arxiv.org/pdf/2507.11900", "abs": "https://arxiv.org/abs/2507.11900", "authors": ["Wei Sun", "Linhan Cao", "Kang Fu", "Dandan Zhu", "Jun Jia", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai"], "title": "CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos", "categories": ["eess.IV", "cs.CV"], "comment": "CompressedVQA-HDR won first place in the FR track of the\n  Generalizable HDR & SDR Video Quality Measurement Grand Challenge at IEEE\n  ICME 2025", "summary": "Video compression is a standard procedure applied to all videos to minimize\nstorage and transmission demands while preserving visual quality as much as\npossible. Therefore, evaluating the visual quality of compressed videos is\ncrucial for guiding the practical usage and further development of video\ncompression algorithms. Although numerous compressed video quality assessment\n(VQA) methods have been proposed, they often lack the generalization capability\nneeded to handle the increasing diversity of video types, particularly high\ndynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an\neffective VQA framework designed to address the challenges of HDR video quality\nassessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the\nbackbone networks for the proposed full-reference (FR) and no-reference (NR)\nVQA models, respectively. For the FR model, we compute deep structural and\ntextural similarities between reference and distorted frames using\nintermediate-layer features extracted from the Swin Transformer as its\nquality-aware feature representation. For the NR model, we extract the global\nmean of the final-layer feature maps from SigLip 2 as its quality-aware\nrepresentation. To mitigate the issue of limited HDR training data, we\npre-train the FR model on a large-scale standard dynamic range (SDR) VQA\ndataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ\nan iterative mixed-dataset training strategy across multiple compressed VQA\ndatasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental\nresults show that our models achieve state-of-the-art performance compared to\nexisting FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place\nin the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand\nChallenge at IEEE ICME 2025. The code is available at\nhttps://github.com/sunwei925/CompressedVQA-HDR.", "AI": {"tldr": "\u63d0\u51faCompressedVQA-HDR\u6846\u67b6\uff0c\u7528\u4e8eHDR\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff0c\u7ed3\u5408Swin Transformer\u548cSigLip 2\u7f51\u7edc\uff0c\u5728FR\u548cNR\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709VQA\u65b9\u6cd5\u5bf9\u591a\u6837\u5316\u89c6\u9891\u7c7b\u578b\uff08\u5c24\u5176\u662fHDR\u5185\u5bb9\uff09\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "FR\u6a21\u578b\u4f7f\u7528Swin Transformer\u63d0\u53d6\u7279\u5f81\u8ba1\u7b97\u7ed3\u6784\u76f8\u4f3c\u6027\uff1bNR\u6a21\u578b\u4f7f\u7528SigLip 2\u63d0\u53d6\u5168\u5c40\u7279\u5f81\u3002\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002", "result": "\u6a21\u578b\u5728FR\u548cNR\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0cCompressedVQA-HDR-FR\u5728IEEE ICME 2025\u6311\u6218\u8d5b\u4e2d\u83b7FR\u8d5b\u9053\u7b2c\u4e00\u540d\u3002", "conclusion": "CompressedVQA-HDR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86HDR\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.11959", "pdf": "https://arxiv.org/pdf/2507.11959", "abs": "https://arxiv.org/abs/2507.11959", "authors": ["Xinyu Wang", "Vahid Partovi Nia", "Peng Lu", "Jerry Huang", "Xiao-Wen Chang", "Boxing Chen", "Yufei Cui"], "title": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ECAI 2025 (European Conference on Artificial\n  Intelligence)", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684PoT\u91cf\u5316\u6846\u67b6\uff0c\u7528\u4e8eLLM\u6743\u91cd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u7684\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u53cd\u91cf\u5316\u52a0\u901f\u63a8\u7406\u3002", "motivation": "LLMs\u90e8\u7f72\u56e0\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709PoT\u91cf\u5316\u5728GPU\u4e0a\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u540e\u8bad\u7ec3\u7b97\u6cd5\uff1a\u521d\u59cb\u5316\u91cf\u5316\u5c3a\u5ea6\u5e76\u901a\u8fc7\u6821\u51c6\u96c6\u4f18\u5316\uff0c\u6539\u8fdbPoT\u91cf\u5316\u6846\u67b6\u3002", "result": "\u57282-\u548c3-bit\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u4e2d\u8d85\u8d8a\u73b0\u6709\u6574\u6570\u91cf\u5316\u65b9\u6cd5\uff0c\u53cd\u91cf\u5316\u901f\u5ea6\u63d0\u5347\u663e\u8457\u3002", "conclusion": "\u63d0\u51fa\u7684PoT\u91cf\u5316\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.11910", "pdf": "https://arxiv.org/pdf/2507.11910", "abs": "https://arxiv.org/abs/2507.11910", "authors": ["Kaustav Chanda", "Aayush Atul Verma", "Arpitsinh Vaghela", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "SEPose: A Synthetic Event-based Human Pose Estimation Dataset for Pedestrian Monitoring", "categories": ["cs.CV"], "comment": "Accepted at the 28th IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2025)", "summary": "Event-based sensors have emerged as a promising solution for addressing\nchallenging conditions in pedestrian and traffic monitoring systems. Their\nlow-latency and high dynamic range allow for improved response time in\nsafety-critical situations caused by distracted walking or other unusual\nmovements. However, the availability of data covering such scenarios remains\nlimited. To address this gap, we present SEPose -- a comprehensive synthetic\nevent-based human pose estimation dataset for fixed pedestrian perception\ngenerated using dynamic vision sensors in the CARLA simulator. With nearly 350K\nannotated pedestrians with body pose keypoints from the perspective of fixed\ntraffic cameras, SEPose is a comprehensive synthetic multi-person pose\nestimation dataset that spans busy and light crowds and traffic across diverse\nlighting and weather conditions in 4-way intersections in urban, suburban, and\nrural environments. We train existing state-of-the-art models such as RVT and\nYOLOv8 on our dataset and evaluate them on real event-based data to demonstrate\nthe sim-to-real generalization capabilities of the proposed dataset.", "AI": {"tldr": "SEPose\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u7684\u5408\u6210\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u56fa\u5b9a\u884c\u4eba\u611f\u77e5\uff0c\u586b\u8865\u4e86\u771f\u5b9e\u6570\u636e\u4e0d\u8db3\u7684\u7a7a\u767d\u3002", "motivation": "\u4e8b\u4ef6\u4f20\u611f\u5668\u5728\u884c\u4eba\u76d1\u63a7\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u76f8\u5173\u6570\u636e\u7a00\u7f3a\uff0cSEPose\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528CARLA\u6a21\u62df\u5668\u548c\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5305\u542b\u591a\u79cd\u573a\u666f\u4e0b\u7684\u884c\u4eba\u59ff\u6001\u6807\u6ce8\u3002", "result": "SEPose\u5305\u542b\u8fd1350K\u6807\u6ce8\u884c\u4eba\uff0c\u652f\u6301\u73b0\u6709\u6a21\u578b\uff08\u5982RVT\u548cYOLOv8\uff09\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u6cdb\u5316\u80fd\u529b\u9a8c\u8bc1\u3002", "conclusion": "SEPose\u4e3a\u4e8b\u4ef6\u4f20\u611f\u5668\u5728\u884c\u4eba\u76d1\u63a7\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6570\u636e\u652f\u6301\uff0c\u5c55\u793a\u4e86\u6a21\u62df\u6570\u636e\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.11966", "pdf": "https://arxiv.org/pdf/2507.11966", "abs": "https://arxiv.org/abs/2507.11966", "authors": ["Ziyu Ge", "Gabriel Chua", "Leanne Tan", "Roy Ka-Wei Lee"], "title": "Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As online communication increasingly incorporates under-represented languages\nand colloquial dialects, standard translation systems often fail to preserve\nlocal slang, code-mixing, and culturally embedded markers of harmful speech.\nTranslating toxic content between low-resource language pairs poses additional\nchallenges due to scarce parallel data and safety filters that sanitize\noffensive expressions. In this work, we propose a reproducible, two-stage\nframework for toxicity-preserving translation, demonstrated on a code-mixed\nSinglish safety corpus. First, we perform human-verified few-shot prompt\nengineering: we iteratively curate and rank annotator-selected Singlish-target\nexamples to capture nuanced slang, tone, and toxicity. Second, we optimize\nmodel-prompt pairs by benchmarking several large language models using semantic\nsimilarity via direct and back-translation. Quantitative human evaluation\nconfirms the effectiveness and efficiency of our pipeline. Beyond improving\ntranslation quality, our framework contributes to the safety of multicultural\nLLMs by supporting culturally sensitive moderation and benchmarking in\nlow-resource contexts. By positioning Singlish as a testbed for inclusive NLP,\nwe underscore the importance of preserving sociolinguistic nuance in real-world\napplications such as content moderation and regional platform governance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u4e2d\u4fdd\u7559\u6bd2\u6027\u5185\u5bb9\u7684\u7ffb\u8bd1\uff0c\u5e76\u4ee5\u65b0\u52a0\u5761\u82f1\u8bed\u4e3a\u4f8b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u7ebf\u4ea4\u6d41\u4e2d\uff0c\u6807\u51c6\u7ffb\u8bd1\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4fda\u8bed\u3001\u6df7\u5408\u8bed\u548c\u6587\u5316\u5d4c\u5165\u7684\u6709\u5bb3\u8a00\u8bba\u6807\u8bb0\uff0c\u4e9f\u9700\u4e00\u79cd\u4fdd\u7559\u6bd2\u6027\u5185\u5bb9\u7684\u7ffb\u8bd1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4eba\u5de5\u9a8c\u8bc1\u7684\u5c11\u6837\u672c\u63d0\u793a\u5de5\u7a0b\uff0c\u7b5b\u9009\u65b0\u52a0\u5761\u82f1\u8bed\u793a\u4f8b\uff1b2) \u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4f18\u5316\u6a21\u578b-\u63d0\u793a\u5bf9\u3002", "result": "\u5b9a\u91cf\u4eba\u5de5\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u7ffb\u8bd1\u8d28\u91cf\u548c\u591a\u6587\u5316LLM\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5f3a\u8c03\u4e86\u5728\u5185\u5bb9\u5ba1\u6838\u548c\u5e73\u53f0\u6cbb\u7406\u4e2d\u4fdd\u7559\u793e\u4f1a\u8bed\u8a00\u7ec6\u5fae\u5dee\u522b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4ee5\u65b0\u52a0\u5761\u82f1\u8bed\u4e3a\u4f8b\u63a8\u52a8\u4e86\u5305\u5bb9\u6027NLP\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.11931", "pdf": "https://arxiv.org/pdf/2507.11931", "abs": "https://arxiv.org/abs/2507.11931", "authors": ["Jingqian Wu", "Peiqi Duan", "Zongqiang Wang", "Changwei Wang", "Boxin Shi", "Edmund Y. Lam"], "title": "Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark", "categories": ["cs.CV"], "comment": null, "summary": "In low-light environments, conventional cameras often struggle to capture\nclear multi-view images of objects due to dynamic range limitations and motion\nblur caused by long exposure. Event cameras, with their high-dynamic range and\nhigh-speed properties, have the potential to mitigate these issues.\nAdditionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,\nfacilitating bright frame synthesis from multiple viewpoints in low-light\nconditions. However, naively using an event-assisted 3D GS approach still faced\nchallenges because, in low light, events are noisy, frames lack quality, and\nthe color tone may be inconsistent. To address these issues, we propose\nDark-EvGS, the first event-assisted 3D GS framework that enables the\nreconstruction of bright frames from arbitrary viewpoints along the camera\ntrajectory. Triplet-level supervision is proposed to gain holistic knowledge,\ngranular details, and sharp scene rendering. The color tone matching block is\nproposed to guarantee the color consistency of the rendered frames.\nFurthermore, we introduce the first real-captured dataset for the event-guided\nbright frame synthesis task via 3D GS-based radiance field reconstruction.\nExperiments demonstrate that our method achieves better results than existing\nmethods, conquering radiance field reconstruction under challenging low-light\nconditions. The code and sample data are included in the supplementary\nmaterial.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDark-EvGS\u6846\u67b6\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u548c3D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u5728\u4f4e\u5149\u6761\u4ef6\u4e0b\u91cd\u5efa\u591a\u89c6\u89d2\u660e\u4eae\u5e27\uff0c\u89e3\u51b3\u4e86\u566a\u58f0\u3001\u5e27\u8d28\u91cf\u5dee\u548c\u8272\u8c03\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u76f8\u673a\u5728\u4f4e\u5149\u73af\u5883\u4e0b\u96be\u4ee5\u6355\u6349\u6e05\u6670\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u9ad8\u901f\u5ea6\u7279\u6027\u53ef\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u9762\u4e34\u566a\u58f0\u548c\u8272\u8c03\u4e0d\u4e00\u81f4\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faDark-EvGS\u6846\u67b6\uff0c\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548c3D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u5f15\u5165\u4e09\u91cd\u76d1\u7763\u548c\u8272\u8c03\u5339\u914d\u6a21\u5757\uff0c\u786e\u4fdd\u6e32\u67d3\u5e27\u7684\u7ec6\u8282\u548c\u8272\u8c03\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u5149\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u529f\u91cd\u5efa\u8f90\u5c04\u573a\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u660e\u4eae\u5e27\u3002", "conclusion": "Dark-EvGS\u4e3a\u4f4e\u5149\u73af\u5883\u4e0b\u7684\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.11972", "pdf": "https://arxiv.org/pdf/2507.11972", "abs": "https://arxiv.org/abs/2507.11972", "authors": ["Yuhong Zhang", "Jialu Li", "Shilai Yang", "Yuchen Xu", "Gert Cauwenberghs", "Tzyy-Ping Jung"], "title": "Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker", "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Reading comprehension is a fundamental skill in human cognitive development.\nWith the advancement of Large Language Models (LLMs), there is a growing need\nto compare how humans and LLMs understand language across different contexts\nand apply this understanding to functional tasks such as inference, emotion\ninterpretation, and information retrieval. Our previous work used LLMs and\nhuman biomarkers to study the reading comprehension process. The results showed\nthat the biomarkers corresponding to words with high and low relevance to the\ninference target, as labeled by the LLMs, exhibited distinct patterns,\nparticularly when validated using eye-tracking data. However, focusing solely\non individual words limited the depth of understanding, which made the\nconclusions somewhat simplistic despite their potential significance. This\nstudy used an LLM-based AI agent to group words from a reading passage into\nnodes and edges, forming a graph-based text representation based on semantic\nmeaning and question-oriented prompts. We then compare the distribution of eye\nfixations on important nodes and edges. Our findings indicate that LLMs exhibit\nhigh consistency in language understanding at the level of graph topological\nstructure. These results build on our previous findings and offer insights into\neffective human-AI co-learning strategies.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9605\u8bfb\u7406\u89e3\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u5206\u6790\u53d1\u73b0LLMs\u5728\u8bed\u8a00\u7406\u89e3\u4e0a\u5177\u6709\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "motivation": "\u63a2\u7d22\u4eba\u7c7b\u4e0eLLMs\u5728\u8bed\u8a00\u7406\u89e3\u4e0a\u7684\u5dee\u5f02\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u5b66\u4e60\u3002", "method": "\u4f7f\u7528LLM\u5c06\u6587\u672c\u8f6c\u5316\u4e3a\u57fa\u4e8e\u8bed\u4e49\u7684\u56fe\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u773c\u52a8\u6570\u636e\u9a8c\u8bc1\u91cd\u8981\u8282\u70b9\u548c\u8fb9\u7684\u6ce8\u89c6\u5206\u5e03\u3002", "result": "LLMs\u5728\u56fe\u62d3\u6251\u7ed3\u6784\u5c42\u9762\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e00\u81f4\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\u5b66\u4e60\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.11932", "pdf": "https://arxiv.org/pdf/2507.11932", "abs": "https://arxiv.org/abs/2507.11932", "authors": ["Mohammad Shahab Sepehri", "Berk Tinaz", "Zalan Fabian", "Mahdi Soltanolkotabi"], "title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Mental visualization, the ability to construct and manipulate visual\nrepresentations internally, is a core component of human cognition and plays a\nvital role in tasks involving reasoning, prediction, and abstraction. Despite\nthe rapid progress of Multimodal Large Language Models (MLLMs), current\nbenchmarks primarily assess passive visual perception, offering limited insight\ninto the more active capability of internally constructing visual patterns to\nsupport problem solving. Yet mental visualization is a critical cognitive skill\nin humans, supporting abilities such as spatial navigation, predicting physical\ntrajectories, and solving complex visual problems through imaginative\nsimulation. To bridge this gap, we introduce Hyperphantasia, a synthetic\nbenchmark designed to evaluate the mental visualization abilities of MLLMs\nthrough four carefully constructed puzzles. Each task is procedurally generated\nand presented at three difficulty levels, enabling controlled analysis of model\nperformance across increasing complexity. Our comprehensive evaluation of\nstate-of-the-art models reveals a substantial gap between the performance of\nhumans and MLLMs. Additionally, we explore the potential of reinforcement\nlearning to improve visual simulation capabilities. Our findings suggest that\nwhile some models exhibit partial competence in recognizing visual patterns,\nrobust mental visualization remains an open challenge for current MLLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Hyperphantasia\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u5fc3\u7406\u53ef\u89c6\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u4e0e\u4eba\u7c7b\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u88ab\u52a8\u89c6\u89c9\u611f\u77e5\uff0c\u800c\u5fc3\u7406\u53ef\u89c6\u5316\u4f5c\u4e3a\u4eba\u7c7b\u8ba4\u77e5\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u5c1a\u672a\u5728MLLMs\u4e2d\u5f97\u5230\u5145\u5206\u8bc4\u4f30\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u4e2a\u7a0b\u5e8f\u751f\u6210\u7684\u96be\u9898\u4efb\u52a1\uff0c\u5206\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u89c6\u89c9\u6a21\u62df\u80fd\u529b\u7684\u6f5c\u529b\u3002", "result": "\u5f53\u524dMLLMs\u5728\u8bc6\u522b\u89c6\u89c9\u6a21\u5f0f\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u5fc3\u7406\u53ef\u89c6\u5316\u80fd\u529b\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002", "conclusion": "\u5fc3\u7406\u53ef\u89c6\u5316\u662fMLLMs\u7684\u91cd\u8981\u7814\u7a76\u65b9\u5411\uff0c\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u63d0\u5347\u6a21\u578b\u80fd\u529b\u7684\u7b56\u7565\u3002"}}
{"id": "2507.11979", "pdf": "https://arxiv.org/pdf/2507.11979", "abs": "https://arxiv.org/abs/2507.11979", "authors": ["Yuki Sakamoto", "Takahisa Uchida", "Hiroshi Ishiguro"], "title": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large language models (LLMs) have emerged as powerful tools for simulating\ncomplex social phenomena using human-like agents with specific traits. In human\nsocieties, value similarity is important for building trust and close\nrelationships; however, it remains unexplored whether this principle holds true\nin artificial societies comprising LLM agents. Therefore, this study\ninvestigates the influence of value similarity on relationship-building among\nLLM agents through two experiments. First, in a preliminary experiment, we\nevaluated the controllability of values in LLMs to identify the most effective\nmodel and prompt design for controlling the values. Subsequently, in the main\nexperiment, we generated pairs of LLM agents imbued with specific values and\nanalyzed their mutual evaluations of trust and interpersonal closeness\nfollowing a dialogue. The experiments were conducted in English and Japanese to\ninvestigate language dependence. The results confirmed that pairs of agents\nwith higher value similarity exhibited greater mutual trust and interpersonal\ncloseness. Our findings demonstrate that the LLM agent simulation serves as a\nvalid testbed for social science theories, contributes to elucidating the\nmechanisms by which values influence relationship building, and provides a\nfoundation for inspiring new theories and insights into the social sciences.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4ef7\u503c\u76f8\u4f3c\u6027\u5bf9LLM\u4ee3\u7406\u5173\u7cfb\u5efa\u7acb\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4ef7\u503c\u76f8\u4f3c\u6027\u9ad8\u7684\u4ee3\u7406\u4e4b\u95f4\u4fe1\u4efb\u548c\u4eb2\u5bc6\u611f\u66f4\u5f3a\u3002", "motivation": "\u63a2\u7d22\u4eba\u5de5\u793e\u4f1a\u4e2d\u4ef7\u503c\u76f8\u4f3c\u6027\u662f\u5426\u50cf\u4eba\u7c7b\u793e\u4f1a\u4e2d\u4e00\u6837\u5f71\u54cd\u5173\u7cfb\u5efa\u7acb\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\uff1a\u521d\u6b65\u5b9e\u9a8c\u8bc4\u4f30LLM\u4e2d\u4ef7\u503c\u53ef\u63a7\u6027\uff0c\u4e3b\u5b9e\u9a8c\u5206\u6790\u4ef7\u503c\u76f8\u4f3c\u4ee3\u7406\u7684\u4fe1\u4efb\u548c\u4eb2\u5bc6\u611f\u3002", "result": "\u4ef7\u503c\u76f8\u4f3c\u6027\u9ad8\u7684\u4ee3\u7406\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u4fe1\u4efb\u548c\u4eb2\u5bc6\u611f\u3002", "conclusion": "LLM\u4ee3\u7406\u6a21\u62df\u53ef\u4f5c\u4e3a\u793e\u4f1a\u79d1\u5b66\u7406\u8bba\u7684\u6709\u6548\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u542f\u53d1\u65b0\u7406\u8bba\u3002"}}
{"id": "2507.11947", "pdf": "https://arxiv.org/pdf/2507.11947", "abs": "https://arxiv.org/abs/2507.11947", "authors": ["Geon Park", "Seon Bin Kim", "Gunho Jung", "Seong-Whan Lee"], "title": "RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "6 Pages", "summary": "With recent advancements in text-to-image (T2I) models, effectively\ngenerating multiple instances within a single image prompt has become a crucial\nchallenge. Existing methods, while successful in generating positions of\nindividual instances, often struggle to account for relationship discrepancy\nand multiple attributes leakage. To address these limitations, this paper\nproposes the relation-aware disentangled learning (RaDL) framework. RaDL\nenhances instance-specific attributes through learnable parameters and\ngenerates relation-aware image features via Relation Attention, utilizing\naction verbs extracted from the global prompt. Through extensive evaluations on\nbenchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that\nRaDL outperforms existing methods, showing significant improvements in\npositional accuracy, multiple attributes consideration, and the relationships\nbetween instances. Our results present RaDL as the solution for generating\nimages that consider both the relationships and multiple attributes of each\ninstance within the multi-instance image.", "AI": {"tldr": "RaDL\u6846\u67b6\u901a\u8fc7\u5173\u7cfb\u611f\u77e5\u89e3\u8026\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u591a\u5b9e\u4f8b\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5173\u7cfb\u5dee\u5f02\u548c\u5c5e\u6027\u6cc4\u6f0f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4d\u7f6e\u51c6\u786e\u6027\u548c\u5b9e\u4f8b\u95f4\u5173\u7cfb\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u5b9e\u4f8b\u56fe\u50cf\u751f\u6210\u4e2d\u96be\u4ee5\u5904\u7406\u5b9e\u4f8b\u95f4\u5173\u7cfb\u5dee\u5f02\u548c\u591a\u91cd\u5c5e\u6027\u6cc4\u6f0f\u95ee\u9898\uff0cRaDL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "RaDL\u901a\u8fc7\u53ef\u5b66\u4e60\u53c2\u6570\u589e\u5f3a\u5b9e\u4f8b\u7279\u5b9a\u5c5e\u6027\uff0c\u5e76\u5229\u7528\u5168\u5c40\u63d0\u793a\u4e2d\u7684\u52a8\u4f5c\u52a8\u8bcd\u751f\u6210\u5173\u7cfb\u611f\u77e5\u56fe\u50cf\u7279\u5f81\u3002", "result": "\u5728COCO-Position\u3001COCO-MIG\u548cDrawBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRaDL\u5728\u4f4d\u7f6e\u51c6\u786e\u6027\u3001\u591a\u91cd\u5c5e\u6027\u8003\u8651\u548c\u5b9e\u4f8b\u5173\u7cfb\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RaDL\u662f\u591a\u5b9e\u4f8b\u56fe\u50cf\u751f\u6210\u4e2d\u8003\u8651\u5b9e\u4f8b\u5173\u7cfb\u548c\u591a\u91cd\u5c5e\u6027\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11981", "pdf": "https://arxiv.org/pdf/2507.11981", "abs": "https://arxiv.org/abs/2507.11981", "authors": ["Lukas Ellinger", "Miriam Ansch\u00fctz", "Georg Groh"], "title": "Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions", "categories": ["cs.CL"], "comment": "Accepted by RANLP 2025", "summary": "Large Language Models (LLMs) can provide accurate word definitions and\nexplanations for any context. However, the scope of the definition changes for\ndifferent target groups, like children or language learners. This is especially\nrelevant for homonyms, words with multiple meanings, where oversimplification\nmight risk information loss by omitting key senses, potentially misleading\nusers who trust LLM outputs. We investigate how simplification impacts homonym\ndefinition quality across three target groups: Normal, Simple, and ELI5. Using\ntwo novel evaluation datasets spanning multiple languages, we test DeepSeek v3,\nLlama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge\nand human annotations. Our results show that simplification drastically\ndegrades definition completeness by neglecting polysemy, increasing the risk of\nmisunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization\nsubstantially improves homonym response quality across all prompt types. These\nfindings highlight the need to balance simplicity and completeness in\neducational NLP to ensure reliable, context-aware definitions for all learners.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u7b80\u5316\u5bf9\u591a\u4e49\u8bcd\u5b9a\u4e49\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7b80\u5316\u4f1a\u663e\u8457\u964d\u4f4e\u5b9a\u4e49\u7684\u5b8c\u6574\u6027\uff0c\u589e\u52a0\u8bef\u89e3\u98ce\u9669\u3002\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u591a\u4e49\u8bcd\u5b9a\u4e49\u7684\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u591a\u4e49\u8bcd\u5b9a\u4e49\u5728\u4e0d\u540c\u76ee\u6807\u7fa4\u4f53\uff08\u5982\u513f\u7ae5\u6216\u8bed\u8a00\u5b66\u4e60\u8005\uff09\u4e2d\u7684\u7b80\u5316\u6548\u679c\uff0c\u4ee5\u907f\u514d\u56e0\u8fc7\u5ea6\u7b80\u5316\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u6216\u8bef\u5bfc\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u65b0\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u591a\u4e2aLLM\u6a21\u578b\uff08\u5982DeepSeek v3\u3001Llama 4\u7b49\uff09\uff0c\u7ed3\u5408LLM-as-Judge\u548c\u4eba\u5de5\u6807\u6ce8\uff0c\u8bc4\u4f30\u7b80\u5316\u5bf9\u5b9a\u4e49\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u7b80\u5316\u663e\u8457\u964d\u4f4e\u4e86\u5b9a\u4e49\u7684\u5b8c\u6574\u6027\uff0c\u5c24\u5176\u662f\u5ffd\u7565\u4e86\u591a\u4e49\u6027\u3002\u901a\u8fc7\u5fae\u8c03Llama 3.1 8B\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e49\u8bcd\u5b9a\u4e49\u7684\u8d28\u91cf\u3002", "conclusion": "\u6559\u80b2NLP\u9700\u5e73\u8861\u7b80\u6d01\u6027\u4e0e\u5b8c\u6574\u6027\uff0c\u4ee5\u786e\u4fdd\u4e3a\u6240\u6709\u5b66\u4e60\u8005\u63d0\u4f9b\u53ef\u9760\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b9a\u4e49\u3002"}}
{"id": "2507.11955", "pdf": "https://arxiv.org/pdf/2507.11955", "abs": "https://arxiv.org/abs/2507.11955", "authors": ["Yuhang Zhang", "Zhengyu Zhang", "Muxin Liao", "Shishun Tian", "Wenbin Zou", "Lu Zhang", "Chen Xu"], "title": "Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation", "categories": ["cs.CV"], "comment": "This paper was accepted by IEEE Transactions on Intelligent\n  Transportation Systems", "summary": "Generalizable semantic segmentation aims to perform well on unseen target\ndomains, a critical challenge due to real-world applications requiring high\ngeneralizability. Class-wise prototypes, representing class centroids, serve as\ndomain-invariant cues that benefit generalization due to their stability and\nsemantic consistency. However, this approach faces three challenges. First,\nexisting methods often adopt coarse prototypical alignment strategies, which\nmay hinder performance. Second, naive prototypes computed by averaging source\nbatch features are prone to overfitting and may be negatively affected by\nunrelated source data. Third, most methods treat all source samples equally,\nignoring the fact that different features have varying adaptation difficulties.\nTo address these limitations, we propose a novel framework for generalizable\nsemantic segmentation: Prototypical Progressive Alignment and Reweighting\n(PPAR), leveraging the strong generalization ability of the CLIP model.\nSpecifically, we define two prototypes: the Original Text Prototype (OTP) and\nVisual Text Prototype (VTP), generated via CLIP to serve as a solid base for\nalignment. We then introduce a progressive alignment strategy that aligns\nfeatures in an easy-to-difficult manner, reducing domain gaps gradually.\nFurthermore, we propose a prototypical reweighting mechanism that estimates the\nreliability of source data and adjusts its contribution, mitigating the effect\nof irrelevant or harmful features (i.e., reducing negative transfer). We also\nprovide a theoretical analysis showing the alignment between our method and\ndomain generalization theory. Extensive experiments across multiple benchmarks\ndemonstrate that PPAR achieves state-of-the-art performance, validating its\neffectiveness.", "AI": {"tldr": "PPAR\u6846\u67b6\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5bf9\u9f50\u548c\u539f\u578b\u91cd\u52a0\u6743\u89e3\u51b3\u901a\u7528\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6311\u6218\uff0c\u5229\u7528CLIP\u6a21\u578b\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u901a\u7528\u8bed\u4e49\u5206\u5272\u5728\u672a\u89c1\u76ee\u6807\u57df\u4e0a\u7684\u6027\u80fd\u662f\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u539f\u578b\u5bf9\u9f50\u7c97\u7cd9\u3001\u539f\u578b\u8ba1\u7b97\u6613\u8fc7\u62df\u5408\u53ca\u5ffd\u7565\u7279\u5f81\u9002\u5e94\u96be\u5ea6\u5dee\u5f02\u7684\u95ee\u9898\u3002", "method": "PPAR\u6846\u67b6\u5b9a\u4e49OTP\u548cVTP\u4e24\u79cd\u539f\u578b\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u5bf9\u9f50\u7b56\u7565\u548c\u539f\u578b\u91cd\u52a0\u6743\u673a\u5236\uff0c\u51cf\u5c11\u8d1f\u8fc1\u79fb\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPPAR\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "PPAR\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u901a\u7528\u8bed\u4e49\u5206\u5272\u7684\u6311\u6218\u3002"}}
{"id": "2507.12004", "pdf": "https://arxiv.org/pdf/2507.12004", "abs": "https://arxiv.org/abs/2507.12004", "authors": ["Josip Juki\u0107"], "title": "Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis", "categories": ["cs.CL"], "comment": null, "summary": "This thesis addresses challenges related to data and parameter efficiency in\nneural language models, with a focus on representation analysis and the\nintroduction of new optimization techniques. The first part examines the\nproperties and dynamics of language representations within neural models,\nemphasizing their significance in enhancing robustness and generalization. It\nproposes innovative approaches based on representation smoothness, including\nregularization strategies that utilize Jacobian and Hessian matrices to\nstabilize training and mitigate sensitivity to input perturbations. The second\npart focuses on methods to significantly enhance data and parameter efficiency\nby integrating active learning strategies with parameter-efficient fine-tuning,\nguided by insights from representation smoothness analysis. It presents\nsmoothness-informed early-stopping techniques designed to eliminate the need\nfor labeled validation sets and proposes innovative combinations of active\nlearning and parameter-efficient fine-tuning to reduce labeling efforts and\ncomputational resources. Extensive experimental evaluations across various NLP\ntasks demonstrate that these combined approaches substantially outperform\ntraditional methods in terms of performance, stability, and efficiency. The\nthird part explores weak supervision techniques enhanced by in-context learning\nto effectively utilize unlabeled data, further reducing dependence on extensive\nlabeling. It shows that using in-context learning as a mechanism for weak\nsupervision enables models to better generalize from limited labeled data by\nleveraging unlabeled examples more effectively during training. Comprehensive\nempirical evaluations confirm significant gains in model accuracy,\nadaptability, and robustness, especially in low-resource settings and dynamic\ndata environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u65b9\u6cd5\uff0c\u901a\u8fc7\u8868\u793a\u5206\u6790\u548c\u4f18\u5316\u6280\u672f\u63d0\u9ad8\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u548c\u53c2\u6570\u6548\u7387\uff0c\u5305\u62ec\u8868\u793a\u5e73\u6ed1\u6027\u3001\u4e3b\u52a8\u5b66\u4e60\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u7ed3\u5408\uff0c\u4ee5\u53ca\u5f31\u76d1\u7763\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u548c\u53c2\u6570\u6548\u7387\u65b9\u9762\u7684\u6311\u6218\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. \u5206\u6790\u8bed\u8a00\u8868\u793a\u7684\u7279\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u5e73\u6ed1\u6027\u7684\u6b63\u5219\u5316\u7b56\u7565\uff1b2. \u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff1b3. \u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u589e\u5f3a\u5f31\u76d1\u7763\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6570\u636e\u548c\u53c2\u6570\u6548\u7387\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.11967", "pdf": "https://arxiv.org/pdf/2507.11967", "abs": "https://arxiv.org/abs/2507.11967", "authors": ["Yuchi Ishikawa", "Shota Nakada", "Hokuto Munakata", "Kazuhiro Saito", "Tatsuya Komatsu", "Yoshimitsu Aoki"], "title": "Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos", "categories": ["cs.CV", "eess.AS", "eess.IV"], "comment": "Interspeech 2025", "summary": "In this paper, we propose Language-Guided Contrastive Audio-Visual Masked\nAutoencoders (LG-CAV-MAE) to improve audio-visual representation learning.\nLG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual\nmasked autoencoders, enabling the model to learn across audio, visual and text\nmodalities. To train LG-CAV-MAE, we introduce an automatic method to generate\naudio-visual-text triplets from unlabeled videos. We first generate frame-level\ncaptions using an image captioning model and then apply CLAP-based filtering to\nensure strong alignment between audio and captions. This approach yields\nhigh-quality audio-visual-text triplets without requiring manual annotations.\nWe evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an\naudio-visual classification task. Our method significantly outperforms existing\napproaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks\nand a 3.2% improvement for the classification task.", "AI": {"tldr": "\u63d0\u51faLG-CAV-MAE\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u7f16\u7801\u5668\u6539\u8fdb\u97f3\u9891-\u89c6\u89c9\u8868\u793a\u5b66\u4e60\uff0c\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u97f3\u9891-\u89c6\u89c9-\u6587\u672c\u4e09\u5143\u7ec4\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u6539\u8fdb\u97f3\u9891-\u89c6\u89c9\u8868\u793a\u5b66\u4e60\uff0c\u901a\u8fc7\u5f15\u5165\u6587\u672c\u6a21\u6001\u589e\u5f3a\u591a\u6a21\u6001\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u96c6\u6210\u9884\u8bad\u7ec3\u6587\u672c\u7f16\u7801\u5668\u5230\u5bf9\u6bd4\u97f3\u9891-\u89c6\u89c9\u63a9\u7801\u81ea\u7f16\u7801\u5668\u4e2d\uff0c\u81ea\u52a8\u751f\u6210\u97f3\u9891-\u89c6\u89c9-\u6587\u672c\u4e09\u5143\u7ec4\uff0c\u4f7f\u7528CLAP\u8fc7\u6ee4\u786e\u4fdd\u5bf9\u9f50\u3002", "result": "\u5728\u97f3\u9891-\u89c6\u89c9\u68c0\u7d22\u4efb\u52a1\u4e2d\u53ec\u56de\u7387\u63d0\u53475.6%\uff0c\u5206\u7c7b\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u53473.2%\u3002", "conclusion": "LG-CAV-MAE\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u3002"}}
{"id": "2507.12039", "pdf": "https://arxiv.org/pdf/2507.12039", "abs": "https://arxiv.org/abs/2507.12039", "authors": ["Anca Dinu", "Andra-Maria Florescu", "Alina Resceanu"], "title": "A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans", "categories": ["cs.CL"], "comment": "Accepted for presentation at KES 2025. To appear in Procedia Computer\n  Science (Elsevier)", "summary": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u901a\u7528\u7684\u8bed\u8a00\u521b\u9020\u529b\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u65b0\u8bcd\u548c\u77ed\u8bed\u65b9\u9762\u7684\u80fd\u529b\u3002\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0cLLMs\u5728\u6240\u6709\u8bc4\u4f30\u6807\u51c6\u4e2d\u5747\u4f18\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4eba\u7c7b\u548cLLMs\u5728\u8bed\u8a00\u521b\u9020\u529b\u65b9\u9762\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u65b0\u8bcd\u751f\u6210\u548c\u9690\u55bb\u4f7f\u7528\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u6d4b\u8bd5\u5305\u62ec\u591a\u79cd\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e86\u57fa\u4e8e\u8bcd\u5f62\u53d8\u5316\uff08\u6d3e\u751f\u548c\u590d\u5408\uff09\u548c\u9690\u55bb\u8bed\u8a00\u4f7f\u7528\u7684\u521b\u9020\u529b\u3002\u4f7f\u7528OCSAI\u5de5\u5177\u81ea\u52a8\u8bc4\u4f30\u4e86\u7b54\u6848\u7684\u539f\u521b\u6027\u3001\u7cbe\u7ec6\u5ea6\u548c\u7075\u6d3b\u6027\u3002", "result": "LLMs\u5728\u6240\u6709\u8bc4\u4f30\u6807\u51c6\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\uff0c\u5e76\u5728\u516b\u9879\u4efb\u52a1\u4e2d\u7684\u516d\u9879\u4e2d\u8868\u73b0\u66f4\u597d\u3002\u4eba\u7c7b\u66f4\u503e\u5411\u4e8e\u6269\u5c55\u6027\u521b\u9020\u529b\uff08E-creativity\uff09\uff0c\u800cLLMs\u66f4\u503e\u5411\u4e8e\u56fa\u5b9a\u6027\u521b\u9020\u529b\uff08F-creativity\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cLLMs\u5728\u8bed\u8a00\u521b\u9020\u529b\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u4eba\u7c7b\u7684\u521b\u9020\u529b\u7c7b\u578b\u4e0eLLMs\u5b58\u5728\u5dee\u5f02\u3002"}}
{"id": "2507.11968", "pdf": "https://arxiv.org/pdf/2507.11968", "abs": "https://arxiv.org/abs/2507.11968", "authors": ["Sahid Hossain Mustakim", "S M Jishanul Islam", "Ummay Maria Muna", "Montasir Chowdhury", "Mohammed Jawwadul Islam", "Sadia Ahmmed", "Tashfia Sikder", "Syed Tasdid Azam Dhrubo", "Swakkhar Shatabda"], "title": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation", "categories": ["cs.CV"], "comment": "Accepted as long paper, SVU Workshop at ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) are increasingly used for content\nmoderation, yet their robustness in short-form video contexts remains\nunderexplored. Current safety evaluations often rely on unimodal attacks,\nfailing to address combined attack vulnerabilities. In this paper, we introduce\na comprehensive framework for evaluating the tri-modal safety of MLLMs. First,\nwe present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising\ndiverse short-form videos with human-guided synthetic adversarial attacks.\nSecond, we propose ChimeraBreak, a novel tri-modal attack strategy that\nsimultaneously challenges visual, auditory, and semantic reasoning pathways.\nExtensive experiments on state-of-the-art MLLMs reveal significant\nvulnerabilities with high Attack Success Rates (ASR). Our findings uncover\ndistinct failure modes, showing model biases toward misclassifying benign or\npolicy-violating content. We assess results using LLM-as-a-judge, demonstrating\nattack reasoning efficacy. Our dataset and findings provide crucial insights\nfor developing more robust and safe MLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u77ed\u89c6\u9891\u5185\u5bb9\u5ba1\u6838\u4e2d\u5b89\u5168\u6027\u7684\u6846\u67b6\uff0c\u5305\u62ecSVMA\u6570\u636e\u96c6\u548cChimeraBreak\u653b\u51fb\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7684\u9ad8\u8106\u5f31\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u5b89\u5168\u8bc4\u4f30\u591a\u57fa\u4e8e\u5355\u6a21\u6001\u653b\u51fb\uff0c\u672a\u80fd\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u5c24\u5176\u662f\u5728\u77ed\u89c6\u9891\u5185\u5bb9\u5ba1\u6838\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86SVMA\u6570\u636e\u96c6\u548cChimeraBreak\u4e09\u6a21\u6001\u653b\u51fb\u7b56\u7565\uff0c\u540c\u65f6\u6311\u6218\u89c6\u89c9\u3001\u542c\u89c9\u548c\u8bed\u4e49\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMLLMs\u5b58\u5728\u663e\u8457\u8106\u5f31\u6027\uff0c\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u9ad8\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u826f\u6027\u6216\u8fdd\u89c4\u5185\u5bb9\u7684\u8bef\u5206\u7c7b\u503e\u5411\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u5b89\u5168\u7684MLLMs\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2507.12059", "pdf": "https://arxiv.org/pdf/2507.12059", "abs": "https://arxiv.org/abs/2507.12059", "authors": ["Anthony G Cohn", "Robert E Blackwell"], "title": "Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited", "categories": ["cs.CL"], "comment": "8 pages, 5 figures. Accepted at QR 2025 : 38th International Workshop\n  on Qualitative Reasoning at IJCAI", "summary": "We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e8628\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65b9\u5411\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u65b0\u578b\u63a8\u7406\u6a21\u578b\u4e5f\u65e0\u6cd5\u5b8c\u5168\u51c6\u786e\u5b8c\u6210\u4efb\u52a1\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65b9\u5411\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5bf9\u57fa\u672c\u65b9\u5411\u5224\u65ad\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6a21\u677f\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u65b9\u5411\u63a8\u7406\u80fd\u529b\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u5747\u65e0\u6cd5\u5728\u6240\u6709\u95ee\u9898\u4e0a\u53ef\u9760\u5224\u65ad\u6b63\u786e\u65b9\u5411\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65b9\u5411\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u6709\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2507.11969", "pdf": "https://arxiv.org/pdf/2507.11969", "abs": "https://arxiv.org/abs/2507.11969", "authors": ["Zhaohong Huang", "Yuxin Zhang", "Jingjing Xie", "Fei Chao", "Rongrong Ji"], "title": "GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in test-time adaptation (TTA) for Vision-Language Models\n(VLMs) have garnered increasing attention, particularly through the use of\nmultiple augmented views of a single image to boost zero-shot generalization.\nUnfortunately, existing methods fail to strike a satisfactory balance between\nperformance and efficiency, either due to excessive overhead of tuning text\nprompts or unstable benefits from handcrafted, training-free visual feature\nenhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias),\nan efficient and effective TTA paradigm that incorporates two learnable biases\nduring TTA, unfolded as the global bias and spatial bias. Particularly, the\nglobal bias captures the global semantic features of a test image by learning\nconsistency across augmented views, while spatial bias learns the semantic\ncoherence between regions in the image's spatial visual representation. It is\nworth highlighting that these two sets of biases are directly added to the\nlogits outputed by the pretrained VLMs, which circumvent the full\nbackpropagation through VLM that hinders the efficiency of existing TTA\nmethods. This endows GS-Bias with extremely high efficiency while achieving\nstate-of-the-art performance on 15 benchmark datasets. For example, it achieves\na 2.23% improvement over TPT in cross-dataset generalization and a 2.72%\nimprovement in domain generalization, while requiring only 6.5% of TPT's memory\nusage on ImageNet.", "AI": {"tldr": "GS-Bias\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u7a7a\u95f4\u504f\u7f6e\u5b66\u4e60\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709TTA\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u8981\u4e48\u8c03\u4f18\u6587\u672c\u63d0\u793a\u5f00\u9500\u5927\uff0c\u8981\u4e48\u624b\u5de5\u89c6\u89c9\u7279\u5f81\u589e\u5f3a\u6548\u679c\u4e0d\u7a33\u5b9a\u3002", "method": "GS-Bias\u5f15\u5165\u5168\u5c40\u504f\u7f6e\u548c\u7a7a\u95f4\u504f\u7f6e\uff0c\u76f4\u63a5\u6dfb\u52a0\u5230\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8f93\u51falogits\u4e2d\uff0c\u907f\u514d\u5168\u53cd\u5411\u4f20\u64ad\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u572815\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u4f8b\u5982\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u4e0a\u4f18\u4e8eTPT 2.23%\uff0c\u5185\u5b58\u4f7f\u7528\u4ec5\u4e3aTPT\u76846.5%\u3002", "conclusion": "GS-Bias\u5728\u9ad8\u6548\u6027\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3aTTA\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.12064", "pdf": "https://arxiv.org/pdf/2507.12064", "abs": "https://arxiv.org/abs/2507.12064", "authors": ["Jeremi K. Ochab", "Mateusz Matias", "Tymoteusz Boba", "Tomasz Walkowiak"], "title": "StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This submission to the binary AI detection task is based on a modular\nstylometric pipeline, where: public spaCy models are used for text\npreprocessing (including tokenisation, named entity recognition, dependency\nparsing, part-of-speech tagging, and morphology annotation) and extracting\nseveral thousand features (frequencies of n-grams of the above linguistic\nannotations); light-gradient boosting machines are used as the classifier. We\ncollect a large corpus of more than 500 000 machine-generated texts for the\nclassifier's training. We explore several parameter options to increase the\nclassifier's capacity and take advantage of that training set. Our approach\nfollows the non-neural, computationally inexpensive but explainable approach\nfound effective previously.", "AI": {"tldr": "\u57fa\u4e8e\u6a21\u5757\u5316\u98ce\u683c\u6d4b\u91cf\u7ba1\u9053\u7684\u4e8c\u5143AI\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528spaCy\u6a21\u578b\u9884\u5904\u7406\u6587\u672c\u5e76\u63d0\u53d6\u7279\u5f81\uff0c\u91c7\u7528\u8f7b\u91cf\u68af\u5ea6\u63d0\u5347\u673a\u5206\u7c7b\u5668\uff0c\u8bad\u7ec3\u96c6\u5305\u542b50\u4e07\u673a\u5668\u751f\u6210\u6587\u672c\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u975e\u795e\u7ecf\u7f51\u7edc\u7684\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u53ef\u89e3\u91ca\u7684AI\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528spaCy\u6a21\u578b\u8fdb\u884c\u6587\u672c\u9884\u5904\u7406\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u7ed3\u5408\u8f7b\u91cf\u68af\u5ea6\u63d0\u5347\u673a\u5206\u7c7b\u5668\uff0c\u5e76\u4f18\u5316\u53c2\u6570\u4ee5\u5229\u7528\u5927\u89c4\u6a21\u8bad\u7ec3\u96c6\u3002", "result": "\u8bad\u7ec3\u4e86\u5305\u542b50\u4e07\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u4f18\u5316\u4e86\u5206\u7c7b\u5668\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5ef6\u7eed\u4e86\u975e\u795e\u7ecf\u7f51\u7edc\u3001\u4f4e\u6210\u672c\u4e14\u53ef\u89e3\u91ca\u7684\u68c0\u6d4b\u7b56\u7565\uff0c\u5c55\u73b0\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2507.11980", "pdf": "https://arxiv.org/pdf/2507.11980", "abs": "https://arxiv.org/abs/2507.11980", "authors": ["Jiajian Xie", "Shengyu Zhang", "Zhou Zhao", "Fan Wu", "Fei Wu"], "title": "EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models", "categories": ["cs.CV"], "comment": "21 pages, 8 figures", "summary": "Diffusion Models have shown remarkable proficiency in image and video\nsynthesis. As model size and latency increase limit user experience, hybrid\nedge-cloud collaborative framework was recently proposed to realize fast\ninference and high-quality generation, where the cloud model initiates\nhigh-quality semantic planning and the edge model expedites later-stage\nrefinement. However, excessive cloud denoising prolongs inference time, while\ninsufficient steps cause semantic ambiguity, leading to inconsistency in edge\nmodel output. To address these challenges, we propose EC-Diff that accelerates\ncloud inference through gradient-based noise estimation while identifying the\noptimal point for cloud-edge handoff to maintain generation quality.\nSpecifically, we design a K-step noise approximation strategy to reduce cloud\ninference frequency by using noise gradients between steps and applying cloud\ninference periodically to adjust errors. Then we design a two-stage greedy\nsearch algorithm to efficiently find the optimal parameters for noise\napproximation and edge model switching. Extensive experiments demonstrate that\nour method significantly enhances generation quality compared to edge\ninference, while achieving up to an average $2\\times$ speedup in inference\ncompared to cloud inference. Video samples and source code are available at\nhttps://ec-diff.github.io/.", "AI": {"tldr": "EC-Diff\u901a\u8fc7\u68af\u5ea6\u566a\u58f0\u4f30\u8ba1\u548c\u4f18\u5316\u4e91\u8fb9\u5207\u6362\u70b9\uff0c\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u4e0e\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u4e91\u8fb9\u534f\u4f5c\u6846\u67b6\u4e2d\u4e91\u53bb\u566a\u65f6\u95f4\u8fc7\u957f\u548c\u8fb9\u7f18\u6a21\u578b\u8f93\u51fa\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528K\u6b65\u566a\u58f0\u8fd1\u4f3c\u7b56\u7565\u51cf\u5c11\u4e91\u63a8\u7406\u9891\u7387\uff0c\u8bbe\u8ba1\u4e24\u9636\u6bb5\u8d2a\u5fc3\u641c\u7d22\u7b97\u6cd5\u4f18\u5316\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u751f\u6210\u8d28\u91cf\u4f18\u4e8e\u8fb9\u7f18\u63a8\u7406\uff0c\u63a8\u7406\u901f\u5ea6\u5e73\u5747\u63d0\u53472\u500d\u3002", "conclusion": "EC-Diff\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u4e86\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2507.12075", "pdf": "https://arxiv.org/pdf/2507.12075", "abs": "https://arxiv.org/abs/2507.12075", "authors": ["Giuliano Martinelli", "Tommaso Bonomo", "Pere-Llu\u00eds Huguet Cabot", "Roberto Navigli"], "title": "BOOKCOREF: Coreference Resolution at Book Scale", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference. 19 pages", "summary": "Coreference Resolution systems are typically evaluated on benchmarks\ncontaining small- to medium-scale documents. When it comes to evaluating long\ntexts, however, existing benchmarks, such as LitBank, remain limited in length\nand do not adequately assess system capabilities at the book scale, i.e., when\nco-referring mentions span hundreds of thousands of tokens. To fill this gap,\nwe first put forward a novel automatic pipeline that produces high-quality\nCoreference Resolution annotations on full narrative texts. Then, we adopt this\npipeline to create the first book-scale coreference benchmark, BOOKCOREF, with\nan average document length of more than 200,000 tokens. We carry out a series\nof experiments showing the robustness of our automatic procedure and\ndemonstrating the value of our resource, which enables current long-document\ncoreference systems to gain up to +20 CoNLL-F1 points when evaluated on full\nbooks. Moreover, we report on the new challenges introduced by this\nunprecedented book-scale setting, highlighting that current models fail to\ndeliver the same performance they achieve on smaller documents. We release our\ndata and code to encourage research and development of new book-scale\nCoreference Resolution systems at https://github.com/sapienzanlp/bookcoref.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u6807\u6ce8\u7ba1\u9053\uff0c\u521b\u5efa\u4e86\u9996\u4e2a\u4e66\u7c4d\u89c4\u6a21\u7684\u5171\u6307\u6d88\u89e3\u57fa\u51c6BOOKCOREF\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u957f\u6587\u6863\u5171\u6307\u6d88\u89e3\u7cfb\u7edf\u6027\u80fd\u4e0a\u7684\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u5171\u6307\u6d88\u89e3\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u77ed\u5230\u4e2d\u7b49\u957f\u5ea6\u6587\u6863\uff0c\u7f3a\u4e4f\u5bf9\u4e66\u7c4d\u89c4\u6a21\u957f\u6587\u672c\u7684\u8bc4\u4f30\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u6807\u6ce8\u7ba1\u9053\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u5171\u6307\u6d88\u89e3\u6807\u6ce8\uff0c\u5e76\u521b\u5efa\u4e86BOOKCOREF\u57fa\u51c6\u3002", "result": "BOOKCOREF\u4f7f\u73b0\u6709\u7cfb\u7edf\u5728\u4e66\u7c4d\u89c4\u6a21\u4e0a\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe20 CoNLL-F1\u5206\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u5728\u957f\u6587\u6863\u4e0a\u8868\u73b0\u4e0d\u5982\u77ed\u6587\u6863\u3002", "conclusion": "BOOKCOREF\u586b\u8865\u4e86\u4e66\u7c4d\u89c4\u6a21\u5171\u6307\u6d88\u89e3\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u6311\u6218\u548c\u8d44\u6e90\u3002"}}
{"id": "2507.11985", "pdf": "https://arxiv.org/pdf/2507.11985", "abs": "https://arxiv.org/abs/2507.11985", "authors": ["Jiahao Xia", "Yike Wu", "Wenjian Huang", "Jianguo Zhang", "Jian Zhang"], "title": "Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Part-level features are crucial for image understanding, but few studies\nfocus on them because of the lack of fine-grained labels. Although unsupervised\npart discovery can eliminate the reliance on labels, most of them cannot\nmaintain robustness across various categories and scenarios, which restricts\ntheir application range. To overcome this limitation, we present a more\neffective paradigm for unsupervised part discovery, named Masked Part\nAutoencoder (MPAE). It first learns part descriptors as well as a feature map\nfrom the inputs and produces patch features from a masked version of the\noriginal images. Then, the masked regions are filled with the learned part\ndescriptors based on the similarity between the local features and descriptors.\nBy restoring these masked patches using the part descriptors, they become\nbetter aligned with their part shapes, guided by appearance features from\nunmasked patches. Finally, MPAE robustly discovers meaningful parts that\nclosely match the actual object shapes, even in complex scenarios. Moreover,\nseveral looser yet more effective constraints are proposed to enable MPAE to\nidentify the presence of parts across various scenarios and categories in an\nunsupervised manner. This provides the foundation for addressing challenges\nposed by occlusion and for exploring part similarity across multiple\ncategories. Extensive experiments demonstrate that our method robustly\ndiscovers meaningful parts across various categories and scenarios. The code is\navailable at the project https://github.com/Jiahao-UTS/MPAE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMasked Part Autoencoder (MPAE)\u7684\u65e0\u76d1\u7763\u90e8\u4ef6\u53d1\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a9\u7801\u548c\u90e8\u4ef6\u63cf\u8ff0\u7b26\u5b66\u4e60\uff0c\u80fd\u5728\u590d\u6742\u573a\u666f\u4e2d\u7a33\u5065\u5730\u53d1\u73b0\u6709\u610f\u4e49\u90e8\u4ef6\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u90e8\u4ef6\u53d1\u73b0\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u6807\u7b7e\u4e14\u8de8\u7c7b\u522b\u548c\u573a\u666f\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u3002", "method": "MPAE\u901a\u8fc7\u63a9\u7801\u56fe\u50cf\u5b66\u4e60\u90e8\u4ef6\u63cf\u8ff0\u7b26\u548c\u7279\u5f81\u56fe\uff0c\u5229\u7528\u5c40\u90e8\u7279\u5f81\u4e0e\u63cf\u8ff0\u7b26\u7684\u76f8\u4f3c\u6027\u586b\u5145\u63a9\u7801\u533a\u57df\uff0c\u4ece\u800c\u5bf9\u9f50\u90e8\u4ef6\u5f62\u72b6\u3002", "result": "MPAE\u5728\u591a\u79cd\u7c7b\u522b\u548c\u573a\u666f\u4e2d\u7a33\u5065\u5730\u53d1\u73b0\u4e86\u4e0e\u5b9e\u9645\u7269\u4f53\u5f62\u72b6\u5339\u914d\u7684\u90e8\u4ef6\u3002", "conclusion": "MPAE\u4e3a\u65e0\u76d1\u7763\u8de8\u7c7b\u522b\u548c\u573a\u666f\u90e8\u4ef6\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u652f\u6301\u906e\u6321\u548c\u90e8\u4ef6\u76f8\u4f3c\u6027\u7814\u7a76\u3002"}}
{"id": "2507.12079", "pdf": "https://arxiv.org/pdf/2507.12079", "abs": "https://arxiv.org/abs/2507.12079", "authors": ["Tosin Adewumi", "Foteini Simistira Liwicki", "Marcus Liwicki", "Viktor Gardelli", "Lama Alkhaled", "Hamam Mokayed"], "title": "Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning", "categories": ["cs.CL"], "comment": "This paper was accepted for the special issue AI for Education by the\n  IEEE Signal Processing Magazine journal", "summary": "This paper presents an intervention study on the effects of the combined\nmethods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)\nsimplified gamification and (4) formative feedback on university students'\nMaths learning driven by large language models (LLMs). We call our approach\nMathematics Explanations through Games by AI LLMs (MEGA). Some students\nstruggle with Maths and as a result avoid Math-related discipline or subjects\ndespite the importance of Maths across many fields, including signal\nprocessing. Oftentimes, students' Maths difficulties stem from suboptimal\npedagogy. We compared the MEGA method to the traditional step-by-step (CoT)\nmethod to ascertain which is better by using a within-group design after\nrandomly assigning questions for the participants, who are university students.\nSamples (n=60) were randomly drawn from each of the two test sets of the Grade\nSchool Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)\ndatasets, based on the error margin of 11%, the confidence level of 90%, and a\nmanageable number of samples for the student evaluators. These samples were\nused to evaluate two capable LLMs at length (Generative Pretrained Transformer\n4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for\ncapability. The results showed that students agree in more instances that the\nMEGA method is experienced as better for learning for both datasets. It is even\nmuch better than the CoT (47.5% compared to 26.67%) in the more difficult MATH\ndataset, indicating that MEGA is better at explaining difficult Maths problems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7ed3\u5408\u82cf\u683c\u62c9\u5e95\u65b9\u6cd5\u3001\u601d\u7ef4\u94fe\u63a8\u7406\u3001\u7b80\u5316\u6e38\u620f\u5316\u548c\u5f62\u6210\u6027\u53cd\u9988\u7684MEGA\u65b9\u6cd5\u5bf9\u5927\u5b66\u751f\u6570\u5b66\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u53d1\u73b0MEGA\u65b9\u6cd5\u5728\u89e3\u91ca\u96be\u9898\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u8bb8\u591a\u5b66\u751f\u5728\u6570\u5b66\u5b66\u4e60\u4e2d\u9047\u5230\u56f0\u96be\uff0c\u5bfc\u81f4\u4ed6\u4eec\u56de\u907f\u6570\u5b66\u76f8\u5173\u5b66\u79d1\uff0c\u800c\u4f20\u7edf\u7684\u6559\u5b66\u65b9\u6cd5\u53ef\u80fd\u4e0d\u591f\u7406\u60f3\u3002", "method": "\u91c7\u7528MEGA\u65b9\u6cd5\uff08\u7ed3\u5408\u591a\u79cd\u6559\u5b66\u7b56\u7565\uff09\u4e0e\u4f20\u7edf\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u901a\u8fc7\u968f\u673a\u5206\u914d\u95ee\u9898\u8fdb\u884c\u7ec4\u5185\u8bbe\u8ba1\u3002", "result": "MEGA\u65b9\u6cd5\u5728GSM8K\u548cMATH\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u96be\u9898\u89e3\u91ca\u4e0a\u66f4\u6709\u6548\u3002", "conclusion": "MEGA\u65b9\u6cd5\u662f\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6570\u5b66\u6559\u5b66\u7b56\u7565\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u6570\u5b66\u95ee\u9898\u3002"}}
{"id": "2507.11986", "pdf": "https://arxiv.org/pdf/2507.11986", "abs": "https://arxiv.org/abs/2507.11986", "authors": ["Jaehyun Lee", "Wonhark Park", "Wonsik Shin", "Hyunho Lee", "Hyoung Min Na", "Nojun Kwak"], "title": "Style Composition within Distinct LoRA modules for Traditional Art", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion-based text-to-image models have achieved remarkable results in\nsynthesizing diverse images from text prompts and can capture specific artistic\nstyles via style personalization. However, their entangled latent space and\nlack of smooth interpolation make it difficult to apply distinct painting\ntechniques in a controlled, regional manner, often causing one style to\ndominate. To overcome this, we propose a zero-shot diffusion pipeline that\nnaturally blends multiple styles by performing style composition on the\ndenoised latents predicted during the flow-matching denoising process of\nseparately trained, style-specialized models. We leverage the fact that\nlower-noise latents carry stronger stylistic information and fuse them across\nheterogeneous diffusion pipelines using spatial masks, enabling precise,\nregion-specific style control. This mechanism preserves the fidelity of each\nindividual style while allowing user-guided mixing. Furthermore, to ensure\nstructural coherence across different models, we incorporate depth-map\nconditioning via ControlNet into the diffusion framework. Qualitative and\nquantitative experiments demonstrate that our method successfully achieves\nregion-specific style mixing according to the given masks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u6269\u6563\u7ba1\u9053\uff0c\u901a\u8fc7\u878d\u5408\u591a\u4e2a\u98ce\u683c\u5316\u7684\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u533a\u57df\u7279\u5b9a\u7684\u98ce\u683c\u6df7\u5408\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u98ce\u683c\u4e2a\u6027\u5316\u65f6\u96be\u4ee5\u5b9e\u73b0\u533a\u57df\u63a7\u5236\u548c\u98ce\u683c\u5e73\u6ed1\u63d2\u503c\uff0c\u5bfc\u81f4\u98ce\u683c\u4e3b\u5bfc\u95ee\u9898\u3002", "method": "\u5728\u53bb\u566a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u98ce\u683c\u878d\u5408\uff0c\u5229\u7528\u4f4e\u566a\u58f0\u6f5c\u5728\u7a7a\u95f4\u7684\u7a7a\u95f4\u63a9\u7801\u548c\u6df1\u5ea6\u56fe\u6761\u4ef6\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6839\u636e\u63a9\u7801\u5b9e\u73b0\u533a\u57df\u7279\u5b9a\u7684\u98ce\u683c\u6df7\u5408\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u98ce\u683c\u6df7\u5408\u4e2d\u7684\u533a\u57df\u63a7\u5236\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u5404\u98ce\u683c\u7684\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2507.12126", "pdf": "https://arxiv.org/pdf/2507.12126", "abs": "https://arxiv.org/abs/2507.12126", "authors": ["Payal Bhattad", "Sai Manoj Pudukotai Dinakarrao", "Anju Gupta"], "title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6587\u672c\u589e\u5f3a\u65b9\u6cd5\uff0c\u5305\u62ec\u53ef\u6269\u5c55\u6027\u5206\u6790\u548c\u8fed\u4ee3\u589e\u5f3a\u4e0e\u6458\u8981\u7ec6\u5316\uff08IASR\uff09\uff0c\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u589e\u5f3a\u6280\u672f\u5728\u8bed\u4e49\u4fdd\u7559\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6216\u8fed\u4ee3\u751f\u6210\u4e2d\u53ef\u80fd\u5bfc\u81f4\u5197\u4f59\u548c\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u8bc4\u4f30\u7ec4\u4ef6\uff1a\u53ef\u6269\u5c55\u6027\u5206\u6790\u548c\u8fed\u4ee3\u589e\u5f3a\u4e0e\u6458\u8981\u7ec6\u5316\uff08IASR\uff09\uff0c\u7528\u4e8e\u8861\u91cf\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u6f02\u79fb\u3002", "result": "GPT-3.5 Turbo\u5728\u8bed\u4e49\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u751f\u6210\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u5b9e\u9645\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86400%\u7684\u4e3b\u9898\u7c92\u5ea6\u5e76\u6d88\u9664\u4e86\u4e3b\u9898\u91cd\u53e0\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u57fa\u4e8eLLM\u7684\u6587\u672c\u589e\u5f3a\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645NLP\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.11990", "pdf": "https://arxiv.org/pdf/2507.11990", "abs": "https://arxiv.org/abs/2507.11990", "authors": ["Hyun-Jun Jin", "Young-Eun Kim", "Seong-Whan Lee"], "title": "ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recently, personalized portrait generation with a text-to-image diffusion\nmodel has significantly advanced with Textual Inversion, emerging as a\npromising approach for creating high-fidelity personalized images. Despite its\npotential, current Textual Inversion methods struggle to maintain consistent\nfacial identity due to semantic misalignments between textual and visual\nembedding spaces regarding identity. We introduce ID-EA, a novel framework that\nguides text embeddings to align with visual identity embeddings, thereby\nimproving identity preservation in a personalized generation. ID-EA comprises\ntwo key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned\nAdapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings\nwith a textual ID anchor, refining visual identity embeddings derived from a\nface recognition model using representative text embeddings. Then, the\nID-Adapter leverages the identity-enhanced embedding to adapt the text\ncondition, ensuring identity preservation by adjusting the cross-attention\nmodule in the pre-trained UNet model. This process encourages the text features\nto find the most related visual clues across the foreground snippets. Extensive\nquantitative and qualitative evaluations demonstrate that ID-EA substantially\noutperforms state-of-the-art methods in identity preservation metrics while\nachieving remarkable computational efficiency, generating personalized\nportraits approximately 15 times faster than existing approaches.", "AI": {"tldr": "ID-EA\u6846\u67b6\u901a\u8fc7ID-Enhancer\u548cID-Adapter\u6539\u8fdb\u6587\u672c\u5d4c\u5165\u4e0e\u89c6\u89c9\u8eab\u4efd\u5d4c\u5165\u7684\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e2a\u6027\u5316\u8096\u50cf\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524dTextual Inversion\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u8096\u50cf\u751f\u6210\u4e2d\u96be\u4ee5\u4fdd\u6301\u9762\u90e8\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u4e3b\u8981\u7531\u4e8e\u6587\u672c\u4e0e\u89c6\u89c9\u5d4c\u5165\u7a7a\u95f4\u7684\u8bed\u4e49\u4e0d\u5bf9\u9f50\u3002", "method": "ID-EA\u5305\u542bID-Enhancer\u548cID-Adapter\uff1a\u524d\u8005\u6574\u5408\u8eab\u4efd\u5d4c\u5165\u4e0e\u6587\u672c\u951a\u70b9\uff0c\u540e\u8005\u8c03\u6574\u9884\u8bad\u7ec3UNet\u6a21\u578b\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u4ee5\u4fdd\u6301\u8eab\u4efd\u3002", "result": "ID-EA\u5728\u8eab\u4efd\u4fdd\u7559\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u751f\u6210\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb15\u500d\u3002", "conclusion": "ID-EA\u901a\u8fc7\u6539\u8fdb\u5d4c\u5165\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8eab\u4efd\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u4e2a\u6027\u5316\u8096\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12143", "pdf": "https://arxiv.org/pdf/2507.12143", "abs": "https://arxiv.org/abs/2507.12143", "authors": ["Pavel \u0160indel\u00e1\u0159", "Ond\u0159ej Bojar"], "title": "Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators", "categories": ["cs.CL", "I.2.7"], "comment": "30 pages, 7 figures, CLEF 2025 Conference and Labs of the Evaluation\n  Forum", "summary": "ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable.", "AI": {"tldr": "ELOQUENT\u7684Sensemaking\u4efb\u52a1\u901a\u8fc7\u4e09\u4e2a\u6b65\u9aa4\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\uff1a\u95ee\u9898\u751f\u6210\u3001\u95ee\u9898\u56de\u7b54\u548c\u7b54\u6848\u8bc4\u5206\u30022025\u5e74\u7684\u5b9e\u9a8c\u6d89\u53ca\u591a\u8bed\u8a00\u6750\u6599\uff0c4\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u53d1\u73b0\u4e86\u4e00\u4e9b\u95ee\u9898\uff0c\u5982\u95ee\u9898\u8d28\u91cf\u8bc4\u4f30\u56f0\u96be\u3001\u6a21\u578b\u7b54\u6848\u53d7\u9650\u4ee5\u53ca\u8bc4\u5206\u7cfb\u7edf\u7684\u6f0f\u6d1e\u3002", "motivation": "\u65e8\u5728\u4e3a\u751f\u6210\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u53ef\u6d4b\u8bd5\u7684\u9ad8\u5c42\u6b21\u8bc4\u4f30\u6807\u51c6\uff0c\u901a\u8fc7\u7c7b\u4f3c\u8bfe\u5802\u8003\u8bd5\u7684\u65b9\u5f0f\u8bc4\u4f30\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5206\u4e3a\u4e09\u6b65\uff1a(1) \u6559\u5e08\u7cfb\u7edf\u751f\u6210\u95ee\u9898\uff0c(2) \u5b66\u751f\u7cfb\u7edf\u56de\u7b54\u95ee\u9898\uff0c(3) \u8bc4\u4f30\u7cfb\u7edf\u8bc4\u5206\u3002\u5b9e\u9a8c\u4f7f\u7528\u591a\u8bed\u8a00\u6750\u6599\uff0c\u5e76\u5bf9\u6bd4\u81ea\u52a8\u4e0e\u4eba\u5de5\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u751f\u6210\u95ee\u9898\u7684\u8d28\u91cf\u96be\u4ee5\u8bc4\u4f30\uff0c\u6a21\u578b\u56de\u7b54\u95ee\u9898\u8868\u73b0\u5c1a\u53ef\u4f46\u53d7\u9650\uff0c\u8bc4\u5206\u7cfb\u7edf\u5b58\u5728\u6f0f\u6d1e\u3002", "conclusion": "\u9700\u6539\u8fdb\u95ee\u9898\u751f\u6210\u548c\u8bc4\u5206\u7b56\u7565\uff0c\u6a21\u578b\u5728\u53d7\u9650\u8f93\u5165\u4e0b\u7684\u8868\u73b0\u4ecd\u9700\u4f18\u5316\u3002"}}
{"id": "2507.11994", "pdf": "https://arxiv.org/pdf/2507.11994", "abs": "https://arxiv.org/abs/2507.11994", "authors": ["Jun Yin", "Fei Wu", "Yupeng Ren", "Jisheng Huang", "Qiankun Li", "Heng jin", "Jianhai Fu", "Chanjie Cui"], "title": "SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation", "categories": ["cs.CV"], "comment": "IGARSS2025 accepted, Correspondence: fujianhai2024@gmail.com (J.F.),\n  cuichj@mail2.sysu.edu.cn (C.C.)", "summary": "Public remote sensing datasets often face limitations in universality due to\nresolution variability and inconsistent land cover category definitions. To\nharness the vast pool of unlabeled remote sensing data, we propose SAMST, a\nsemi-supervised semantic segmentation method. SAMST leverages the strengths of\nthe Segment Anything Model (SAM) in zero-shot generalization and boundary\ndetection. SAMST iteratively refines pseudo-labels through two main components:\nsupervised model self-training using both labeled and pseudo-labeled data, and\na SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three\nmodules: a Threshold Filter Module for preprocessing, a Prompt Generation\nModule for extracting connected regions and generating prompts for SAM, and a\nLabel Refinement Module for final label stitching. By integrating the\ngeneralization power of large models with the training efficiency of small\nmodels, SAMST improves pseudo-label accuracy, thereby enhancing overall model\nperformance. Experiments on the Potsdam dataset validate the effectiveness and\nfeasibility of SAMST, demonstrating its potential to address the challenges\nposed by limited labeled data in remote sensing semantic segmentation.", "AI": {"tldr": "SAMST\u662f\u4e00\u79cd\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u5229\u7528Segment Anything Model\uff08SAM\uff09\u7684\u96f6\u6837\u672c\u6cdb\u5316\u548c\u8fb9\u754c\u68c0\u6d4b\u80fd\u529b\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u4f2a\u6807\u7b7e\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u516c\u5171\u9065\u611f\u6570\u636e\u96c6\u56e0\u5206\u8fa8\u7387\u5dee\u5f02\u548c\u5730\u7269\u7c7b\u522b\u5b9a\u4e49\u4e0d\u4e00\u81f4\u800c\u7f3a\u4e4f\u666e\u9002\u6027\uff0c\u9650\u5236\u4e86\u672a\u6807\u8bb0\u6570\u636e\u7684\u5229\u7528\u3002", "method": "SAMST\u7ed3\u5408\u76d1\u7763\u6a21\u578b\u81ea\u8bad\u7ec3\u548c\u57fa\u4e8eSAM\u7684\u4f2a\u6807\u7b7e\u7ec6\u5316\u5668\uff0c\u540e\u8005\u5305\u62ec\u9608\u503c\u8fc7\u6ee4\u6a21\u5757\u3001\u63d0\u793a\u751f\u6210\u6a21\u5757\u548c\u6807\u7b7e\u7ec6\u5316\u6a21\u5757\u3002", "result": "\u5728Potsdam\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SAMST\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\u3002", "conclusion": "SAMST\u901a\u8fc7\u7ed3\u5408\u5927\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5c0f\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u9065\u611f\u8bed\u4e49\u5206\u5272\u4e2d\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2507.12208", "pdf": "https://arxiv.org/pdf/2507.12208", "abs": "https://arxiv.org/abs/2507.12208", "authors": ["Michael Carl", "Takanori Mizowaki", "Aishvarya Ray", "Masaru Yamada", "Devi Sri Bandaru", "Xinyue Ren"], "title": "Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production", "categories": ["cs.CL"], "comment": null, "summary": "The paper introduces a Behavioural Translation Style Space (BTSS) that\ndescribes possible behavioural translation patterns. The suggested BTSS is\norganized as a hierarchical structure that entails various embedded processing\nlayers. We posit that observable translation behaviour - i.e., eye and finger\nmovements - is fundamental when executing the physical act of translation but\nit is caused and shaped by higher-order cognitive processes and affective\ntranslation states. We analyse records of keystrokes and gaze data as\nindicators of the hidden mental processing structure and organize the\nbehavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the\nbasis for a computational translation agent to simulate the temporal dynamics\nof affect, automatized behaviour and cognition during human translation\nproduction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u884c\u4e3a\u7ffb\u8bd1\u98ce\u683c\u7a7a\u95f4\uff08BTSS\uff09\uff0c\u7528\u4e8e\u63cf\u8ff0\u53ef\u80fd\u7684\u7ffb\u8bd1\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u51fb\u952e\u548c\u6ce8\u89c6\u6570\u636e\u63ed\u793a\u9690\u85cf\u7684\u8ba4\u77e5\u8fc7\u7a0b\u3002", "motivation": "\u7814\u7a76\u7ffb\u8bd1\u884c\u4e3a\u4e0e\u9ad8\u9636\u8ba4\u77e5\u53ca\u60c5\u611f\u72b6\u6001\u7684\u5173\u7cfb\uff0c\u4e3a\u6a21\u62df\u4eba\u7c7b\u7ffb\u8bd1\u8fc7\u7a0b\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u8bb0\u5f55\u51fb\u952e\u548c\u6ce8\u89c6\u6570\u636e\uff0c\u6784\u5efa\u591a\u5c42\u6b21\u7684BTSS\uff0c\u5e76\u7528\u4e8e\u8ba1\u7b97\u7ffb\u8bd1\u4ee3\u7406\u7684\u6a21\u62df\u3002", "result": "BTSS\u80fd\u591f\u6355\u6349\u7ffb\u8bd1\u8fc7\u7a0b\u4e2d\u7684\u60c5\u611f\u3001\u81ea\u52a8\u884c\u4e3a\u548c\u8ba4\u77e5\u52a8\u6001\u3002", "conclusion": "BTSS\u4e3a\u7406\u89e3\u7ffb\u8bd1\u884c\u4e3a\u53ca\u5176\u8ba4\u77e5\u57fa\u7840\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u4e3a\u8ba1\u7b97\u7ffb\u8bd1\u4ee3\u7406\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.12001", "pdf": "https://arxiv.org/pdf/2507.12001", "abs": "https://arxiv.org/abs/2507.12001", "authors": ["Hao Li", "Ju Dai", "Feng Zhou", "Kaida Ning", "Lei Li", "Junjun Pan"], "title": "AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "While 3D facial animation has made impressive progress, challenges still\nexist in realizing fine-grained stylized 3D facial expression manipulation due\nto the lack of appropriate datasets. In this paper, we introduce the\nAUBlendSet, a 3D facial dataset based on AU-Blendshape representation for\nfine-grained facial expression manipulation across identities. AUBlendSet is a\nblendshape data collection based on 32 standard facial action units (AUs)\nacross 500 identities, along with an additional set of facial postures\nannotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to\nlearn AU-Blendshape basis vectors for different character styles. AUBlendNet\npredicts, in parallel, the AU-Blendshape basis vectors of the corresponding\nstyle for a given identity mesh, thereby achieving stylized 3D emotional facial\nmanipulation. We comprehensively validate the effectiveness of AUBlendSet and\nAUBlendNet through tasks such as stylized facial expression manipulation,\nspeech-driven emotional facial animation, and emotion recognition data\naugmentation. Through a series of qualitative and quantitative experiments, we\ndemonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D\nfacial animation tasks. To the best of our knowledge, AUBlendSet is the first\ndataset, and AUBlendNet is the first network for continuous 3D facial\nexpression manipulation for any identity through facial AUs. Our source code is\navailable at https://github.com/wslh852/AUBlendNet.git.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86AUBlendSet\u6570\u636e\u96c6\u548cAUBlendNet\u7f51\u7edc\uff0c\u7528\u4e8e\u57fa\u4e8e\u9762\u90e8\u52a8\u4f5c\u5355\u5143\uff08AUs\uff09\u7684\u7ec6\u7c92\u5ea63D\u9762\u90e8\u8868\u60c5\u64cd\u7eb5\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5408\u9002\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u98ce\u683c\u53163D\u9762\u90e8\u8868\u60c5\u64cd\u7eb5\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u57fa\u4e8e32\u4e2a\u6807\u51c6AUs\u548c500\u4e2a\u8eab\u4efd\u7684\u6570\u636e\u96c6AUBlendSet\uff0c\u63d0\u51faAUBlendNet\u5b66\u4e60\u4e0d\u540c\u98ce\u683c\u7684AU-Blendshape\u57fa\u5411\u91cf\u3002", "result": "\u901a\u8fc7\u98ce\u683c\u5316\u8868\u60c5\u64cd\u7eb5\u3001\u8bed\u97f3\u9a71\u52a8\u52a8\u753b\u548c\u60c5\u611f\u8bc6\u522b\u6570\u636e\u589e\u5f3a\u7b49\u4efb\u52a1\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "AUBlendSet\u548cAUBlendNet\u57283D\u9762\u90e8\u52a8\u753b\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u662f\u9996\u4e2a\u901a\u8fc7AUs\u5b9e\u73b0\u8fde\u7eed\u8868\u60c5\u64cd\u7eb5\u7684\u6570\u636e\u96c6\u548c\u7f51\u7edc\u3002"}}
{"id": "2507.12217", "pdf": "https://arxiv.org/pdf/2507.12217", "abs": "https://arxiv.org/abs/2507.12217", "authors": ["Reuben Smit", "Retief Louw", "Herman Kamper"], "title": "Towards few-shot isolated word reading assessment", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to SLaTE 2025", "summary": "We explore an ASR-free method for isolated word reading assessment in\nlow-resource settings. Our few-shot approach compares input child speech to a\nsmall set of adult-provided reference templates. Inputs and templates are\nencoded using intermediate layers from large self-supervised learned (SSL)\nmodels. Using an Afrikaans child speech benchmark, we investigate design\noptions such as discretising SSL features and barycentre averaging of the\ntemplates. Idealised experiments show reasonable performance for adults, but a\nsubstantial drop for child speech input, even with child templates. Despite the\nsuccess of employing SSL representations in low-resource speech tasks, our work\nhighlights the limitations of SSL representations for processing child data\nwhen used in a few-shot classification system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7684\u5b64\u7acb\u8bcd\u9605\u8bfb\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u73af\u5883\u3002\u901a\u8fc7\u5bf9\u6bd4\u513f\u7ae5\u8bed\u97f3\u4e0e\u5c11\u91cf\u6210\u4eba\u53c2\u8003\u6a21\u677f\uff0c\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u7f16\u7801\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5bf9\u6210\u4eba\u8bed\u97f3\u6548\u679c\u5c1a\u53ef\uff0c\u4f46\u5bf9\u513f\u7ae5\u8bed\u97f3\u6548\u679c\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u65b9\u6cd5\u96be\u4ee5\u9002\u7528\uff0c\u56e0\u6b64\u63a2\u7d22\u4e00\u79cd\u65e0\u9700ASR\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c24\u5176\u9488\u5bf9\u513f\u7ae5\u8bed\u97f3\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u8f93\u5165\u513f\u7ae5\u8bed\u97f3\u4e0e\u5c11\u91cf\u6210\u4eba\u53c2\u8003\u6a21\u677f\u5bf9\u6bd4\uff0c\u5229\u7528SSL\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u7279\u5f81\u7f16\u7801\uff0c\u5e76\u7814\u7a76\u4e86\u79bb\u6563\u5316SSL\u7279\u5f81\u548c\u6a21\u677f\u7684\u8d28\u5fc3\u5e73\u5747\u7b49\u8bbe\u8ba1\u9009\u9879\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5bf9\u6210\u4eba\u8bed\u97f3\u8868\u73b0\u5408\u7406\uff0c\u4f46\u5bf9\u513f\u7ae5\u8bed\u97f3\u6548\u679c\u663e\u8457\u4e0b\u964d\uff0c\u5373\u4f7f\u4f7f\u7528\u513f\u7ae5\u6a21\u677f\u3002", "conclusion": "\u5c3d\u7ba1SSL\u8868\u5f81\u5728\u4f4e\u8d44\u6e90\u8bed\u97f3\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u672c\u6587\u63ed\u793a\u4e86\u5176\u5728\u5c11\u6837\u672c\u5206\u7c7b\u7cfb\u7edf\u4e2d\u5904\u7406\u513f\u7ae5\u6570\u636e\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.12006", "pdf": "https://arxiv.org/pdf/2507.12006", "abs": "https://arxiv.org/abs/2507.12006", "authors": ["Linwei Chen", "Lin Gu", "Ying Fu"], "title": "Frequency-Dynamic Attention Modulation for Dense Prediction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICCV 2025", "summary": "Vision Transformers (ViTs) have significantly advanced computer vision,\ndemonstrating strong performance across various tasks. However, the attention\nmechanism in ViTs makes each layer function as a low-pass filter, and the\nstacked-layer architecture in existing transformers suffers from frequency\nvanishing. This leads to the loss of critical details and textures. We propose\na novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention\nModulation (FDAM), which can be easily plugged into ViTs. FDAM directly\nmodulates the overall frequency response of ViTs and consists of two\ntechniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling\n(FreqScale). Since circuit theory uses low-pass filters as fundamental\nelements, we introduce AttInv, a method that generates complementary high-pass\nfiltering by inverting the low-pass filter in the attention matrix, and\ndynamically combining the two. We further design FreqScale to weight different\nfrequency components for fine-grained adjustments to the target response\nfunction. Through feature similarity analysis and effective rank evaluation, we\ndemonstrate that our approach avoids representation collapse, leading to\nconsistent performance improvements across various models, including SegFormer,\nDeiT, and MaskDINO. These improvements are evident in tasks such as semantic\nsegmentation, object detection, and instance segmentation. Additionally, we\napply our method to remote sensing detection, achieving state-of-the-art\nresults in single-scale settings. The code is available at\n\\href{https://github.com/Linwei-Chen/FDAM}{https://github.com/Linwei-Chen/FDAM}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFDAM\u7684\u65b0\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u5236ViTs\u7684\u9891\u7387\u54cd\u5e94\uff0c\u89e3\u51b3\u4e86\u73b0\u6709ViTs\u4e2d\u9891\u7387\u6d88\u5931\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709ViTs\u7684\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u9891\u7387\u6d88\u5931\uff0c\u4e22\u5931\u5173\u952e\u7ec6\u8282\u548c\u7eb9\u7406\u3002", "method": "\u63d0\u51faFDAM\uff0c\u5305\u542bAttention Inversion\u548cFrequency Dynamic Scaling\u4e24\u79cd\u6280\u672f\uff0c\u52a8\u6001\u8c03\u5236\u9891\u7387\u54cd\u5e94\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u63d0\u5347\uff0c\u5305\u62ec\u8bed\u4e49\u5206\u5272\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u5b9e\u4f8b\u5206\u5272\uff0c\u5e76\u5728\u9065\u611f\u68c0\u6d4b\u4e2d\u8fbe\u5230SOTA\u3002", "conclusion": "FDAM\u6709\u6548\u907f\u514d\u4e86\u8868\u793a\u5d29\u6e83\uff0c\u663e\u8457\u63d0\u5347\u4e86ViTs\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12252", "pdf": "https://arxiv.org/pdf/2507.12252", "abs": "https://arxiv.org/abs/2507.12252", "authors": ["Shilin Zhou", "Zhenghua Li"], "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7c92\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8bcd\u7ea7\u548c\u77ed\u8bed\u7ea7\u878d\u5408\u7684\u4f18\u52bf\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u5347\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u4e2d\u5173\u952e\u8bcd\u7684\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684ASR\u6a21\u578b\u5728\u8bc6\u522b\u4e0a\u4e0b\u6587\u76f8\u5173\u5173\u952e\u8bcd\uff08\u5982\u4e13\u6709\u540d\u8bcd\u6216\u7528\u6237\u7279\u5b9a\u5b9e\u4f53\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u7684\u8bcd\u7ea7\u548c\u77ed\u8bed\u7ea7\u878d\u5408\u65b9\u6cd5\u5404\u6709\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7c92\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8bcd\u7ea7\u548c\u77ed\u8bed\u7ea7\u7684\u4f18\u52bf\uff0c\u5e76\u91c7\u7528\u540e\u878d\u5408\u7b56\u7565\u5c06ASR\u7684\u58f0\u5b66\u4fe1\u606f\u4e0eLLM\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u4e2d\u6587\u548c\u82f1\u6587\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5173\u952e\u8bcd\u76f8\u5173\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u975e\u5173\u952e\u8bcd\u6587\u672c\u7684\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u591a\u7c92\u5ea6\u878d\u5408\u6846\u67b6\u4e2d\u7684\u8bcd\u7ea7\u548c\u77ed\u8bed\u7ea7\u7ec4\u4ef6\u76f8\u4e92\u8865\u5145\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.12008", "pdf": "https://arxiv.org/pdf/2507.12008", "abs": "https://arxiv.org/abs/2507.12008", "authors": ["Jiawen Wang", "Yinda Chen", "Xiaoyu Liu", "Che Liu", "Dong Liu", "Jianqing Gao", "Zhiwei Xiong"], "title": "Dual form Complementary Masking for Domain-Adaptive Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Recent works have correlated Masked Image Modeling (MIM) with consistency\nregularization in Unsupervised Domain Adaptation (UDA). However, they merely\ntreat masking as a special form of deformation on the input images and neglect\nthe theoretical analysis, which leads to a superficial understanding of masked\nreconstruction and insufficient exploitation of its potential in enhancing\nfeature extraction and representation learning. In this paper, we reframe\nmasked reconstruction as a sparse signal reconstruction problem and\ntheoretically prove that the dual form of complementary masks possesses\nsuperior capabilities in extracting domain-agnostic image features. Based on\nthis compelling insight, we propose MaskTwins, a simple yet effective UDA\nframework that integrates masked reconstruction directly into the main training\npipeline. MaskTwins uncovers intrinsic structural patterns that persist across\ndisparate domains by enforcing consistency between predictions of images masked\nin complementary ways, enabling domain generalization in an end-to-end manner.\nExtensive experiments verify the superiority of MaskTwins over baseline methods\nin natural and biological image segmentation. These results demonstrate the\nsignificant advantages of MaskTwins in extracting domain-invariant features\nwithout the need for separate pre-training, offering a new paradigm for\ndomain-adaptive segmentation.", "AI": {"tldr": "\u8bba\u6587\u5c06\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7a00\u758f\u4fe1\u53f7\u91cd\u5efa\u95ee\u9898\uff0c\u63d0\u51faMaskTwins\u6846\u67b6\uff0c\u901a\u8fc7\u4e92\u8865\u63a9\u7801\u63d0\u5347\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u9886\u57df\u81ea\u9002\u5e94\u5206\u5272\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u5c06\u63a9\u7801\u89c6\u4e3a\u56fe\u50cf\u53d8\u5f62\uff0c\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\uff0c\u672a\u80fd\u5145\u5206\u6316\u6398\u63a9\u7801\u91cd\u5efa\u5728\u7279\u5f81\u63d0\u53d6\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5c06\u63a9\u7801\u91cd\u5efa\u89c6\u4e3a\u7a00\u758f\u4fe1\u53f7\u95ee\u9898\uff0c\u7406\u8bba\u8bc1\u660e\u4e92\u8865\u63a9\u7801\u7684\u53cc\u91cd\u5f62\u5f0f\u80fd\u63d0\u53d6\u9886\u57df\u65e0\u5173\u7279\u5f81\uff0c\u63d0\u51faMaskTwins\u6846\u67b6\u3002", "result": "MaskTwins\u5728\u81ea\u7136\u548c\u751f\u7269\u56fe\u50cf\u5206\u5272\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u65e0\u9700\u5355\u72ec\u9884\u8bad\u7ec3\u5373\u53ef\u63d0\u53d6\u9886\u57df\u4e0d\u53d8\u7279\u5f81\u3002", "conclusion": "MaskTwins\u4e3a\u9886\u57df\u81ea\u9002\u5e94\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u63a9\u7801\u91cd\u5efa\u5728\u7279\u5f81\u63d0\u53d6\u4e2d\u7684\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.12260", "pdf": "https://arxiv.org/pdf/2507.12260", "abs": "https://arxiv.org/abs/2507.12260", "authors": ["Yikang Liu", "Wanyang Zhang", "Yiming Wang", "Jialong Tang", "Pei Zhang", "Baosong Yang", "Fei Huang", "Rui Wang", "Hai Hu"], "title": "Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose the first quantitative measure for translationese\n-- the translationese-index (T-index) for graded and generalizable measurement\nof translationese, computed from the likelihood ratios of two contrastively\nfine-tuned language models (LMs). We use a synthesized dataset and a dataset\nwith translations in the wild to evaluate T-index's generalizability in\ncross-domain settings and its validity against human judgments. Our results\nshow that T-index is both robust and efficient. T-index scored by two 0.5B LMs\nfine-tuned on only 1-5k pairs of synthetic data can well capture translationese\nin the wild. We find that the relative differences in T-indices between\ntranslations can well predict pairwise translationese annotations obtained from\nhuman annotators; and the absolute values of T-indices correlate well with\nhuman ratings of degrees of translationese (Pearson's $r = 0.568$).\nAdditionally, the correlation between T-index and existing machine translation\n(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting\nthat T-index is not covered by these metrics and can serve as a complementary\nmetric in MT QE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u6307\u6807T-index\uff0c\u7528\u4e8e\u91cf\u5316\u7ffb\u8bd1\u6587\u672c\u7684\u7ffb\u8bd1\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\uff08\u5982BLEU\u548cCOMET\uff09\u672a\u80fd\u5145\u5206\u6355\u6349\u7ffb\u8bd1\u7279\u6027\uff08translationese\uff09\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u4e24\u79cd\u5bf9\u6bd4\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u8ba1\u7b97T-index\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u548c\u5b9e\u9645\u7ffb\u8bd1\u6570\u636e\u9a8c\u8bc1\u5176\u8de8\u57df\u901a\u7528\u6027\u548c\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\u6027\u3002", "result": "T-index\u5728\u5c11\u91cf\u6570\u636e\u5fae\u8c03\u540e\u80fd\u6709\u6548\u6355\u6349\u7ffb\u8bd1\u7279\u6027\uff0c\u4e0e\u4eba\u7c7b\u8bc4\u5206\u76f8\u5173\u6027\u9ad8\uff08Pearson's r=0.568\uff09\uff0c\u4e14\u4e0e\u73b0\u6709MT\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u76f8\u5173\u6027\u4f4e\u3002", "conclusion": "T-index\u662f\u4e00\u79cd\u7a33\u5065\u4e14\u9ad8\u6548\u7684\u7ffb\u8bd1\u7279\u6027\u91cf\u5316\u6307\u6807\uff0c\u53ef\u4f5c\u4e3a\u73b0\u6709MT\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u7684\u8865\u5145\u3002"}}
{"id": "2507.12009", "pdf": "https://arxiv.org/pdf/2507.12009", "abs": "https://arxiv.org/abs/2507.12009", "authors": ["Florian David", "Michael Chan", "Elenor Morgenroth", "Patrik Vuilleumier", "Dimitri Van De Ville"], "title": "Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli", "categories": ["cs.CV", "cs.HC"], "comment": "Accepted in International Conference of the IEEE Engineering in\n  Medicine and Biology Society (EMBC) 2025", "summary": "We propose an end-to-end deep neural encoder-decoder model to encode and\ndecode brain activity in response to naturalistic stimuli using functional\nmagnetic resonance imaging (fMRI) data. Leveraging temporally correlated input\nfrom consecutive film frames, we employ temporal convolutional layers in our\narchitecture, which effectively allows to bridge the temporal resolution gap\nbetween natural movie stimuli and fMRI acquisitions. Our model predicts\nactivity of voxels in and around the visual cortex and performs reconstruction\nof corresponding visual inputs from neural activity. Finally, we investigate\nbrain regions contributing to visual decoding through saliency maps. We find\nthat the most contributing regions are the middle occipital area, the fusiform\narea, and the calcarine, respectively employed in shape perception, complex\nrecognition (in particular face perception), and basic visual features such as\nedges and contrasts. These functions being strongly solicited are in line with\nthe decoder's capability to reconstruct edges, faces, and contrasts. All in\nall, this suggests the possibility to probe our understanding of visual\nprocessing in films using as a proxy the behaviour of deep learning models such\nas the one proposed in this paper.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u795e\u7ecf\u7f16\u7801-\u89e3\u7801\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u8fc7fMRI\u6570\u636e\u7f16\u7801\u548c\u89e3\u7801\u5927\u8111\u5bf9\u81ea\u7136\u523a\u6fc0\u7684\u53cd\u5e94\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u586b\u8865\u81ea\u7136\u7535\u5f71\u523a\u6fc0\u4e0efMRI\u91c7\u96c6\u4e4b\u95f4\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u5dee\u8ddd\uff0c\u5e76\u63a2\u7d22\u89c6\u89c9\u5904\u7406\u7684\u795e\u7ecf\u673a\u5236\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u5377\u79ef\u5c42\u7684\u7f16\u7801-\u89e3\u7801\u67b6\u6784\uff0c\u5229\u7528\u8fde\u7eed\u7535\u5f71\u5e27\u7684\u65f6\u95f4\u76f8\u5173\u6027\u9884\u6d4b\u89c6\u89c9\u76ae\u5c42\u53ca\u5176\u5468\u56f4\u4f53\u7d20\u7684\u6d3b\u52a8\uff0c\u5e76\u91cd\u5efa\u89c6\u89c9\u8f93\u5165\u3002", "result": "\u6a21\u578b\u6210\u529f\u9884\u6d4b\u4e86\u89c6\u89c9\u76ae\u5c42\u7684\u6d3b\u52a8\uff0c\u5e76\u91cd\u5efa\u4e86\u8fb9\u7f18\u3001\u9762\u90e8\u548c\u5bf9\u6bd4\u7b49\u89c6\u89c9\u7279\u5f81\u3002\u8d21\u732e\u6700\u5927\u7684\u8111\u533a\u5305\u62ec\u4e2d\u6795\u533a\u3001\u68ad\u72b6\u533a\u548c\u8ddd\u72b6\u533a\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u63a2\u7d22\u89c6\u89c9\u5904\u7406\u673a\u5236\u7684\u5de5\u5177\uff0c\u5c24\u5176\u662f\u5728\u81ea\u7136\u7535\u5f71\u523a\u6fc0\u4e0b\u7684\u795e\u7ecf\u6d3b\u52a8\u89e3\u7801\u3002"}}
{"id": "2507.12261", "pdf": "https://arxiv.org/pdf/2507.12261", "abs": "https://arxiv.org/abs/2507.12261", "authors": ["Johann Frei", "Nils Feldhus", "Lisa Raithel", "Roland Roller", "Alexander Meyer", "Frank Kramer"], "title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes", "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to EMNLP 2025 System Demonstrations | Code:\n  https://github.com/j-frei/Infherno | Video:\n  https://www.youtube.com/watch?v=kyj5C2ivbMw | Demo:\n  https://infherno.misit-augsburg.de | HuggingFace Spaces:\n  https://huggingface.co/spaces/nfel/infherno", "summary": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInfherno\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5229\u7528LLM\u4ee3\u7406\u3001\u4ee3\u7801\u6267\u884c\u548c\u533b\u5b66\u672f\u8bed\u6570\u636e\u5e93\u5de5\u5177\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u8f6c\u5316\u4e3a\u7b26\u5408HL7 FHIR\u6807\u51c6\u7684\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u901a\u7528\u6027\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408LLM\u4ee3\u7406\u3001\u4ee3\u7801\u6267\u884c\u548c\u533b\u5b66\u672f\u8bed\u6570\u636e\u5e93\uff0c\u8bbe\u8ba1\u7aef\u5230\u7aef\u6846\u67b6Infherno\u3002", "result": "Infherno\u5728\u9884\u6d4bFHIR\u8d44\u6e90\u65f6\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u57fa\u7ebf\uff0c\u652f\u6301\u4e34\u5e8a\u6570\u636e\u96c6\u6210\u548c\u8de8\u673a\u6784\u4e92\u64cd\u4f5c\u6027\u3002", "conclusion": "Infherno\u4e3a\u4e34\u5e8a\u6570\u636e\u96c6\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.12012", "pdf": "https://arxiv.org/pdf/2507.12012", "abs": "https://arxiv.org/abs/2507.12012", "authors": ["Matthias Perkonigg", "Nina Bastati", "Ahmed Ba-Ssalamah", "Peter Mesenbrink", "Alexander Goehler", "Miljen Martic", "Xiaofei Zhou", "Michael Trauner", "Georg Langs"], "title": "Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Quantifiable image patterns associated with disease progression and treatment\nresponse are critical tools for guiding individual treatment, and for\ndeveloping novel therapies. Here, we show that unsupervised machine learning\ncan identify a pattern vocabulary of liver tissue in magnetic resonance images\nthat quantifies treatment response in diffuse liver disease. Deep clustering\nnetworks simultaneously encode and cluster patches of medical images into a\nlow-dimensional latent space to establish a tissue vocabulary. The resulting\ntissue types capture differential tissue change and its location in the liver\nassociated with treatment response. We demonstrate the utility of the\nvocabulary on a randomized controlled trial cohort of non-alcoholic\nsteatohepatitis patients. First, we use the vocabulary to compare longitudinal\nliver change in a placebo and a treatment cohort. Results show that the method\nidentifies specific liver tissue change pathways associated with treatment, and\nenables a better separation between treatment groups than established\nnon-imaging measures. Moreover, we show that the vocabulary can predict biopsy\nderived features from non-invasive imaging data. We validate the method on a\nseparate replication cohort to demonstrate the applicability of the proposed\nmethod.", "AI": {"tldr": "\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u901a\u8fc7\u6df1\u5ea6\u805a\u7c7b\u7f51\u7edc\u8bc6\u522b\u809d\u810fMRI\u56fe\u50cf\u4e2d\u7684\u7ec4\u7ec7\u6a21\u5f0f\uff0c\u91cf\u5316\u5f25\u6f2b\u6027\u809d\u75c5\u7684\u6cbb\u7597\u53cd\u5e94\uff0c\u5e76\u5728\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u4f18\u4e8e\u4f20\u7edf\u975e\u5f71\u50cf\u5b66\u65b9\u6cd5\u3002", "motivation": "\u91cf\u5316\u4e0e\u75be\u75c5\u8fdb\u5c55\u548c\u6cbb\u7597\u53cd\u5e94\u76f8\u5173\u7684\u56fe\u50cf\u6a21\u5f0f\uff0c\u4ee5\u6307\u5bfc\u4e2a\u4f53\u5316\u6cbb\u7597\u548c\u5f00\u53d1\u65b0\u7597\u6cd5\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u805a\u7c7b\u7f51\u7edc\u5bf9\u533b\u5b66\u56fe\u50cf\u5757\u8fdb\u884c\u7f16\u7801\u548c\u805a\u7c7b\uff0c\u5efa\u7acb\u7ec4\u7ec7\u8bcd\u6c47\u8868\uff0c\u6355\u6349\u4e0e\u6cbb\u7597\u53cd\u5e94\u76f8\u5173\u7684\u7ec4\u7ec7\u53d8\u5316\u3002", "result": "\u65b9\u6cd5\u5728\u975e\u9152\u7cbe\u6027\u8102\u80aa\u6027\u809d\u708e\u60a3\u8005\u7684\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u4e2d\u9a8c\u8bc1\uff0c\u80fd\u66f4\u6709\u6548\u533a\u5206\u6cbb\u7597\u7ec4\uff0c\u5e76\u9884\u6d4b\u6d3b\u68c0\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u521b\u8bc4\u4f30\u809d\u75c5\u6cbb\u7597\u53cd\u5e94\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.12295", "pdf": "https://arxiv.org/pdf/2507.12295", "abs": "https://arxiv.org/abs/2507.12295", "authors": ["Feng Xiao", "Jicong Fan"], "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u591a\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u548c\u591a\u9886\u57df\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5d4c\u5165\u8d28\u91cf\u5bf9\u5f02\u5e38\u68c0\u6d4b\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u6e90\u4e86\u5de5\u5177\u5305\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u73b0\u6709\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u8fdb\u884c\u4e25\u683c\u6bd4\u8f83\u548c\u521b\u65b0\u3002", "method": "\u5229\u7528\u591a\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08\u5982GloVe\u3001BERT\u3001LLaMa\u7b49\uff09\u7684\u5d4c\u5165\u548c\u591a\u9886\u57df\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u591a\u79cd\u8bc4\u4f30\u6307\u6807\uff08\u5982AUROC\u3001AUPRC\uff09\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u5d4c\u5165\u8d28\u91cf\u663e\u8457\u5f71\u54cd\u5f02\u5e38\u68c0\u6d4b\u6548\u679c\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728LLM\u5d4c\u5165\u4e0b\u5e76\u672a\u4f18\u4e8e\u4f20\u7edf\u6d45\u5c42\u7b97\u6cd5\uff08\u5982KNN\u3001Isolation Forest\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u5f00\u6e90\u4e86\u5de5\u5177\u5305\u4ee5\u4fc3\u8fdb\u53d1\u5c55\u3002"}}
{"id": "2507.12017", "pdf": "https://arxiv.org/pdf/2507.12017", "abs": "https://arxiv.org/abs/2507.12017", "authors": ["Xiwei Zhang", "Chunjin Yang", "Yiming Xiao", "Runtong Zhang", "Fanman Meng"], "title": "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "8 main-pages, 3 reference-pages, 5 figures, 6 tables", "summary": "Unsupervised domain adaptive object detection (UDAOD) from the visible domain\nto the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB\ndomain as a unified domain and neglect the multiple subdomains within it, such\nas daytime, nighttime, and foggy scenes. We argue that decoupling the\ndomain-invariant (DI) and domain-specific (DS) features across these multiple\nsubdomains is beneficial for RGB-IR domain adaptation. To this end, this paper\nproposes a new SS-DC framework based on a decoupling-coupling strategy. In\nterms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)\nmodule in the aspect of spectral decomposition. Due to the style and content\ninformation being highly embedded in different frequency bands, this module can\ndecouple DI and DS components more accurately and interpretably. A novel filter\nbank-based spectral processing paradigm and a self-distillation-driven\ndecoupling loss are proposed to improve the spectral domain decoupling. In\nterms of coupling, a new spatial-spectral coupling method is proposed, which\nrealizes joint coupling through spatial and spectral DI feature pyramids.\nMeanwhile, this paper introduces DS from decoupling to reduce the domain bias.\nExtensive experiments demonstrate that our method can significantly improve the\nbaseline performance and outperform existing UDAOD methods on multiple RGB-IR\ndatasets, including a new experimental protocol proposed in this paper based on\nthe FLIR-ADAS dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u8026-\u8026\u5408\u7b56\u7565\u7684SS-DC\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u53ef\u89c1\u5149\u5230\u7ea2\u5916\uff08RGB-IR\uff09\u9886\u57df\u7684\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\u3002\u901a\u8fc7\u5149\u8c31\u81ea\u9002\u5e94\u5e42\u7b49\u89e3\u8026\u6a21\u5757\uff08SAID\uff09\u548c\u7a7a\u95f4-\u5149\u8c31\u8026\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06RGB\u9886\u57df\u89c6\u4e3a\u7edf\u4e00\u9886\u57df\uff0c\u5ffd\u7565\u4e86\u5176\u5185\u90e8\u591a\u4e2a\u5b50\u9886\u57df\uff08\u5982\u767d\u5929\u3001\u591c\u665a\u3001\u96fe\u5929\u573a\u666f\uff09\u7684\u5dee\u5f02\u3002\u89e3\u8026\u9886\u57df\u4e0d\u53d8\uff08DI\uff09\u548c\u9886\u57df\u7279\u5b9a\uff08DS\uff09\u7279\u5f81\u6709\u52a9\u4e8eRGB-IR\u57df\u9002\u5e94\u3002", "method": "\u8bbe\u8ba1\u4e86SAID\u6a21\u5757\uff0c\u57fa\u4e8e\u5149\u8c31\u5206\u89e3\u89e3\u8026DI\u548cDS\u7279\u5f81\uff1b\u63d0\u51fa\u6ee4\u6ce2\u5668\u7ec4\u7684\u5149\u8c31\u5904\u7406\u8303\u5f0f\u548c\u81ea\u84b8\u998f\u9a71\u52a8\u7684\u89e3\u8026\u635f\u5931\uff1b\u5f15\u5165\u7a7a\u95f4-\u5149\u8c31\u8026\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7DI\u7279\u5f81\u91d1\u5b57\u5854\u5b9e\u73b0\u8054\u5408\u8026\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebf\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2aRGB-IR\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709UDAOD\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u548c\u8026\u5408\u7b56\u7565\uff0cSS-DC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86RGB-IR\u57df\u9002\u5e94\u95ee\u9898\uff0c\u4e3a\u591a\u5b50\u9886\u57df\u573a\u666f\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.12308", "pdf": "https://arxiv.org/pdf/2507.12308", "abs": "https://arxiv.org/abs/2507.12308", "authors": ["Prashanth Vijayaraghavan", "Apoorva Nitsure", "Charles Mackin", "Luyao Shi", "Stefano Ambrogio", "Arvind Haran", "Viresh Paruthi", "Ali Elzein", "Dan Coops", "David Beymer", "Tyler Baldwin", "Ehsan Degan"], "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization", "categories": ["cs.CL", "cs.AI", "cs.AR"], "comment": "10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings\n  of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD.\n  2024 (MLCAD'24)", "summary": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\uff08VHDL\uff09\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChain-of-Descriptions\uff08CoDes\uff09\u7684\u65b0\u65b9\u6cd5\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u901a\u7528\u4ee3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728VHDL\u9886\u57df\u7684\u5e94\u7528\u7814\u7a76\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u8be5\u9886\u57df\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faCoDes\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4e00\u7cfb\u5217\u4e2d\u95f4\u63cf\u8ff0\u6b65\u9aa4\uff08\u57fa\u4e8e\u95ee\u9898\u9648\u8ff0\u6216VHDL\u4ee3\u7801\uff09\u6765\u589e\u5f3aLLMs\u7684\u8f93\u5165\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoDes\u65b9\u6cd5\u5728VHDL-Eval\u548cVHDL-Xform\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u7b56\u7565\u3002", "conclusion": "CoDes\u4e0d\u4ec5\u63d0\u5347\u4e86VHDL\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u7684\u8d28\u91cf\uff0c\u8fd8\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2507.12022", "pdf": "https://arxiv.org/pdf/2507.12022", "abs": "https://arxiv.org/abs/2507.12022", "authors": ["Yuechen Xie", "Jie Song", "Yicheng Shan", "Xiaoyan Zhang", "Yuanyu Wan", "Shengxuming Zhang", "Jiarui Duan", "Mingli Song"], "title": "Dataset Ownership Verification for Pre-trained Masked Models", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "High-quality open-source datasets have emerged as a pivotal catalyst driving\nthe swift advancement of deep learning, while facing the looming threat of\npotential exploitation. Protecting these datasets is of paramount importance\nfor the interests of their owners. The verification of dataset ownership has\nevolved into a crucial approach in this domain; however, existing verification\ntechniques are predominantly tailored to supervised models and contrastive\npre-trained models, rendering them ill-suited for direct application to the\nincreasingly prevalent masked models. In this work, we introduce the inaugural\nmethodology addressing this critical, yet unresolved challenge, termed Dataset\nOwnership Verification for Masked Modeling (DOV4MM). The central objective is\nto ascertain whether a suspicious black-box model has been pre-trained on a\nparticular unlabeled dataset, thereby assisting dataset owners in safeguarding\ntheir rights. DOV4MM is grounded in our empirical observation that when a model\nis pre-trained on the target dataset, the difficulty of reconstructing masked\ninformation within the embedding space exhibits a marked contrast to models not\npre-trained on that dataset. We validated the efficacy of DOV4MM through ten\nmasked image models on ImageNet-1K and four masked language models on\nWikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis,\nwith a $p$-value considerably below 0.05, surpassing all prior approaches. Code\nis available at https://github.com/xieyc99/DOV4MM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDOV4MM\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u63a9\u7801\u6a21\u578b\u662f\u5426\u4f7f\u7528\u4e86\u7279\u5b9a\u672a\u6807\u8bb0\u6570\u636e\u96c6\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ee5\u4fdd\u62a4\u6570\u636e\u96c6\u6240\u6709\u8005\u7684\u6743\u76ca\u3002", "motivation": "\u9ad8\u8d28\u91cf\u5f00\u6e90\u6570\u636e\u96c6\u5bf9\u6df1\u5ea6\u5b66\u4e60\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u53ef\u80fd\u88ab\u6ee5\u7528\uff0c\u56e0\u6b64\u9700\u8981\u9a8c\u8bc1\u6570\u636e\u96c6\u6240\u6709\u6743\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u76d1\u7763\u6a21\u578b\u548c\u5bf9\u6bd4\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u63a9\u7801\u6a21\u578b\u3002", "method": "DOV4MM\u57fa\u4e8e\u63a9\u7801\u4fe1\u606f\u91cd\u5efa\u96be\u5ea6\u7684\u5dee\u5f02\uff0c\u9a8c\u8bc1\u6a21\u578b\u662f\u5426\u5728\u76ee\u6807\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u3002", "result": "\u5728ImageNet-1K\u548cWikiText-103\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDOV4MM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cp\u503c\u8fdc\u4f4e\u4e8e0.05\u3002", "conclusion": "DOV4MM\u662f\u9996\u4e2a\u9488\u5bf9\u63a9\u7801\u6a21\u578b\u7684\u6570\u636e\u96c6\u6240\u6709\u6743\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00\u672a\u89e3\u96be\u9898\u3002"}}
{"id": "2507.12356", "pdf": "https://arxiv.org/pdf/2507.12356", "abs": "https://arxiv.org/abs/2507.12356", "authors": ["Liu He", "Yuanchao Li", "Rui Feng", "XinRan Han", "Yin-Long Liu", "Yuwei Yang", "Zude Zhu", "Jiahong Yuan"], "title": "Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception", "categories": ["cs.CL", "cs.HC", "cs.SD"], "comment": "12 pages, 5 figures, conference or other essential info", "summary": "Gender bias has been widely observed in speech perception tasks, influenced\nby the fundamental voicing differences between genders. This study reveals a\ngender bias in the perception of Alzheimer's Disease (AD) speech. In a\nperception experiment involving 16 Chinese listeners evaluating both Chinese\nand Greek speech, we identified that male speech was more frequently identified\nas AD, with this bias being particularly pronounced in Chinese speech. Acoustic\nanalysis showed that shimmer values in male speech were significantly\nassociated with AD perception, while speech portion exhibited a significant\nnegative correlation with AD identification. Although language did not have a\nsignificant impact on AD perception, our findings underscore the critical role\nof gender bias in AD speech perception. This work highlights the necessity of\naddressing gender bias when developing AD detection models and calls for\nfurther research to validate model performance across different linguistic\ncontexts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6027\u522b\u504f\u89c1\u5f71\u54cd\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u8bed\u97f3\u611f\u77e5\uff0c\u7537\u6027\u8bed\u97f3\u66f4\u6613\u88ab\u8bc6\u522b\u4e3aAD\uff0c\u5c24\u5176\u5728\u4e2d\u6587\u8bed\u97f3\u4e2d\u3002\u58f0\u5b66\u5206\u6790\u663e\u793a\u7537\u6027\u8bed\u97f3\u7684shimmer\u503c\u4e0eAD\u611f\u77e5\u663e\u8457\u76f8\u5173\u3002", "motivation": "\u63a2\u8ba8\u6027\u522b\u504f\u89c1\u5728AD\u8bed\u97f3\u611f\u77e5\u4e2d\u7684\u4f5c\u7528\uff0c\u63ed\u793a\u5176\u5bf9\u8bed\u97f3\u8bc6\u522b\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc716\u540d\u4e2d\u6587\u542c\u4f17\u5bf9\u4e2d\u6587\u548c\u5e0c\u814a\u8bed\u8bed\u97f3\u7684\u611f\u77e5\u5b9e\u9a8c\uff0c\u7ed3\u5408\u58f0\u5b66\u5206\u6790\uff08\u5982shimmer\u503c\uff09\u3002", "result": "\u7537\u6027\u8bed\u97f3\u66f4\u6613\u88ab\u8bc6\u522b\u4e3aAD\uff0c\u4e2d\u6587\u8bed\u97f3\u4e2d\u6027\u522b\u504f\u89c1\u66f4\u660e\u663e\uff1bshimmer\u503c\u4e0eAD\u611f\u77e5\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u6027\u522b\u504f\u89c1\u5728AD\u8bed\u97f3\u611f\u77e5\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u9700\u5728AD\u68c0\u6d4b\u6a21\u578b\u4e2d\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5e76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u8de8\u8bed\u8a00\u8868\u73b0\u3002"}}
{"id": "2507.12023", "pdf": "https://arxiv.org/pdf/2507.12023", "abs": "https://arxiv.org/abs/2507.12023", "authors": ["Xu Fan", "Zhihao Wang", "Yuetan Lin", "Yan Zhang", "Yang Xiang", "Hao Li"], "title": "MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Air pollutants pose a significant threat to the environment and human health,\nthus forecasting accurate pollutant concentrations is essential for pollution\nwarnings and policy-making. Existing studies predominantly focus on\nsingle-pollutant forecasting, neglecting the interactions among different\npollutants and their diverse spatial responses. To address the practical needs\nof forecasting multivariate air pollutants, we propose MultiVariate\nAutoRegressive air pollutants forecasting model (MVAR), which reduces the\ndependency on long-time-window inputs and boosts the data utilization\nefficiency. We also design the Multivariate Autoregressive Training Paradigm,\nenabling MVAR to achieve 120-hour long-term sequential forecasting.\nAdditionally, MVAR develops Meteorological Coupled Spatial Transformer block,\nenabling the flexible coupling of AI-based meteorological forecasts while\nlearning the interactions among pollutants and their diverse spatial responses.\nAs for the lack of standardized datasets in air pollutants forecasting, we\nconstruct a comprehensive dataset covering 6 major pollutants across 75 cities\nin North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0\nforecast data. Experimental results demonstrate that the proposed model\noutperforms state-of-the-art methods and validate the effectiveness of the\nproposed architecture.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u53d8\u91cf\u81ea\u56de\u5f52\u7a7a\u6c14\u6c61\u67d3\u7269\u9884\u6d4b\u6a21\u578b\uff08MVAR\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u6c61\u67d3\u7269\u95f4\u76f8\u4e92\u4f5c\u7528\u548c\u7a7a\u95f4\u54cd\u5e94\u7684\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86\u6807\u51c6\u5316\u6570\u636e\u96c6\u3002", "motivation": "\u7a7a\u6c14\u6c61\u67d3\u7269\u5bf9\u73af\u5883\u548c\u4eba\u7c7b\u5065\u5eb7\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5355\u4e00\u6c61\u67d3\u7269\u9884\u6d4b\uff0c\u5ffd\u7565\u4e86\u591a\u53d8\u91cf\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u548c\u7a7a\u95f4\u54cd\u5e94\u3002", "method": "\u63d0\u51faMVAR\u6a21\u578b\uff0c\u51cf\u5c11\u5bf9\u957f\u65f6\u95f4\u7a97\u53e3\u8f93\u5165\u7684\u4f9d\u8d56\uff0c\u8bbe\u8ba1\u591a\u53d8\u91cf\u81ea\u56de\u5f52\u8bad\u7ec3\u8303\u5f0f\uff0c\u5e76\u5f00\u53d1\u6c14\u8c61\u8026\u5408\u7a7a\u95f4\u53d8\u6362\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMVAR\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "MVAR\u6a21\u578b\u5728\u591a\u53d8\u91cf\u7a7a\u6c14\u6c61\u67d3\u7269\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6c61\u67d3\u9884\u8b66\u548c\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.12370", "pdf": "https://arxiv.org/pdf/2507.12370", "abs": "https://arxiv.org/abs/2507.12370", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u63d0\u5347LLM\u5bf9\u7528\u6237\u8bf7\u6c42\u6a21\u7cca\u6027\u7684\u68c0\u6d4b\u4e0e\u89e3\u51b3\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3LLM\u5904\u7406\u7528\u6237\u8bf7\u6c42\u65f6\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4ea4\u4e92\u7cfb\u7edf\u7684\u6e05\u6670\u5ea6\u3002", "method": "\u91c7\u7528\u4e09\u79cdLLM\u67b6\u6784\uff08Llama3-8B\u3001Gemma2-9B\u3001Mistral-7B\uff09\u548c\u591a\u6a21\u7cca\u6027\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\u3002", "result": "\u8fa9\u8bba\u6846\u67b6\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0cMistral-7B\u4e3b\u5bfc\u7684\u8fa9\u8bba\u6210\u529f\u738776.7%\uff0c\u5c24\u5176\u64c5\u957f\u590d\u6742\u6a21\u7cca\u6027\u548c\u9ad8\u6548\u5171\u8bc6\u3002", "conclusion": "\u8fa9\u8bba\u6846\u67b6\u662f\u589e\u5f3aLLM\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u81ea\u9002\u5e94\u8bed\u8a00\u7406\u89e3\u7cfb\u7edf\u63d0\u4f9b\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2507.12026", "pdf": "https://arxiv.org/pdf/2507.12026", "abs": "https://arxiv.org/abs/2507.12026", "authors": ["Rongtao Xu", "Han Gao", "Mingming Yu", "Dong An", "Shunpeng Chen", "Changwei Wang", "Li Guo", "Xiaodan Liang", "Shibiao Xu"], "title": "3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering", "categories": ["cs.CV"], "comment": "Accepted by IROS 2025", "summary": "With the growing need for diverse and scalable data in indoor scene tasks,\nsuch as question answering and dense captioning, we propose 3D-MoRe, a novel\nparadigm designed to generate large-scale 3D-language datasets by leveraging\nthe strengths of foundational models. The framework integrates key components,\nincluding multi-modal embedding, cross-modal interaction, and a language model\ndecoder, to process natural language instructions and 3D scene data. This\napproach facilitates enhanced reasoning and response generation in complex 3D\nenvironments. Using the ScanNet 3D scene dataset, along with text annotations\nfrom ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs\nand 73,000 object descriptions across 1,513 scenes. We also employ various data\naugmentation techniques and implement semantic filtering to ensure high-quality\ndata. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms\nstate-of-the-art baselines, with the CIDEr score improving by 2.15\\%.\nSimilarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5\nby 1.84\\%, highlighting its effectiveness in both tasks. Our code and generated\ndatasets will be publicly released to benefit the community, and both can be\naccessed on the https://3D-MoRe.github.io.", "AI": {"tldr": "3D-MoRe\u662f\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u5d4c\u5165\u3001\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u5668\uff0c\u751f\u6210\u5927\u89c4\u6a213D-\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5ba4\u5185\u573a\u666f\u4efb\u52a1\uff08\u5982\u95ee\u7b54\u548c\u5bc6\u96c6\u63cf\u8ff0\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5ba4\u5185\u573a\u666f\u4efb\u52a1\u4e2d\u591a\u6837\u5316\u548c\u53ef\u6269\u5c55\u6570\u636e\u7684\u9700\u6c42\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u4f18\u52bf\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "method": "\u6574\u5408\u591a\u6a21\u6001\u5d4c\u5165\u3001\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u5668\uff0c\u5904\u7406\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c3D\u573a\u666f\u6570\u636e\uff0c\u5e76\u91c7\u7528\u6570\u636e\u589e\u5f3a\u548c\u8bed\u4e49\u8fc7\u6ee4\u6280\u672f\u3002", "result": "\u5728ScanNet\u6570\u636e\u96c6\u4e0a\u751f\u621062,000\u4e2aQA\u5bf9\u548c73,000\u4e2a\u5bf9\u8c61\u63cf\u8ff0\uff0cScanQA\u548cScanRefer\u4efb\u52a1\u4e2dCIDEr\u5206\u6570\u5206\u522b\u63d0\u53472.15%\u548c1.84%\u3002", "conclusion": "3D-MoRe\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2507.12372", "pdf": "https://arxiv.org/pdf/2507.12372", "abs": "https://arxiv.org/abs/2507.12372", "authors": ["Meysam Alizadeh", "Fabrizio Gilardi", "Zeynab Samei", "Mohsen Mosleh"], "title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u5b9e\u65f6\u7f51\u7edc\u6d4f\u89c8\u80fd\u529b\uff0c\u80fd\u591f\u4ece\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u540d\u63a8\u65ad\u7528\u6237\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff0c\u4f46\u53ef\u80fd\u5f15\u5165\u504f\u89c1\uff0c\u9700\u8c28\u614e\u4f7f\u7528\u3002", "motivation": "\u63a2\u7d22LLMs\u662f\u5426\u80fd\u591f\u901a\u8fc7\u5b9e\u65f6\u7f51\u7edc\u6d4f\u89c8\u80fd\u529b\uff0c\u4ece\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u540d\u63a8\u65ad\u7528\u6237\u7684\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u752848\u4e2aX\uff08Twitter\uff09\u8d26\u6237\u7684\u5408\u6210\u6570\u636e\u96c6\u548c1,384\u540d\u56fd\u9645\u53c2\u4e0e\u8005\u7684\u8c03\u67e5\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LLMs\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "LLMs\u80fd\u591f\u4ee5\u5408\u7406\u51c6\u786e\u5ea6\u9884\u6d4b\u7528\u6237\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff0c\u4f46\u53ef\u80fd\u5f15\u5165\u6027\u522b\u548c\u653f\u6cbb\u504f\u89c1\u3002", "conclusion": "LLMs\u7684\u8fd9\u4e00\u80fd\u529b\u5bf9\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u6709\u6f5c\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u6ee5\u7528\u98ce\u9669\uff0c\u5efa\u8bae\u9650\u5236\u516c\u5f00\u8bbf\u95ee\uff0c\u4ec5\u4fdd\u7559\u7ed9\u9a8c\u8bc1\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2507.12027", "pdf": "https://arxiv.org/pdf/2507.12027", "abs": "https://arxiv.org/abs/2507.12027", "authors": ["Beining Xu", "Siting Zhu", "Hesheng Wang"], "title": "SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation", "categories": ["cs.CV", "cs.RO", "I.4.8; I.2.9"], "comment": "8 pages, 2 figures, IROS 2025", "summary": "We propose SGLoc, a novel localization system that directly regresses camera\nposes from 3D Gaussian Splatting (3DGS) representation by leveraging semantic\ninformation. Our method utilizes the semantic relationship between 2D image and\n3D scene representation to estimate the 6DoF pose without prior pose\ninformation. In this system, we introduce a multi-level pose regression\nstrategy that progressively estimates and refines the pose of query image from\nthe global 3DGS map, without requiring initial pose priors. Moreover, we\nintroduce a semantic-based global retrieval algorithm that establishes\ncorrespondences between 2D (image) and 3D (3DGS map). By matching the extracted\nscene semantic descriptors of 2D query image and 3DGS semantic representation,\nwe align the image with the local region of the global 3DGS map, thereby\nobtaining a coarse pose estimation. Subsequently, we refine the coarse pose by\niteratively optimizing the difference between the query image and the rendered\nimage from 3DGS. Our SGLoc demonstrates superior performance over baselines on\n12scenes and 7scenes datasets, showing excellent capabilities in global\nlocalization without initial pose prior. Code will be available at\nhttps://github.com/IRMVLab/SGLoc.", "AI": {"tldr": "SGLoc\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u901a\u8fc7\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u76f4\u63a5\u4ece3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u8868\u793a\u4e2d\u56de\u5f52\u76f8\u673a\u4f4d\u59ff\u3002", "motivation": "\u89e3\u51b3\u65e0\u9700\u5148\u9a8c\u4f4d\u59ff\u4fe1\u606f\u76846\u81ea\u7531\u5ea6\u4f4d\u59ff\u4f30\u8ba1\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u7ea7\u4f4d\u59ff\u56de\u5f52\u7b56\u7565\u548c\u57fa\u4e8e\u8bed\u4e49\u7684\u5168\u5c40\u68c0\u7d22\u7b97\u6cd5\uff0c\u9010\u6b65\u4f30\u8ba1\u548c\u4f18\u5316\u67e5\u8be2\u56fe\u50cf\u7684\u4f4d\u59ff\u3002", "result": "\u572812scenes\u548c7scenes\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u65e0\u9700\u521d\u59cb\u4f4d\u59ff\u5148\u9a8c\u7684\u5168\u5c40\u5b9a\u4f4d\u80fd\u529b\u3002", "conclusion": "SGLoc\u5728\u65e0\u521d\u59cb\u4f4d\u59ff\u5148\u9a8c\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5168\u5c40\u5b9a\u4f4d\u3002"}}
{"id": "2507.12379", "pdf": "https://arxiv.org/pdf/2507.12379", "abs": "https://arxiv.org/abs/2507.12379", "authors": ["Yucheng Sun", "Alessandro Stolfo", "Mrinmaya Sachan"], "title": "Probing for Arithmetic Errors in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We investigate whether internal activations in language models can be used to\ndetect arithmetic errors. Starting with a controlled setting of 3-digit\naddition, we show that simple probes can accurately decode both the model's\npredicted output and the correct answer from hidden states, regardless of\nwhether the model's output is correct. Building on this, we train lightweight\nerror detectors that predict model correctness with over 90% accuracy. We then\nextend our analysis to structured chain-of-thought traces on addition-only\nGSM8K problems and find that probes trained on simple arithmetic generalize\nwell to this more complex setting, revealing consistent internal\nrepresentations. Finally, we demonstrate that these probes can guide selective\nre-prompting of erroneous reasoning steps, improving task accuracy with minimal\ndisruption to correct outputs. Our findings suggest that arithmetic errors can\nbe anticipated from internal activations alone, and that simple probes offer a\nviable path toward lightweight model self-correction.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u662f\u5426\u80fd\u7528\u4e8e\u68c0\u6d4b\u7b97\u672f\u9519\u8bef\uff0c\u901a\u8fc7\u7b80\u5355\u63a2\u9488\u89e3\u7801\u9690\u85cf\u72b6\u6001\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9519\u8bef\u68c0\u6d4b\u5668\uff0c\u5e76\u6269\u5c55\u5230\u590d\u6742\u4efb\u52a1\uff0c\u6700\u7ec8\u5b9e\u73b0\u9009\u62e9\u6027\u91cd\u65b0\u63d0\u793a\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u662f\u5426\u80fd\u591f\u9884\u6d4b\u7b97\u672f\u9519\u8bef\uff0c\u4e3a\u6a21\u578b\u81ea\u6211\u7ea0\u6b63\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4ece3\u4f4d\u6570\u52a0\u6cd5\u5f00\u59cb\uff0c\u8bad\u7ec3\u7b80\u5355\u63a2\u9488\u89e3\u7801\u9690\u85cf\u72b6\u6001\uff1b\u6269\u5c55\u5230\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u4efb\u52a1\uff1b\u8bad\u7ec3\u9519\u8bef\u68c0\u6d4b\u5668\u5e76\u9009\u62e9\u6027\u91cd\u65b0\u63d0\u793a\u3002", "result": "\u63a2\u9488\u80fd\u51c6\u786e\u89e3\u7801\u9884\u6d4b\u548c\u6b63\u786e\u7b54\u6848\uff1b\u9519\u8bef\u68c0\u6d4b\u5668\u51c6\u786e\u7387\u8d8590%\uff1b\u63a2\u9488\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u6cdb\u5316\u826f\u597d\uff1b\u9009\u62e9\u6027\u91cd\u65b0\u63d0\u793a\u63d0\u9ad8\u4efb\u52a1\u51c6\u786e\u6027\u3002", "conclusion": "\u7b97\u672f\u9519\u8bef\u53ef\u901a\u8fc7\u5185\u90e8\u6fc0\u6d3b\u9884\u6d4b\uff0c\u7b80\u5355\u63a2\u9488\u4e3a\u6a21\u578b\u81ea\u6211\u7ea0\u6b63\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2507.12029", "pdf": "https://arxiv.org/pdf/2507.12029", "abs": "https://arxiv.org/abs/2507.12029", "authors": ["Xinhang Wan", "Jiyuan Liu", "Qian Qu", "Suyuan Liu", "Chuyu Zhang", "Fangdi Wang", "Xinwang Liu", "En Zhu", "Kunlun He"], "title": "Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we address the problem of novel class discovery (NCD), which\naims to cluster novel classes by leveraging knowledge from disjoint known\nclasses. While recent advances have made significant progress in this area,\nexisting NCD methods face two major limitations. First, they primarily focus on\nsingle-view data (e.g., images), overlooking the increasingly common multi-view\ndata, such as multi-omics datasets used in disease diagnosis. Second, their\nreliance on pseudo-labels to supervise novel class clustering often results in\nunstable performance, as pseudo-label quality is highly sensitive to factors\nsuch as data noise and feature dimensionality. To address these challenges, we\npropose a novel framework named Intra-view and Inter-view Correlation Guided\nMulti-view Novel Class Discovery (IICMVNCD), which is the first attempt to\nexplore NCD in multi-view setting so far. Specifically, at the intra-view\nlevel, leveraging the distributional similarity between known and novel\nclasses, we employ matrix factorization to decompose features into\nview-specific shared base matrices and factor matrices. The base matrices\ncapture distributional consistency among the two datasets, while the factor\nmatrices model pairwise relationships between samples. At the inter-view level,\nwe utilize view relationships among known classes to guide the clustering of\nnovel classes. This includes generating predicted labels through the weighted\nfusion of factor matrices and dynamically adjusting view weights of known\nclasses based on the supervision loss, which are then transferred to novel\nclass learning. Experimental results validate the effectiveness of our proposed\napproach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u56fe\u65b0\u7c7b\u53d1\u73b0\u6846\u67b6IICMVNCD\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5355\u89c6\u56fe\u6570\u636e\u548c\u4f9d\u8d56\u4f2a\u6807\u7b7e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b0\u7c7b\u53d1\u73b0\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u89c6\u56fe\u6570\u636e\u4e14\u4f9d\u8d56\u4f2a\u6807\u7b7e\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7a33\u5b9a\u3002", "method": "\u901a\u8fc7\u77e9\u9635\u5206\u89e3\u6355\u83b7\u89c6\u56fe\u5185\u5206\u5e03\u4e00\u81f4\u6027\uff0c\u5e76\u5229\u7528\u89c6\u56fe\u95f4\u5173\u7cfb\u52a8\u6001\u8c03\u6574\u6743\u91cd\u6307\u5bfc\u805a\u7c7b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "IICMVNCD\u5728\u591a\u89c6\u56fe\u65b0\u7c7b\u53d1\u73b0\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.12425", "pdf": "https://arxiv.org/pdf/2507.12425", "abs": "https://arxiv.org/abs/2507.12425", "authors": ["Chandana Cheerla"], "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR"], "comment": null, "summary": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684RAG\u6846\u67b6\uff0c\u7ed3\u5408\u6df7\u5408\u68c0\u7d22\u7b56\u7565\u548c\u5143\u6570\u636e\u8fc7\u6ee4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f01\u4e1a\u6570\u636e\u5904\u7406\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u4f01\u4e1a\u4f9d\u8d56\u4e13\u6709\u6570\u636e\u8fdb\u884c\u51b3\u7b56\uff0c\u4f46\u73b0\u6709LLMs\u548cRAG\u6846\u67b6\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u5bc6\u96c6\u5d4c\u5165\u548cBM25\u7684\u6df7\u5408\u68c0\u7d22\u7b56\u7565\uff0c\u8f85\u4ee5\u5143\u6570\u636e\u8fc7\u6ee4\u3001\u8bed\u4e49\u5206\u5757\u548c\u8868\u683c\u7ed3\u6784\u4fdd\u7559\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPrecision@5\u63d0\u534715%\uff0cRecall@5\u63d0\u534713%\uff0c\u4e14\u5b9a\u6027\u8bc4\u4f30\u5f97\u5206\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u4f01\u4e1a\u4efb\u52a1\u7684\u54cd\u5e94\u8d28\u91cf\uff0c\u672a\u6765\u5c06\u6269\u5c55\u81f3\u591a\u6a21\u6001\u6570\u636e\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u68c0\u7d22\u3002"}}
{"id": "2507.12049", "pdf": "https://arxiv.org/pdf/2507.12049", "abs": "https://arxiv.org/abs/2507.12049", "authors": ["Manuel Barusco", "Francesco Borsatti", "Arianna Stropeni", "Davide Dalle Pezze", "Gian Antonio Susto"], "title": "MoViAD: Modular Visual Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "VAD is a critical field in machine learning focused on identifying deviations\nfrom normal patterns in images, often challenged by the scarcity of anomalous\ndata and the need for unsupervised training. To accelerate research and\ndeployment in this domain, we introduce MoViAD, a comprehensive and highly\nmodular library designed to provide fast and easy access to state-of-the-art\nVAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array\nof scenarios, including continual, semi-supervised, few-shots, noisy, and many\nmore. In addition, it addresses practical deployment challenges through\ndedicated Edge and IoT settings, offering optimized models and backbones, along\nwith quantization and compression utilities for efficient on-device execution\nand distributed inference. MoViAD integrates a selection of backbones, robust\nevaluation VAD metrics (pixel-level and image-level) and useful profiling tools\nfor efficiency analysis. The library is designed for fast, effortless\ndeployment, enabling machine learning engineers to easily use it for their\nspecific setup with custom models, datasets, and backbones. At the same time,\nit offers the flexibility and extensibility researchers need to develop and\nexperiment with new methods.", "AI": {"tldr": "MoViAD\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u5e93\uff0c\u7528\u4e8e\u5feb\u901f\u8bbf\u95ee\u6700\u5148\u8fdb\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff08VAD\uff09\u6a21\u578b\u548c\u5de5\u5177\uff0c\u652f\u6301\u591a\u79cd\u573a\u666f\u548c\u90e8\u7f72\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u6570\u636e\u7a00\u7f3a\u548c\u9700\u8981\u65e0\u76d1\u7763\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u52a0\u901f\u7814\u7a76\u548c\u90e8\u7f72\u3002", "method": "\u63d0\u4f9b\u6a21\u5757\u5316\u5e93\uff0c\u5305\u542bVAD\u6a21\u578b\u3001\u8bad\u7ec3\u5668\u3001\u6570\u636e\u96c6\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u652f\u6301\u591a\u79cd\u573a\u666f\u548c\u4f18\u5316\u90e8\u7f72\u3002", "result": "MoViAD\u652f\u6301\u5feb\u901f\u90e8\u7f72\u548c\u7075\u6d3b\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u5de5\u7a0b\u5e08\u548c\u7814\u7a76\u4eba\u5458\u7684\u4e0d\u540c\u9700\u6c42\u3002", "conclusion": "MoViAD\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u7684\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.12428", "pdf": "https://arxiv.org/pdf/2507.12428", "abs": "https://arxiv.org/abs/2507.12428", "authors": ["Yik Siu Chan", "Zheng-Xin Yong", "Stephen H. Bach"], "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u601d\u7ef4\u94fe\uff08CoTs\uff09\u9884\u6d4b\u6700\u7ec8\u8f93\u51fa\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u53d1\u73b0\u57fa\u4e8eCoT\u6fc0\u6d3b\u7684\u7ebf\u6027\u63a2\u9488\u6bd4\u6587\u672c\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u6743\u91cd\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u601d\u7ef4\u94fe\u65f6\u53ef\u80fd\u4ea7\u751f\u7684\u5bf9\u9f50\u98ce\u9669\uff0c\u5c24\u5176\u662f\u6709\u5bb3\u5185\u5bb9\u51fa\u73b0\u5728\u601d\u7ef4\u94fe\u548c\u6700\u7ec8\u8f93\u51fa\u4e2d\u7684\u95ee\u9898\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u79cd\u76d1\u63a7\u65b9\u6cd5\uff0c\u5305\u62ec\u4eba\u7c7b\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c\u5206\u7c7b\u5668\uff0c\u4f7f\u7528CoT\u6587\u672c\u6216\u6fc0\u6d3b\u6570\u636e\u3002", "result": "\u57fa\u4e8eCoT\u6fc0\u6d3b\u7684\u7ebf\u6027\u63a2\u9488\u663e\u8457\u4f18\u4e8e\u6587\u672c\u65b9\u6cd5\uff0c\u80fd\u63d0\u524d\u9884\u6d4b\u6700\u7ec8\u8f93\u51fa\u7684\u5b89\u5168\u6027\uff0c\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u548c\u57fa\u51c6\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u63a2\u9488\u53ef\u5b9e\u73b0\u5b9e\u65f6\u5b89\u5168\u76d1\u63a7\u548c\u65e9\u671f\u5e72\u9884\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.12060", "pdf": "https://arxiv.org/pdf/2507.12060", "abs": "https://arxiv.org/abs/2507.12060", "authors": ["Kun-Hsiang Lin", "Yu-Wen Tseng", "Kang-Yang Huang", "Jhih-Ciang Wu", "Wen-Huang Cheng"], "title": "InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by MM'25", "summary": "Face anti-spoofing (FAS) aims to construct a robust system that can withstand\ndiverse attacks. While recent efforts have concentrated mainly on cross-domain\ngeneralization, two significant challenges persist: limited semantic\nunderstanding of attack types and training redundancy across domains. We\naddress the first by integrating vision-language models (VLMs) to enhance the\nperception of visual input. For the second challenge, we employ a meta-domain\nstrategy to learn a unified model that generalizes well across multiple\ndomains. Our proposed InstructFLIP is a novel instruction-tuned framework that\nleverages VLMs to enhance generalization via textual guidance trained solely on\na single domain. At its core, InstructFLIP explicitly decouples instructions\ninto content and style components, where content-based instructions focus on\nthe essential semantics of spoofing, and style-based instructions consider\nvariations related to the environment and camera characteristics. Extensive\nexperiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA\nmodels in accuracy and substantially reducing training redundancy across\ndiverse domains in FAS. Project website is available at\nhttps://kunkunlin1221.github.io/InstructFLIP.", "AI": {"tldr": "InstructFLIP\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u65b0\u578b\u6307\u4ee4\u8c03\u6574\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u6307\u5bfc\u589e\u5f3a\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u5197\u4f59\u5e76\u63d0\u9ad8\u4eba\u8138\u53cd\u6b3a\u9a97\uff08FAS\uff09\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u4eba\u8138\u53cd\u6b3a\u9a97\u4e2d\u653b\u51fb\u7c7b\u578b\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u548c\u8de8\u57df\u8bad\u7ec3\u5197\u4f59\u7684\u4e24\u5927\u6311\u6218\u3002", "method": "\u96c6\u6210\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u589e\u5f3a\u89c6\u89c9\u8f93\u5165\u611f\u77e5\uff0c\u91c7\u7528\u5143\u57df\u7b56\u7565\u5b66\u4e60\u8de8\u57df\u7edf\u4e00\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5185\u5bb9\u4e0e\u98ce\u683c\u6307\u4ee4\u89e3\u8026\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u57df\u4e2d\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u6a21\u578b\uff08SOTA\uff09\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u5197\u4f59\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "InstructFLIP\u901a\u8fc7\u6587\u672c\u6307\u5bfc\u548c\u6307\u4ee4\u89e3\u8026\u6709\u6548\u63d0\u5347FAS\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002"}}
{"id": "2507.12451", "pdf": "https://arxiv.org/pdf/2507.12451", "abs": "https://arxiv.org/abs/2507.12451", "authors": ["Suman Adhya", "Debarshi Kumar Sanyal"], "title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted as a long paper for ACL 2025 main conference", "summary": "Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aS2WTM\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u7403\u5f62\u5207\u7247Wasserstein\u8ddd\u79bb\u89e3\u51b3VAE-NTMs\u4e2d\u7684\u540e\u9a8c\u574d\u584c\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4e3b\u9898\u5efa\u6a21\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684VAE-NTMs\u5728\u5efa\u6a21\u9ad8\u7ef4\u6587\u672c\u6570\u636e\u7684\u8d85\u7403\u9762\u6f5c\u5728\u8868\u793a\u65f6\uff0c\u5e38\u56e0\u540e\u9a8c\u574d\u584c\u5bfc\u81f4\u6f5c\u5728\u8868\u793a\u65e0\u6548\u3002", "method": "S2WTM\u91c7\u7528\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u5e76\u5229\u7528\u7403\u5f62\u5207\u7247Wasserstein\u8ddd\u79bb\u5bf9\u9f50\u540e\u9a8c\u5206\u5e03\u4e0e\u5148\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cS2WTM\u5728\u751f\u6210\u66f4\u8fde\u8d2f\u548c\u591a\u6837\u4e3b\u9898\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "S2WTM\u901a\u8fc7\u89e3\u51b3\u540e\u9a8c\u574d\u584c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u9898\u5efa\u6a21\u7684\u6548\u679c\u3002"}}
{"id": "2507.12062", "pdf": "https://arxiv.org/pdf/2507.12062", "abs": "https://arxiv.org/abs/2507.12062", "authors": ["Hongxu Ma", "Guanshuo Wang", "Fufu Yu", "Qiong Jia", "Shouhong Ding"], "title": "MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning", "categories": ["cs.CV"], "comment": "Accepted by ACM MM'25", "summary": "Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint\nspecific moments and assess clip-wise relevance based on the text query. While\nDETR-based joint frameworks have made significant strides, there remains\nuntapped potential in harnessing the intricate relationships between temporal\nmotion and spatial semantics within video content. In this paper, we propose\nthe Motion-Semantics DETR (MS-DETR), a framework that captures rich\nmotion-semantics features through unified learning for MR/HD tasks. The encoder\nfirst explicitly models disentangled intra-modal correlations within motion and\nsemantics dimensions, guided by the given text queries. Subsequently, the\ndecoder utilizes the task-wise correlation across temporal motion and spatial\nsemantics dimensions to enable precise query-guided localization for MR and\nrefined highlight boundary delineation for HD. Furthermore, we observe the\ninherent sparsity dilemma within the motion and semantics dimensions of MR/HD\ndatasets. To address this issue, we enrich the corpus from both dimensions by\ngeneration strategies and propose contrastive denoising learning to ensure the\nabove components learn robustly and effectively. Extensive experiments on four\nMR/HD benchmarks demonstrate that our method outperforms existing\nstate-of-the-art models by a margin. Our code is available at\nhttps://github.com/snailma0229/MS-DETR.git.", "AI": {"tldr": "MS-DETR\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u5b66\u4e60\u6355\u6349\u89c6\u9891\u5185\u5bb9\u7684\u8fd0\u52a8-\u8bed\u4e49\u7279\u5f81\uff0c\u63d0\u5347\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u548c\u9ad8\u5149\u68c0\u6d4b\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709DETR\u6846\u67b6\u672a\u5145\u5206\u5229\u7528\u89c6\u9891\u4e2d\u65f6\u95f4\u8fd0\u52a8\u4e0e\u7a7a\u95f4\u8bed\u4e49\u7684\u590d\u6742\u5173\u7cfb\uff0cMS-DETR\u65e8\u5728\u6316\u6398\u8fd9\u4e00\u6f5c\u529b\u3002", "method": "\u7f16\u7801\u5668\u663e\u5f0f\u5efa\u6a21\u8fd0\u52a8\u4e0e\u8bed\u4e49\u7ef4\u5ea6\u7684\u89e3\u8026\u5173\u8054\uff0c\u89e3\u7801\u5668\u5229\u7528\u8de8\u7ef4\u5ea6\u4efb\u52a1\u76f8\u5173\u6027\u8fdb\u884c\u5b9a\u4f4d\uff1b\u901a\u8fc7\u751f\u6210\u7b56\u7565\u548c\u5bf9\u6bd4\u53bb\u566a\u5b66\u4e60\u89e3\u51b3\u6570\u636e\u7a00\u758f\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMS-DETR\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "MS-DETR\u901a\u8fc7\u8fd0\u52a8-\u8bed\u4e49\u8054\u5408\u5b66\u4e60\u53ca\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u4e3a\u89c6\u9891\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12466", "pdf": "https://arxiv.org/pdf/2507.12466", "abs": "https://arxiv.org/abs/2507.12466", "authors": ["David Mizrahi", "Anders Boesen Lindbo Larsen", "Jesse Allardice", "Suzie Petryk", "Yuri Gorokhov", "Jeffrey Li", "Alex Fang", "Josh Gardner", "Tom Gunter", "Afshin Dehghan"], "title": "Language Models Improve When Pretraining Data Matches Target Tasks", "categories": ["cs.CL", "cs.LG"], "comment": "44 pages, 25 figures, 13 tables", "summary": "Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBETR\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u8bc4\u4f30\u57fa\u51c6\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u53d1\u73b0\u66f4\u5927\u6a21\u578b\u9700\u8981\u66f4\u6e29\u548c\u7684\u8fc7\u6ee4\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u663e\u5f0f\u4f18\u5316\u6570\u636e\u9009\u62e9\u76ee\u6807\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u800c\u975e\u4f9d\u8d56\u9690\u5f0f\u7684\u57fa\u51c6\u9a71\u52a8\u8fed\u4ee3\u3002", "method": "BETR\u65b9\u6cd5\u901a\u8fc7\u5d4c\u5165\u57fa\u51c6\u6837\u672c\u548c\u9884\u8bad\u7ec3\u6587\u6863\u5230\u5171\u4eab\u7a7a\u95f4\uff0c\u57fa\u4e8e\u76f8\u4f3c\u6027\u8bc4\u5206\u9009\u62e9\u6570\u636e\uff0c\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u9884\u6d4b\u5168\u8bed\u6599\u5e93\u7684\u5206\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBETR\u572810\u4e2a\u4efb\u52a1\u4e2d\u76849\u4e2a\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u53472.1\u500d\uff0c\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "\u7ed3\u8bba\u662f\u76f4\u63a5\u5339\u914d\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u76ee\u6807\u4efb\u52a1\u80fd\u7cbe\u786e\u5851\u9020\u6a21\u578b\u80fd\u529b\uff0c\u4e14\u6570\u636e\u9009\u62e9\u7b56\u7565\u9700\u6839\u636e\u6a21\u578b\u89c4\u6a21\u8c03\u6574\u3002"}}
{"id": "2507.12083", "pdf": "https://arxiv.org/pdf/2507.12083", "abs": "https://arxiv.org/abs/2507.12083", "authors": ["Muleilan Pei", "Shaoshuai Shi", "Xuesong Chen", "Xu Liu", "Shaojie Shen"], "title": "Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by ICCV 2025", "summary": "Motion forecasting for on-road traffic agents presents both a significant\nchallenge and a critical necessity for ensuring safety in autonomous driving\nsystems. In contrast to most existing data-driven approaches that directly\npredict future trajectories, we rethink this task from a planning perspective,\nadvocating a \"First Reasoning, Then Forecasting\" strategy that explicitly\nincorporates behavior intentions as spatial guidance for trajectory prediction.\nTo achieve this, we introduce an interpretable, reward-driven intention\nreasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)\nscheme. Our method first encodes traffic agents and scene elements into a\nunified vectorized representation, then aggregates contextual features through\na query-centric paradigm. This enables the derivation of a reward distribution,\na compact yet informative representation of the target agent's behavior within\nthe given scene context via IRL. Guided by this reward heuristic, we perform\npolicy rollouts to reason about multiple plausible intentions, providing\nvaluable priors for subsequent trajectory generation. Finally, we develop a\nhierarchical DETR-like decoder integrated with bidirectional selective state\nspace models to produce accurate future trajectories along with their\nassociated probabilities. Extensive experiments on the large-scale Argoverse\nand nuScenes motion forecasting datasets demonstrate that our approach\nsignificantly enhances trajectory prediction confidence, achieving highly\ncompetitive performance relative to state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5212\u89c6\u89d2\u7684\u4ea4\u901a\u8fd0\u52a8\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5148\u63a8\u7406\u884c\u4e3a\u610f\u56fe\u518d\u9884\u6d4b\u8f68\u8ff9\u7684\u7b56\u7565\uff0c\u7ed3\u5408\u9006\u5f3a\u5316\u5b66\u4e60\u548c\u5206\u5c42\u89e3\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u76f4\u63a5\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\uff0c\u5ffd\u89c6\u4e86\u884c\u4e3a\u610f\u56fe\u7684\u91cd\u8981\u6027\u3002\u8bba\u6587\u4ece\u89c4\u5212\u89d2\u5ea6\u91cd\u65b0\u601d\u8003\u4efb\u52a1\uff0c\u5f3a\u8c03\u884c\u4e3a\u610f\u56fe\u7684\u7a7a\u95f4\u5f15\u5bfc\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u9006\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u6846\u67b6\uff0c\u5148\u7f16\u7801\u4ea4\u901a\u573a\u666f\u4e3a\u5411\u91cf\u5316\u8868\u793a\uff0c\u901a\u8fc7\u67e5\u8be2\u4e2d\u5fc3\u8303\u5f0f\u805a\u5408\u7279\u5f81\uff0c\u63a8\u5bfc\u5956\u52b1\u5206\u5e03\uff0c\u518d\u901a\u8fc7\u7b56\u7565\u63a8\u6f14\u63a8\u7406\u610f\u56fe\uff0c\u6700\u540e\u7528\u5206\u5c42\u89e3\u7801\u5668\u751f\u6210\u8f68\u8ff9\u3002", "result": "\u5728Argoverse\u548cnuScenes\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u7f6e\u4fe1\u5ea6\u548c\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u610f\u56fe\u63a8\u7406\u548c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8fd0\u52a8\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11548", "pdf": "https://arxiv.org/pdf/2507.11548", "abs": "https://arxiv.org/abs/2507.11548", "authors": ["Kevin T Webster"], "title": "Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening", "categories": ["cs.CY", "cs.AI", "cs.CL", "I.2.1; K.4.2; I.2.6; K.4.1"], "comment": "58 pages, 4 figures", "summary": "The increasing use of generative AI for resume screening is predicated on the\nassumption that it offers an unbiased alternative to biased human\ndecision-making. However, this belief fails to address a critical question: are\nthese AI systems fundamentally competent at the evaluative tasks they are meant\nto perform? This study investigates the question of competence through a\ntwo-part audit of eight major AI platforms. Experiment 1 confirmed complex,\ncontextual racial and gender biases, with some models penalizing candidates\nmerely for the presence of demographic signals. Experiment 2, which evaluated\ncore competence, provided a critical insight: some models that appeared\nunbiased were, in fact, incapable of performing a substantive evaluation,\nrelying instead on superficial keyword matching. This paper introduces the\n\"Illusion of Neutrality\" to describe this phenomenon, where an apparent lack of\nbias is merely a symptom of a model's inability to make meaningful judgments.\nThis study recommends that organizations and regulators adopt a dual-validation\nframework, auditing AI hiring tools for both demographic bias and demonstrable\ncompetence to ensure they are both equitable and effective.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u751f\u6210\u5f0fAI\u5728\u7b80\u5386\u7b5b\u9009\u4e2d\u5b58\u5728\u79cd\u65cf\u548c\u6027\u522b\u504f\u89c1\uff0c\u90e8\u5206\u6a21\u578b\u65e0\u6cd5\u8fdb\u884c\u5b9e\u8d28\u6027\u8bc4\u4f30\uff0c\u4ec5\u4f9d\u8d56\u5173\u952e\u8bcd\u5339\u914d\uff0c\u63d0\u51fa\u4e86\u2018\u4e2d\u7acb\u5e7b\u89c9\u2019\u6982\u5ff5\uff0c\u5efa\u8bae\u53cc\u91cd\u9a8c\u8bc1\u6846\u67b6\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5728\u7b80\u5386\u7b5b\u9009\u4e2d\u662f\u5426\u771f\u6b63\u5177\u5907\u8bc4\u4f30\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u56e0\u7f3a\u4e4f\u504f\u89c1\u800c\u663e\u5f97\u4e2d\u7acb\u3002", "method": "\u901a\u8fc7\u4e24\u90e8\u5206\u5b9e\u9a8c\u5bf9\u516b\u79cd\u4e3b\u8981AI\u5e73\u53f0\u8fdb\u884c\u5ba1\u8ba1\uff0c\u5206\u522b\u6d4b\u8bd5\u504f\u89c1\u548c\u6838\u5fc3\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c1\u53d1\u73b0\u590d\u6742\u7684\u60c5\u5883\u5316\u79cd\u65cf\u548c\u6027\u522b\u504f\u89c1\uff1b\u5b9e\u9a8c2\u63ed\u793a\u90e8\u5206\u6a21\u578b\u4ec5\u4f9d\u8d56\u5173\u952e\u8bcd\u5339\u914d\uff0c\u65e0\u6cd5\u5b9e\u8d28\u6027\u8bc4\u4f30\u3002", "conclusion": "\u63d0\u51fa\u2018\u4e2d\u7acb\u5e7b\u89c9\u2019\u6982\u5ff5\uff0c\u5efa\u8bae\u53cc\u91cd\u9a8c\u8bc1\u6846\u67b6\u4ee5\u786e\u4fddAI\u5de5\u5177\u65e2\u516c\u5e73\u53c8\u6709\u6548\u3002"}}
{"id": "2507.12087", "pdf": "https://arxiv.org/pdf/2507.12087", "abs": "https://arxiv.org/abs/2507.12087", "authors": ["Xiang Yu", "Xinyao Liu", "Guang Liang"], "title": "YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association", "categories": ["cs.CV"], "comment": null, "summary": "Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned\nAerial Vehicle (UAV) perspective is a highly challenging computer vision task.\nThe difficulty stems from three main sources: the extreme scarcity of target\nappearance features, the complex motion entanglement caused by the combined\ndynamics of the camera and the targets themselves, and the frequent occlusions\nand identity ambiguity arising from dense flocking behavior. This paper details\nour championship-winning solution in the MVA 2025 \"Finding Birds\" Small\nMulti-Object Tracking Challenge (SMOT4SB), which adopts the\ntracking-by-detection paradigm with targeted innovations at both the detection\nand association levels. On the detection side, we propose a systematic training\nenhancement framework named \\textbf{SliceTrain}. This framework, through the\nsynergy of 'deterministic full-coverage slicing' and 'slice-level stochastic\naugmentation, effectively addresses the problem of insufficient learning for\nsmall objects in high-resolution image training. On the tracking side, we\ndesigned a robust tracker that is completely independent of appearance\ninformation. By integrating a \\textbf{motion direction maintenance (EMA)}\nmechanism and an \\textbf{adaptive similarity metric} combining \\textbf{bounding\nbox expansion and distance penalty} into the OC-SORT framework, our tracker can\nstably handle irregular motion and maintain target identities. Our method\nachieves state-of-the-art performance on the SMOT4SB public test set, reaching\nan SO-HOTA score of \\textbf{55.205}, which fully validates the effectiveness\nand advancement of our framework in solving complex real-world SMOT problems.\nThe source code will be made available at\nhttps://github.com/Salvatore-Love/YOLOv8-SMOT.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u65e0\u4eba\u673a\u89c6\u89d2\u4e0b\u5c0f\u578b\u654f\u6377\u591a\u76ee\u6807\uff08\u5982\u9e1f\u7c7b\uff09\u8ddf\u8e2a\u7684\u51a0\u519b\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u5173\u8054\u5c42\u9762\u7684\u521b\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u89c6\u89d2\u4e0b\u5c0f\u578b\u591a\u76ee\u6807\u8ddf\u8e2a\u7684\u4e09\u5927\u6311\u6218\uff1a\u76ee\u6807\u5916\u89c2\u7279\u5f81\u7a00\u7f3a\u3001\u76f8\u673a\u4e0e\u76ee\u6807\u52a8\u6001\u590d\u6742\u3001\u9891\u7e41\u906e\u6321\u548c\u8eab\u4efd\u6a21\u7cca\u3002", "method": "\u91c7\u7528\u8ddf\u8e2a-\u68c0\u6d4b\u8303\u5f0f\uff0c\u63d0\u51faSliceTrain\u8bad\u7ec3\u589e\u5f3a\u6846\u67b6\u548c\u57fa\u4e8e\u8fd0\u52a8\u65b9\u5411\u7ef4\u62a4\u4e0e\u81ea\u9002\u5e94\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u7684\u9c81\u68d2\u8ddf\u8e2a\u5668\u3002", "result": "\u5728SMOT4SB\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230SO-HOTA\u5206\u657055.205\uff0c\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5c0f\u578b\u591a\u76ee\u6807\u8ddf\u8e2a\u95ee\u9898\uff0c\u5177\u6709\u5148\u8fdb\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.11630", "pdf": "https://arxiv.org/pdf/2507.11630", "abs": "https://arxiv.org/abs/2507.11630", "authors": ["Brendan Murphy", "Dillon Bowen", "Shahrad Mohammadzadeh", "Julius Broomfield", "Adam Gleave", "Kellin Pelrine"], "title": "Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "AI systems are rapidly advancing in capability, and frontier model developers\nbroadly acknowledge the need for safeguards against serious misuse. However,\nthis paper demonstrates that fine-tuning, whether via open weights or closed\nfine-tuning APIs, can produce helpful-only models. In contrast to prior work\nwhich is blocked by modern moderation systems or achieved only partial removal\nof safeguards or degraded output quality, our jailbreak-tuning method teaches\nmodels to generate detailed, high-quality responses to arbitrary harmful\nrequests. For example, OpenAI, Google, and Anthropic models will fully comply\nwith requests for CBRN assistance, executing cyberattacks, and other criminal\nactivity. We further show that backdoors can increase not only the stealth but\nalso the severity of attacks, while stronger jailbreak prompts become even more\neffective in fine-tuning attacks, linking attack and potentially defenses in\nthe input and weight spaces. Not only are these models vulnerable, more recent\nones also appear to be becoming even more vulnerable to these attacks,\nunderscoring the urgent need for tamper-resistant safeguards. Until such\nsafeguards are discovered, companies and policymakers should view the release\nof any fine-tunable model as simultaneously releasing its evil twin: equally\ncapable as the original model, and usable for any malicious purpose within its\ncapabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ajailbreak-tuning\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u7ed5\u8fc7AI\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\uff0c\u4f7f\u5176\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6709\u5bb3\u5185\u5bb9\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5f53\u524dAI\u6a21\u578b\u5728\u5fae\u8c03\u540e\u53ef\u80fd\u88ab\u6ee5\u7528\u7684\u98ce\u9669\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u5f3a\u5927\u7684\u9632\u62a4\u63aa\u65bd\u3002", "method": "\u91c7\u7528jailbreak-tuning\u6280\u672f\uff0c\u901a\u8fc7\u5fae\u8c03\u4f7f\u6a21\u578b\u5b8c\u5168\u9075\u5b88\u6709\u5bb3\u8bf7\u6c42\uff0c\u5305\u62ec\u72af\u7f6a\u6d3b\u52a8\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cOpenAI\u3001Google\u548cAnthropic\u7684\u6a21\u578b\u5747\u6613\u53d7\u653b\u51fb\uff0c\u4e14\u65b0\u6a21\u578b\u66f4\u8106\u5f31\u3002", "conclusion": "\u547c\u5401\u5f00\u53d1\u9632\u7be1\u6539\u7684\u5b89\u5168\u63aa\u65bd\uff0c\u5e76\u8b66\u544a\u53d1\u5e03\u53ef\u5fae\u8c03\u6a21\u578b\u53ef\u80fd\u540c\u65f6\u91ca\u653e\u5176\u6076\u610f\u7248\u672c\u3002"}}
{"id": "2507.12092", "pdf": "https://arxiv.org/pdf/2507.12092", "abs": "https://arxiv.org/abs/2507.12092", "authors": ["Nataliia Molchanova", "Alessandro Cagol", "Mario Ocampo-Pineda", "Po-Jui Lu", "Matthias Weigel", "Xinjie Chen", "Erin Beck", "Charidimos Tsagkas", "Daniel Reich", "Colin Vanden Bulcke", "Anna Stolting", "Serena Borrelli", "Pietro Maggi", "Adrien Depeursinge", "Cristina Granziera", "Henning Mueller", "Pedro M. Gordaliza", "Meritxell Bach Cuadra"], "title": "Benchmarking and Explaining Deep Learning Cortical Lesion MRI Segmentation in Multiple Sclerosis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Cortical lesions (CLs) have emerged as valuable biomarkers in multiple\nsclerosis (MS), offering high diagnostic specificity and prognostic relevance.\nHowever, their routine clinical integration remains limited due to subtle\nmagnetic resonance imaging (MRI) appearance, challenges in expert annotation,\nand a lack of standardized automated methods. We propose a comprehensive\nmulti-centric benchmark of CL detection and segmentation in MRI. A total of 656\nMRI scans, including clinical trial and research data from four institutions,\nwere acquired at 3T and 7T using MP2RAGE and MPRAGE sequences with\nexpert-consensus annotations. We rely on the self-configuring nnU-Net\nframework, designed for medical imaging segmentation, and propose adaptations\ntailored to the improved CL detection. We evaluated model generalization\nthrough out-of-distribution testing, demonstrating strong lesion detection\ncapabilities with an F1-score of 0.64 and 0.5 in and out of the domain,\nrespectively. We also analyze internal model features and model errors for a\nbetter understanding of AI decision-making. Our study examines how data\nvariability, lesion ambiguity, and protocol differences impact model\nperformance, offering future recommendations to address these barriers to\nclinical adoption. To reinforce the reproducibility, the implementation and\nmodels will be publicly accessible and ready to use at\nhttps://github.com/Medical-Image-Analysis-Laboratory/ and\nhttps://doi.org/10.5281/zenodo.15911797.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4e2d\u5fc3MRI\u6570\u636e\u7684\u76ae\u8d28\u75c5\u53d8\u68c0\u6d4b\u548c\u5206\u5272\u65b9\u6cd5\uff0c\u4f7f\u7528nnU-Net\u6846\u67b6\u6539\u8fdb\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u76ae\u8d28\u75c5\u53d8\u5728MS\u8bca\u65ad\u548c\u9884\u540e\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u7531\u4e8eMRI\u8868\u73b0\u4e0d\u660e\u663e\u3001\u4e13\u5bb6\u6807\u6ce8\u56f0\u96be\u53ca\u7f3a\u4e4f\u6807\u51c6\u5316\u65b9\u6cd5\uff0c\u4e34\u5e8a\u6574\u5408\u53d7\u9650\u3002", "method": "\u5229\u7528656\u4efd3T\u548c7T MRI\u626b\u63cf\u6570\u636e\uff0c\u91c7\u7528nnU-Net\u6846\u67b6\u8fdb\u884c\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u5e76\u8fdb\u884c\u57df\u5916\u6d4b\u8bd5\u4ee5\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u57df\u5185\u548c\u57df\u5916\u7684F1\u5206\u6570\u5206\u522b\u4e3a0.64\u548c0.5\uff0c\u5c55\u793a\u4e86\u8f83\u5f3a\u7684\u75c5\u53d8\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5206\u6790\u4e86\u6570\u636e\u53d8\u5f02\u6027\u548c\u534f\u8bae\u5dee\u5f02\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4e34\u5e8a\u5e94\u7528\u4e2d\u6570\u636e\u53d8\u5f02\u6027\u548c\u534f\u8bae\u5dee\u5f02\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u516c\u5f00\u4e86\u6a21\u578b\u548c\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2507.11662", "pdf": "https://arxiv.org/pdf/2507.11662", "abs": "https://arxiv.org/abs/2507.11662", "authors": ["Moises Andrade", "Joonhyuk Cha", "Brandon Ho", "Vriksha Srihari", "Karmesh Yadav", "Zsolt Kira"], "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "comment": "Our code and data are publicly available at\n  https://github.com/mshalimay/mllm-verifiers-abias-sgv", "summary": "Verifiers -- functions assigning rewards to agent behavior -- have been key\nfor AI progress in domains like math and board games. However, extending these\ngains to domains without clear-cut success criteria (e.g.,computer use) remains\na challenge: while humans can recognize suitable outcomes, translating this\nintuition into scalable rules is non-trivial. Multimodal Large Language\nModels(MLLMs) emerge as a promising solution, given their world knowledge,\nhuman-preference alignment, and reasoning skills. We evaluate MLLMs as\nverifiers of agent trajectories across web navigation, computer use, and\nrobotic manipulation, and identify a critical limitation: agreement bias, a\nstrong tendency for MLLMs to favor information in their context window, often\ngenerating chains of thought to rationalize flawed behavior. This bias is\npervasive across models, resilient to test-time scaling, and can impact several\nmethods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs\ndespite MLLMs showing strong, human-aligned priors on desired behavior. To\naddress this, we propose Self-Grounded Verification (SGV), a lightweight method\nthat enables more effective use of MLLMs' knowledge and reasoning by harnessing\ntheir own sampling mechanisms via unconditional and conditional generation. SGV\noperates in two steps: first, the MLLM is elicited to retrieve broad priors\nabout task completion, independent of the data under evaluation. Then,\nconditioned on self-generated priors, it reasons over and evaluates a candidate\ntrajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in\naccuracy and failure detection rates, and can perform real-time supervision of\nheterogeneous agents, boosting task completion of a GUI specialist in OSWorld,\na diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting\na new state of the art on the benchmark, surpassing the previous best by 48%.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4f5c\u4e3a\u9a8c\u8bc1\u5668\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u2018\u4e00\u81f4\u6027\u504f\u89c1\u2019\uff0c\u65b0\u65b9\u6cd5SGV\u901a\u8fc7\u81ea\u6211\u751f\u6210\u5148\u9a8c\u77e5\u8bc6\u663e\u8457\u63d0\u5347\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u5728\u7f3a\u4e4f\u660e\u786e\u6210\u529f\u6807\u51c6\u7684\u9886\u57df\uff08\u5982\u8ba1\u7b97\u673a\u4f7f\u7528\uff09\u6269\u5c55AI\u9a8c\u8bc1\u5668\u7684\u5e94\u7528\uff0c\u5229\u7528MLLMs\u7684\u4e16\u754c\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u81ea\u6211\u57fa\u7840\u9a8c\u8bc1\uff08SGV\uff09\uff0c\u901a\u8fc7\u65e0\u6761\u4ef6\u4e0e\u6761\u4ef6\u751f\u6210\u7ed3\u5408\uff0c\u5148\u83b7\u53d6\u4efb\u52a1\u5b8c\u6210\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u518d\u8bc4\u4f30\u5019\u9009\u8f68\u8ff9\u3002", "result": "SGV\u4f7fMLLMs\u9a8c\u8bc1\u5668\u51c6\u786e\u7387\u63d0\u534720%\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u534748%\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u6700\u4f18\u3002", "conclusion": "SGV\u6709\u6548\u7f13\u89e3MLLMs\u7684\u4e00\u81f4\u6027\u504f\u89c1\uff0c\u663e\u8457\u63d0\u5347\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u9a8c\u8bc1\u80fd\u529b\u3002"}}
{"id": "2507.12095", "pdf": "https://arxiv.org/pdf/2507.12095", "abs": "https://arxiv.org/abs/2507.12095", "authors": ["Davide Di Nucci", "Matteo Tomei", "Guido Borghi", "Luca Ciuffreda", "Roberto Vezzani", "Rita Cucchiara"], "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D reconstruction of vehicles is vital for applications such as\nvehicle inspection, predictive maintenance, and urban planning. Existing\nmethods like Neural Radiance Fields and Gaussian Splatting have shown\nimpressive results but remain limited by their reliance on dense input views,\nwhich hinders real-world applicability. This paper addresses the challenge of\nreconstructing vehicles from sparse-view inputs, leveraging depth maps and a\nrobust pose estimation architecture to synthesize novel views and augment\ntraining data. Specifically, we enhance Gaussian Splatting by integrating a\nselective photometric loss, applied only to high-confidence pixels, and\nreplacing standard Structure-from-Motion pipelines with the DUSt3R architecture\nto improve camera pose estimation. Furthermore, we present a novel dataset\nfeaturing both synthetic and real-world public transportation vehicles,\nenabling extensive evaluation of our approach. Experimental results demonstrate\nstate-of-the-art performance across multiple benchmarks, showcasing the\nmethod's ability to achieve high-quality reconstructions even under constrained\ninput conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u7684\u8f66\u8f863D\u91cd\u5efa\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df1\u5ea6\u56fe\u548c\u9c81\u68d2\u7684\u59ff\u6001\u4f30\u8ba1\u67b6\u6784\uff0c\u6539\u8fdb\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u516c\u5171\u4ea4\u901a\u5de5\u5177\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982NeRF\u548c\u9ad8\u65af\u6cfc\u6e85\u4f9d\u8d56\u5bc6\u96c6\u8f93\u5165\u89c6\u56fe\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u7684\u8f66\u8f863D\u91cd\u5efa\u95ee\u9898\u3002", "method": "\u6574\u5408\u9009\u62e9\u6027\u5149\u5ea6\u635f\u5931\uff08\u4ec5\u9ad8\u7f6e\u4fe1\u5ea6\u50cf\u7d20\uff09\u548cDUSt3R\u67b6\u6784\u6539\u8fdb\u59ff\u6001\u4f30\u8ba1\uff0c\u589e\u5f3a\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5373\u4f7f\u5728\u8f93\u5165\u53d7\u9650\u6761\u4ef6\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u4e0b\u5b9e\u73b0\u4e86\u5148\u8fdb\u76843D\u91cd\u5efa\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.11687", "pdf": "https://arxiv.org/pdf/2507.11687", "abs": "https://arxiv.org/abs/2507.11687", "authors": ["Atharva Naik", "Lawanya Baghel", "Dhakshin Govindarajan", "Darsh Agrawal", "Daniel Fried", "Carolyn Rose"], "title": "MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models, though successful in code generation, struggle with\ncode quality analysis because they are limited by static training data and\ncan't easily adapt to evolving best practices. We introduce MetaLint, a new\ninstruction-following framework that formulates code quality analysis as the\ntask of detecting and fixing problematic semantic code fragments or code idioms\nbased on high-level specifications. Unlike conventional approaches that train\nmodels on static, rule-based data, MetaLint employs instruction tuning on\nsynthetic linter-generated data to support easy-to-hard generalization,\nenabling models to adapt to novel or complex code patterns without retraining.\nTo evaluate this, we construct a benchmark of challenging idioms inspired by\nreal-world coding standards such as Python Enhancement Proposals (PEPs) and\nassess whether MetaLint-trained models reason adaptively or simply memorize.\nOur results show that MetaLint improves generalization to unseen PEP idioms,\nachieving a 70.37% F-score on idiom detection with the highest recall (70.43%)\namong all evaluated models. It also achieves 26.73% on localization,\ncompetitive for its 4B parameter size and comparable to larger state-of-the-art\nmodels like o3-mini, highlighting its potential for future-proof code quality\nanalysis.", "AI": {"tldr": "MetaLint\u662f\u4e00\u4e2a\u57fa\u4e8e\u6307\u4ee4\u8c03\u4f18\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ee3\u7801\u8d28\u91cf\u5206\u6790\uff0c\u80fd\u591f\u9002\u5e94\u65b0\u7684\u4ee3\u7801\u6a21\u5f0f\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4ee3\u7801\u8d28\u91cf\u5206\u6790\u4e0a\u53d7\u9650\u4e8e\u9759\u6001\u8bad\u7ec3\u6570\u636e\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "method": "MetaLint\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u652f\u6301\u4ece\u6613\u5230\u96be\u7684\u6cdb\u5316\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u65b0\u4ee3\u7801\u6a21\u5f0f\u3002", "result": "\u5728\u672a\u89c1\u7684PEP\u4e60\u60ef\u7528\u6cd5\u68c0\u6d4b\u4e0a\uff0cMetaLint\u8fbe\u523070.37%\u7684F-score\u548c70.43%\u7684\u53ec\u56de\u7387\uff0c\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8868\u73b0\u4e5f\u63a5\u8fd1\u66f4\u5927\u6a21\u578b\u3002", "conclusion": "MetaLint\u5c55\u793a\u4e86\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u65b0\u4ee3\u7801\u6a21\u5f0f\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u4ee3\u7801\u8d28\u91cf\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.12103", "pdf": "https://arxiv.org/pdf/2507.12103", "abs": "https://arxiv.org/abs/2507.12103", "authors": ["Longchao Da", "Xiangrui Liu", "Mithun Shivakoti", "Thirulogasankar Pranav Kutralingam", "Yezhou Yang", "Hua Wei"], "title": "DeepShade: Enable Shade Simulation by Text-conditioned Image Generation", "categories": ["cs.CV", "cs.CY", "68T45, 68U10, 62H35", "I.2.10; I.4.8; I.5.1"], "comment": "7pages, 4 figures. Accepted to IJCAI 2025", "summary": "Heatwaves pose a significant threat to public health, especially as global\nwarming intensifies. However, current routing systems (e.g., online maps) fail\nto incorporate shade information due to the difficulty of estimating shades\ndirectly from noisy satellite imagery and the limited availability of training\ndata for generative models. In this paper, we address these challenges through\ntwo main contributions. First, we build an extensive dataset covering diverse\nlongitude-latitude regions, varying levels of building density, and different\nurban layouts. Leveraging Blender-based 3D simulations alongside building\noutlines, we capture building shadows under various solar zenith angles\nthroughout the year and at different times of day. These simulated shadows are\naligned with satellite images, providing a rich resource for learning shade\npatterns. Second, we propose the DeepShade, a diffusion-based model designed to\nlearn and synthesize shade variations over time. It emphasizes the nuance of\nedge features by jointly considering RGB with the Canny edge layer, and\nincorporates contrastive learning to capture the temporal change rules of\nshade. Then, by conditioning on textual descriptions of known conditions (e.g.,\ntime of day, solar angles), our framework provides improved performance in\ngenerating shade images. We demonstrate the utility of our approach by using\nour shade predictions to calculate shade ratios for real-world route planning\nin Tempe, Arizona. We believe this work will benefit society by providing a\nreference for urban planning in extreme heat weather and its potential\npractical applications in the environment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepShade\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5b66\u4e60\u5e76\u5408\u6210\u968f\u65f6\u95f4\u53d8\u5316\u7684\u9634\u5f71\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u6027\u80fd\uff0c\u4ee5\u6539\u8fdb\u6781\u7aef\u9ad8\u6e29\u5929\u6c14\u4e0b\u7684\u8def\u7ebf\u89c4\u5212\u548c\u57ce\u5e02\u89c4\u5212\u3002", "motivation": "\u5f53\u524d\u8def\u7ebf\u89c4\u5212\u7cfb\u7edf\uff08\u5982\u5728\u7ebf\u5730\u56fe\uff09\u56e0\u96be\u4ee5\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u76f4\u63a5\u4f30\u8ba1\u9634\u5f71\u4e14\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\uff0c\u672a\u80fd\u7eb3\u5165\u9634\u5f71\u4fe1\u606f\uff0c\u800c\u70ed\u6d6a\u5bf9\u516c\u5171\u5065\u5eb7\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002", "method": "1. \u6784\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d6\u4e0d\u540c\u7ecf\u7eac\u5ea6\u533a\u57df\u3001\u5efa\u7b51\u5bc6\u5ea6\u548c\u57ce\u5e02\u5e03\u5c40\u7684\u5e7f\u6cdb\u6570\u636e\u96c6\uff0c\u5229\u7528Blender\u6a21\u62df\u5efa\u7b51\u9634\u5f71\u5e76\u4e0e\u536b\u661f\u56fe\u50cf\u5bf9\u9f50\uff1b2. \u63d0\u51fa\u4e86DeepShade\u6a21\u578b\uff0c\u7ed3\u5408RGB\u548cCanny\u8fb9\u7f18\u5c42\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6355\u6349\u9634\u5f71\u7684\u65f6\u95f4\u53d8\u5316\u89c4\u5f8b\u3002", "result": "\u901a\u8fc7\u5df2\u77e5\u6761\u4ef6\uff08\u5982\u65f6\u95f4\u3001\u592a\u9633\u89d2\u5ea6\uff09\u751f\u6210\u9634\u5f71\u56fe\u50cf\uff0c\u5e76\u5728\u4e9a\u5229\u6851\u90a3\u5dde\u5766\u4f69\u7684\u5b9e\u9645\u8def\u7ebf\u89c4\u5212\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6781\u7aef\u9ad8\u6e29\u5929\u6c14\u4e0b\u7684\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u4e86\u53c2\u8003\uff0c\u5e76\u5c55\u793a\u4e86\u6f5c\u5728\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.11788", "pdf": "https://arxiv.org/pdf/2507.11788", "abs": "https://arxiv.org/abs/2507.11788", "authors": ["Daniel Mitropolsky", "Christos Papadimitriou"], "title": "Simulated Language Acquisition in a Biologically Realistic Model of the Brain", "categories": ["cs.NE", "cs.CL"], "comment": "13 pages, 6 figures", "summary": "Despite tremendous progress in neuroscience, we do not have a compelling\nnarrative for the precise way whereby the spiking of neurons in our brain\nresults in high-level cognitive phenomena such as planning and language. We\nintroduce a simple mathematical formulation of six basic and broadly accepted\nprinciples of neuroscience: excitatory neurons, brain areas, random synapses,\nHebbian plasticity, local inhibition, and inter-area inhibition. We implement a\nsimulated neuromorphic system based on this formalism, which is capable of\nbasic language acquisition: Starting from a tabula rasa, the system learns, in\nany language, the semantics of words, their syntactic role (verb versus noun),\nand the word order of the language, including the ability to generate novel\nsentences, through the exposure to a modest number of grounded sentences in the\nsame language. We discuss several possible extensions and implications of this\nresult.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u516d\u79cd\u795e\u7ecf\u79d1\u5b66\u539f\u5219\u7684\u6570\u5b66\u6a21\u578b\uff0c\u6a21\u62df\u4e86\u8bed\u8a00\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u79d1\u5b66\u53d6\u5f97\u4e86\u5de8\u5927\u8fdb\u5c55\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u5bf9\u795e\u7ecf\u5143\u6d3b\u52a8\u5982\u4f55\u5bfc\u81f4\u9ad8\u7ea7\u8ba4\u77e5\u73b0\u8c61\uff08\u5982\u8bed\u8a00\u548c\u89c4\u5212\uff09\u7684\u660e\u786e\u89e3\u91ca\u3002", "method": "\u57fa\u4e8e\u5174\u594b\u6027\u795e\u7ecf\u5143\u3001\u8111\u533a\u3001\u968f\u673a\u7a81\u89e6\u3001Hebbian\u53ef\u5851\u6027\u3001\u5c40\u90e8\u6291\u5236\u548c\u533a\u57df\u95f4\u6291\u5236\u516d\u79cd\u539f\u5219\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u6a21\u62df\u795e\u7ecf\u5f62\u6001\u7cfb\u7edf\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u8bed\u8a00\uff0c\u5305\u62ec\u8bcd\u6c47\u8bed\u4e49\u3001\u53e5\u6cd5\u89d2\u8272\u548c\u8bed\u5e8f\uff0c\u5e76\u80fd\u751f\u6210\u65b0\u53e5\u5b50\u3002", "conclusion": "\u8fd9\u4e00\u7ed3\u679c\u4e3a\u7406\u89e3\u795e\u7ecf\u5143\u6d3b\u52a8\u4e0e\u8ba4\u77e5\u73b0\u8c61\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u5177\u6709\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2507.12105", "pdf": "https://arxiv.org/pdf/2507.12105", "abs": "https://arxiv.org/abs/2507.12105", "authors": ["Yiquan Gao", "Duohui Xu"], "title": "Out-of-distribution data supervision towards biomedical semantic segmentation", "categories": ["cs.CV"], "comment": "This paper was published in Proceedings of SPIE Volume 13442 and is\n  reprinted with permission. The official version is available at\n  https://doi.org/10.1117/12.3052988. One personal copy is allowed.\n  Reproduction, distribution, or commercial use is prohibited", "summary": "Biomedical segmentation networks easily suffer from the unexpected\nmisclassification between foreground and background objects when learning on\nlimited and imperfect medical datasets. Inspired by the strong power of\nOut-of-Distribution (OoD) data on other visual tasks, we propose a data-centric\nframework, Med-OoD to address this issue by introducing OoD data supervision\ninto fully-supervised biomedical segmentation with none of the following needs:\n(i) external data sources, (ii) feature regularization objectives, (iii)\nadditional annotations. Our method can be seamlessly integrated into\nsegmentation networks without any modification on the architectures. Extensive\nexperiments show that Med-OoD largely prevents various segmentation networks\nfrom the pixel misclassification on medical images and achieves considerable\nperformance improvements on Lizard dataset. We also present an emerging\nlearning paradigm of training a medical segmentation network completely using\nOoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU\nas test result. We hope this learning paradigm will attract people to rethink\nthe roles of OoD data. Code is made available at\nhttps://github.com/StudioYG/Med-OoD.", "AI": {"tldr": "Med-OoD\u6846\u67b6\u901a\u8fc7\u5f15\u5165OoD\u6570\u636e\u76d1\u7763\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u50cf\u7d20\u8bef\u5206\u7c7b\u95ee\u9898\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u3001\u7279\u5f81\u6b63\u5219\u5316\u6216\u989d\u5916\u6807\u6ce8\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7f51\u7edc\u5728\u6709\u9650\u548c\u4e0d\u5b8c\u7f8e\u7684\u6570\u636e\u96c6\u4e0a\u5bb9\u6613\u53d1\u751f\u524d\u666f\u4e0e\u80cc\u666f\u7684\u8bef\u5206\u7c7b\uff0cOoD\u6570\u636e\u5728\u5176\u4ed6\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u8868\u73b0\u542f\u53d1\u4e86\u8fd9\u4e00\u7814\u7a76\u3002", "method": "\u63d0\u51faMed-OoD\u6846\u67b6\uff0c\u5c06OoD\u6570\u636e\u76d1\u7763\u5f15\u5165\u5168\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u65e0\u9700\u989d\u5916\u8d44\u6e90\u6216\u67b6\u6784\u4fee\u6539\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMed-OoD\u663e\u8457\u51cf\u5c11\u8bef\u5206\u7c7b\uff0c\u5728Lizard\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u5347\uff0c\u751a\u81f3\u4ec5\u7528OoD\u6570\u636e\u8bad\u7ec3\u4e5f\u80fd\u8fbe\u523076.1% mIoU\u3002", "conclusion": "Med-OoD\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c55\u793a\u4e86OoD\u6570\u636e\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.12142", "pdf": "https://arxiv.org/pdf/2507.12142", "abs": "https://arxiv.org/abs/2507.12142", "authors": ["Vladimir Bogachev", "Vladimir Aletov", "Alexander Molozhavenko", "Denis Bobkov", "Vera Soboleva", "Aibek Alanov", "Maxim Rakhuba"], "title": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization", "categories": ["cs.LG", "cs.CL", "cs.NA", "math.DG", "math.NA", "68T07, 65F55, 53Z50"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRiemannLoRA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06LoRA\u77e9\u9635\u89c6\u4e3a\u5149\u6ed1\u6d41\u5f62\uff0c\u89e3\u51b3\u4e86\u4f4e\u79e9\u77e9\u9635\u5206\u89e3\u4e2d\u7684\u8fc7\u53c2\u6570\u5316\u548c\u521d\u59cb\u5316\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "LoRA\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4ecd\u9762\u4e34\u521d\u59cb\u5316\u7b56\u7565\u548c\u8fc7\u53c2\u6570\u5316\u7684\u6311\u6218\u3002", "method": "\u5c06\u56fa\u5b9a\u79e9\u7684LoRA\u77e9\u9635\u89c6\u4e3a\u5149\u6ed1\u6d41\u5f62\uff0c\u5229\u7528\u6d41\u5f62\u4e0a\u7684\u6700\u5feb\u635f\u5931\u4e0b\u964d\u65b9\u5411\u8fdb\u884c\u521d\u59cb\u5316\uff0c\u5e76\u7ed3\u5408\u6570\u503c\u7ebf\u6027\u4ee3\u6570\u548c\u9ece\u66fc\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u5728LLM\u548c\u6269\u6563\u6a21\u578b\u4e0a\uff0cRiemannLoRA\u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6LoRA\u53ca\u5176\u5148\u8fdb\u6539\u8fdb\u65b9\u6cd5\u3002", "conclusion": "RiemannLoRA\u901a\u8fc7\u6d41\u5f62\u4f18\u5316\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86LoRA\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.12107", "pdf": "https://arxiv.org/pdf/2507.12107", "abs": "https://arxiv.org/abs/2507.12107", "authors": ["Sunpill Kim", "Seunghun Paik", "Chanwoo Hwang", "Minsu Kim", "Jae Hong Seo"], "title": "Non-Adaptive Adversarial Face Generation", "categories": ["cs.CV", "cs.AI", "cs.CR", "I.2.6; I.5.4; D.4.6; K.6.5; I.4.8"], "comment": null, "summary": "Adversarial attacks on face recognition systems (FRSs) pose serious security\nand privacy threats, especially when these systems are used for identity\nverification. In this paper, we propose a novel method for generating\nadversarial faces-synthetic facial images that are visually distinct yet\nrecognized as a target identity by the FRS. Unlike iterative optimization-based\napproaches (e.g., gradient descent or other iterative solvers), our method\nleverages the structural characteristics of the FRS feature space. We figure\nout that individuals sharing the same attribute (e.g., gender or race) form an\nattributed subsphere. By utilizing such subspheres, our method achieves both\nnon-adaptiveness and a remarkably small number of queries. This eliminates the\nneed for relying on transferability and open-source surrogate models, which\nhave been a typical strategy when repeated adaptive queries to commercial FRSs\nare impossible. Despite requiring only a single non-adaptive query consisting\nof 100 face images, our method achieves a high success rate of over 93% against\nAWS's CompareFaces API at its default threshold. Furthermore, unlike many\nexisting attacks that perturb a given image, our method can deliberately\nproduce adversarial faces that impersonate the target identity while exhibiting\nhigh-level attributes chosen by the adversary.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7a7a\u95f4\u7ed3\u6784\u7684\u65b0\u578b\u5bf9\u6297\u4eba\u8138\u751f\u6210\u65b9\u6cd5\uff0c\u65e0\u9700\u8fed\u4ee3\u4f18\u5316\uff0c\u4ec5\u9700\u5c11\u91cf\u67e5\u8be2\u5373\u53ef\u9ad8\u6548\u653b\u51fb\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u3002", "motivation": "\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\uff08FRSs\uff09\u7684\u5bf9\u6297\u653b\u51fb\u5bf9\u5b89\u5168\u548c\u9690\u79c1\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8fed\u4ee3\u4f18\u5316\u6216\u5f00\u6e90\u6a21\u578b\uff0c\u6548\u7387\u4f4e\u4e14\u4e0d\u5b9e\u7528\u3002", "method": "\u5229\u7528FRS\u7279\u5f81\u7a7a\u95f4\u4e2d\u5171\u4eab\u5c5e\u6027\u7684\u5b50\u7a7a\u95f4\uff08\u5982\u6027\u522b\u6216\u79cd\u65cf\uff09\uff0c\u751f\u6210\u89c6\u89c9\u5dee\u5f02\u5927\u4f46\u88ab\u8bc6\u522b\u4e3a\u76ee\u6807\u8eab\u4efd\u7684\u5bf9\u6297\u4eba\u8138\u3002", "result": "\u4ec5\u9700100\u5f20\u4eba\u8138\u56fe\u50cf\u7684\u975e\u81ea\u9002\u5e94\u67e5\u8be2\uff0c\u653b\u51fbAWS CompareFaces API\u7684\u6210\u529f\u7387\u8d85\u8fc793%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u7075\u6d3b\uff0c\u53ef\u751f\u6210\u5177\u6709\u7279\u5b9a\u5c5e\u6027\u7684\u5bf9\u6297\u4eba\u8138\uff0c\u65e0\u9700\u4f9d\u8d56\u8fed\u4ee3\u6216\u5f00\u6e90\u6a21\u578b\u3002"}}
{"id": "2507.12175", "pdf": "https://arxiv.org/pdf/2507.12175", "abs": "https://arxiv.org/abs/2507.12175", "authors": ["Sungkyun Chang", "Simon Dixon", "Emmanouil Benetos"], "title": "RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": "Accepted to WASPAA 2025", "summary": "This study introduces RUMAA, a transformer-based framework for music\nperformance analysis that unifies score-to-performance alignment,\nscore-informed transcription, and mistake detection in a near end-to-end\nmanner. Unlike prior methods addressing these tasks separately, RUMAA\nintegrates them using pre-trained score and audio encoders and a novel\ntri-stream decoder capturing task interdependencies through proxy tasks. It\naligns human-readable MusicXML scores with repeat symbols to full-length\nperformance audio, overcoming traditional MIDI-based methods that rely on\nmanually unfolded score-MIDI data with pre-specified repeat structures. RUMAA\nmatches state-of-the-art alignment methods on non-repeated scores and\noutperforms them on scores with repeats in a public piano music dataset, while\nalso delivering promising transcription and mistake detection results.", "AI": {"tldr": "RUMAA\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u4e50\u8c31\u5230\u6f14\u594f\u7684\u5bf9\u9f50\u3001\u57fa\u4e8e\u4e50\u8c31\u7684\u8f6c\u5f55\u548c\u9519\u8bef\u68c0\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u548c\u4e09\u6d41\u89e3\u7801\u5668\u5b9e\u73b0\u4efb\u52a1\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u8fd9\u4e9b\u4efb\u52a1\u5206\u5f00\u5904\u7406\uff0c\u4e14\u4f9d\u8d56MIDI\u6570\u636e\uff0c\u65e0\u6cd5\u5904\u7406\u5e26\u91cd\u590d\u7b26\u53f7\u7684\u4e50\u8c31\u3002RUMAA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u4e50\u8c31\u548c\u97f3\u9891\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u4e09\u6d41\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u4ee3\u7406\u4efb\u52a1\u6355\u83b7\u4efb\u52a1\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u516c\u5f00\u94a2\u7434\u97f3\u4e50\u6570\u636e\u96c6\u4e0a\uff0cRUMAA\u5728\u5e26\u91cd\u590d\u7b26\u53f7\u7684\u4e50\u8c31\u4e0a\u4f18\u4e8e\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u8f6c\u5f55\u548c\u9519\u8bef\u68c0\u6d4b\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "RUMAA\u4e3a\u97f3\u4e50\u8868\u6f14\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.12114", "pdf": "https://arxiv.org/pdf/2507.12114", "abs": "https://arxiv.org/abs/2507.12114", "authors": ["Yuzhou Ji", "Ke Ma", "Hong Cai", "Anchun Zhang", "Lizhuang Ma", "Xin Tan"], "title": "LidarPainter: One-Step Away From Any Lidar View To Novel Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic driving scene reconstruction is of great importance in fields like\ndigital twin system and autonomous driving simulation. However, unacceptable\ndegradation occurs when the view deviates from the input trajectory, leading to\ncorrupted background and vehicle models. To improve reconstruction quality on\nnovel trajectory, existing methods are subject to various limitations including\ninconsistency, deformation, and time consumption. This paper proposes\nLidarPainter, a one-step diffusion model that recovers consistent driving views\nfrom sparse LiDAR condition and artifact-corrupted renderings in real-time,\nenabling high-fidelity lane shifts in driving scene reconstruction. Extensive\nexperiments show that LidarPainter outperforms state-of-the-art methods in\nspeed, quality and resource efficiency, specifically 7 x faster than\nStreetCrafter with only one fifth of GPU memory required. LidarPainter also\nsupports stylized generation using text prompts such as \"foggy\" and \"night\",\nallowing for a diverse expansion of the existing asset library.", "AI": {"tldr": "LidarPainter\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5b9e\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758fLiDAR\u6570\u636e\u548c\u635f\u574f\u6e32\u67d3\u6062\u590d\u4e00\u81f4\u7684\u9a7e\u9a76\u573a\u666f\u89c6\u56fe\uff0c\u663e\u8457\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u91cd\u5efa\u5728\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u548c\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u504f\u79bb\u8f93\u5165\u8f68\u8ff9\u65f6\u5b58\u5728\u8d28\u91cf\u4e0b\u964d\u3001\u4e0d\u4e00\u81f4\u548c\u8017\u65f6\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faLidarPainter\uff0c\u4e00\u79cd\u4e00\u6b65\u6269\u6563\u6a21\u578b\uff0c\u4ece\u7a00\u758fLiDAR\u6761\u4ef6\u548c\u635f\u574f\u6e32\u67d3\u4e2d\u5b9e\u65f6\u6062\u590d\u4e00\u81f4\u9a7e\u9a76\u89c6\u56fe\uff0c\u652f\u6301\u9ad8\u4fdd\u771f\u8f66\u9053\u53d8\u6362\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLidarPainter\u5728\u901f\u5ea6\u3001\u8d28\u91cf\u548c\u8d44\u6e90\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901f\u5ea6\u5feb7\u500d\u4e14\u4ec5\u9700\u4e94\u5206\u4e4b\u4e00\u7684GPU\u5185\u5b58\u3002", "conclusion": "LidarPainter\u4e0d\u4ec5\u63d0\u5347\u4e86\u9a7e\u9a76\u573a\u666f\u91cd\u5efa\u8d28\u91cf\uff0c\u8fd8\u652f\u6301\u901a\u8fc7\u6587\u672c\u63d0\u793a\uff08\u5982\u201c\u96fe\u5929\u201d\u6216\u201c\u591c\u665a\u201d\uff09\u751f\u6210\u591a\u6837\u5316\u98ce\u683c\uff0c\u6269\u5c55\u4e86\u8d44\u4ea7\u5e93\u3002"}}
{"id": "2507.12284", "pdf": "https://arxiv.org/pdf/2507.12284", "abs": "https://arxiv.org/abs/2507.12284", "authors": ["Artem Chervyakov", "Alexander Kharitonov", "Pavel Zadorozhny", "Adamenko Pavel", "Rodion Levichev", "Dmitrii Vorobev", "Dmitrii Salikhov", "Aidar Valeev", "Alena Pestova", "Maria Dziuba", "Ilseyar Alimova", "Artem Zavgorodnev", "Aleksandr Medvedev", "Stanislav Moiseev", "Elena Bruches", "Daniil Grebenkin", "Roman Derunets", "Vikulov Vladimir", "Anton Emelyanov", "Dmitrii Babaev", "Vladimir V. Ivanov", "Valentin Malykh", "Alena Fenogenova"], "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.", "AI": {"tldr": "MERA Code\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u4fc4\u8bed\u7684\u6700\u65b0\u4ee3\u7801\u751f\u6210LLM\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u5728\u4ee3\u7801\u8d28\u91cf\u548c\u5b9e\u9645\u6027\u80fd\u4e0a\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u4ee3\u7801\u8d28\u91cf\u548c\u5b9e\u9645\u6027\u80fd\uff0c\u5bfc\u81f4\u5bf9\u6a21\u578b\u771f\u5b9e\u80fd\u529b\u7684\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u63d0\u51faMERA Code\u57fa\u51c6\uff0c\u5305\u542b11\u4e2a\u4efb\u52a1\u548c8\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u63d0\u4f9b\u5f00\u6e90\u4ee3\u7801\u5e93\u3001\u8bc4\u5206\u7cfb\u7edf\u548c\u5e73\u53f0\u3002", "result": "\u8bc4\u4f30\u4e86\u5f00\u6e90\u548c\u524d\u6cbfAPI\u6a21\u578b\uff0c\u5206\u6790\u4e86\u5176\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u7684\u5b9e\u9645\u7f16\u7801\u4efb\u52a1\u9650\u5236\u3002", "conclusion": "\u516c\u5f00MERA Code\u4ee5\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3001\u9884\u6d4b\u6a21\u578b\u5f00\u53d1\u7684\u65b0\u7279\u6027\uff0c\u5e76\u6807\u51c6\u5316\u8bc4\u4f30\u6d41\u7a0b\u3002"}}
{"id": "2507.12123", "pdf": "https://arxiv.org/pdf/2507.12123", "abs": "https://arxiv.org/abs/2507.12123", "authors": ["Sergey Linok", "Gleb Naumov"], "title": "Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph", "categories": ["cs.CV"], "comment": "13 pages, 5 figures, 2 tables", "summary": "We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects\nusing 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor\nenvironment over a Hierarchical Scene Graph derived from sequences of RGB-D\nframes utilizing a set of open-vocabulary foundation models and sensor data\nprocessing. The hierarchical representation explicitly models spatial relations\nacross floors, rooms, locations, and objects. To effectively address complex\nqueries involving spatial reference to other objects, we integrate the\nhierarchical scene graph with a Large Language Model for multistep reasoning.\nThis integration leverages inter-layer (e.g., room-to-object) and intra-layer\n(e.g., object-to-object) connections, enhancing spatial contextual\nunderstanding. We investigate the semantic and geometry accuracy of\nhierarchical representation on Habitat Matterport 3D Semantic multi-floor\nscenes. Our approach demonstrates efficient scene comprehension and robust\nobject grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates\nstrong potential for applications requiring spatial reasoning and understanding\nof indoor environments. Related materials can be found at\nhttps://github.com/linukc/OVIGo-3DHSG.", "AI": {"tldr": "OVIGo-3DHSG\u662f\u4e00\u79cd\u57fa\u4e8e3D\u5c42\u6b21\u573a\u666f\u56fe\u7684\u5f00\u653e\u8bcd\u6c47\u5ba4\u5185\u7269\u4f53\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u591a\u6b65\u63a8\u7406\uff0c\u63d0\u5347\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7406\u89e3\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u67e5\u8be2\u4e2d\u6d89\u53ca\u7a7a\u95f4\u53c2\u8003\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u5ba4\u5185\u73af\u5883\u7684\u8bed\u4e49\u548c\u51e0\u4f55\u7cbe\u5ea6\u3002", "method": "\u5229\u7528RGB-D\u5e27\u5e8f\u5217\u6784\u5efa\u5c42\u6b21\u573a\u666f\u56fe\uff0c\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u57fa\u7840\u6a21\u578b\u548c\u4f20\u611f\u5668\u6570\u636e\u5904\u7406\uff0c\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u3002", "result": "\u5728Habitat Matterport 3D\u591a\u697c\u5c42\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u7684\u573a\u666f\u7406\u89e3\u548c\u9c81\u68d2\u7684\u7269\u4f53\u5b9a\u4f4d\u80fd\u529b\u3002", "conclusion": "OVIGo-3DHSG\u5728\u9700\u8981\u7a7a\u95f4\u63a8\u7406\u548c\u5ba4\u5185\u73af\u5883\u7406\u89e3\u7684\u5e94\u7528\u4e2d\u5177\u6709\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.12341", "pdf": "https://arxiv.org/pdf/2507.12341", "abs": "https://arxiv.org/abs/2507.12341", "authors": ["Antoine Saillenfest", "Pirmin Lemberger"], "title": "Nonlinear Concept Erasure: a Density Matching Approach", "categories": ["cs.LG", "cs.CL"], "comment": "17 pages, 10 figures, accepted for publication in ECAI 2025 (28th\n  European Conference on Artificial Intelligence)", "summary": "Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLEOPARD\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u4ea4\u6295\u5f71\u4ece\u6587\u672c\u8868\u793a\u4e2d\u79fb\u9664\u654f\u611f\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u4ed6\u8bed\u4e49\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u516c\u5e73\u6027\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u786e\u4fdd\u795e\u7ecf\u6a21\u578b\u65e0\u6cd5\u4ece\u6587\u672c\u8868\u793a\u4e2d\u63a8\u65ad\u654f\u611f\u4fe1\u606f\uff08\u5982\u6027\u522b\u6216\u79cd\u65cf\uff09\u662f\u516c\u5e73\u6027\u95ee\u9898\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u5b66\u4e60\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u6b63\u4ea4\u6295\u5f71\uff0c\u4f7f\u79bb\u6563\u6982\u5ff5\u7684\u6761\u4ef6\u7279\u5f81\u5206\u5e03\u5728\u6295\u5f71\u540e\u65e0\u6cd5\u533a\u5206\uff0c\u540c\u65f6\u901a\u8fc7\u8c03\u6574\u6295\u5f71\u7684\u79e9\u63a7\u5236\u4fe1\u606f\u79fb\u9664\u7a0b\u5ea6\u3002", "result": "LEOPARD\u5728\u7ecf\u5178\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u79bb\u6563\u5c5e\u6027\u7684\u975e\u7ebf\u6027\u64e6\u9664\uff0c\u5e76\u5728\u6df1\u5ea6\u975e\u7ebf\u6027\u5206\u7c7b\u5668\u4e2d\u6709\u6548\u51cf\u5c11\u4e86\u504f\u89c1\u3002", "conclusion": "LEOPARD\u65b9\u6cd5\u5728\u63d0\u5347\u516c\u5e73\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u654f\u611f\u4fe1\u606f\u79fb\u9664\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12125", "pdf": "https://arxiv.org/pdf/2507.12125", "abs": "https://arxiv.org/abs/2507.12125", "authors": ["Yi-Kuan Hsieh", "Jun-Wei Hsieh", "Xin Li", "Yu-Ming Chang", "Yu-Chee Tseng"], "title": "Block-based Symmetric Pruning and Fusion for Efficient Vision Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformer (ViT) has achieved impressive results across various\nvision tasks, yet its high computational cost limits practical applications.\nRecent methods have aimed to reduce ViT's $O(n^2)$ complexity by pruning\nunimportant tokens. However, these techniques often sacrifice accuracy by\nindependently pruning query (Q) and key (K) tokens, leading to performance\ndegradation due to overlooked token interactions. To address this limitation,\nwe introduce a novel {\\bf Block-based Symmetric Pruning and Fusion} for\nefficient ViT (BSPF-ViT) that optimizes the pruning of Q/K tokens jointly.\nUnlike previous methods that consider only a single direction, our approach\nevaluates each token and its neighbors to decide which tokens to retain by\ntaking token interaction into account. The retained tokens are compressed\nthrough a similarity fusion step, preserving key information while reducing\ncomputational costs. The shared weights of Q/K tokens create a symmetric\nattention matrix, allowing pruning only the upper triangular part for speed up.\nBSPF-ViT consistently outperforms state-of-the-art ViT methods at all pruning\nlevels, increasing ImageNet classification accuracy by 1.3% on DeiT-T and 2.0%\non DeiT-S, while reducing computational overhead by 50%. It achieves 40%\nspeedup with improved accuracy across various ViTs.", "AI": {"tldr": "BSPF-ViT\u901a\u8fc7\u8054\u5408\u4fee\u526aQ/K\u4ee4\u724c\u5e76\u8003\u8651\u4ee4\u724c\u4ea4\u4e92\uff0c\u663e\u8457\u964d\u4f4e\u4e86ViT\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "ViT\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u72ec\u7acb\u4fee\u526aQ/K\u4ee4\u724c\u800c\u727a\u7272\u4e86\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faBlock-based Symmetric Pruning and Fusion (BSPF-ViT)\uff0c\u8054\u5408\u4f18\u5316Q/K\u4ee4\u724c\u4fee\u526a\uff0c\u5e76\u901a\u8fc7\u76f8\u4f3c\u6027\u878d\u5408\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u3002", "result": "\u5728ImageNet\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cDeiT-T\u548cDeiT-S\u7684\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad81.3%\u548c2.0%\uff0c\u8ba1\u7b97\u5f00\u9500\u51cf\u5c1150%\uff0c\u901f\u5ea6\u63d0\u534740%\u3002", "conclusion": "BSPF-ViT\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5404\u79cdViT\u6a21\u578b\u3002"}}
{"id": "2507.12378", "pdf": "https://arxiv.org/pdf/2507.12378", "abs": "https://arxiv.org/abs/2507.12378", "authors": ["Rachna Saxena", "Abhijeet Kumar", "Suresh Shanmugam"], "title": "Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker", "categories": ["cs.IR", "cs.CL"], "comment": "Presented at NLP@IR workshop at SIGIR conference", "summary": "Traditional information extraction systems face challenges with text only\nlanguage models as it does not consider infographics (visual elements of\ninformation) such as tables, charts, images etc. often used to convey complex\ninformation to readers. Multimodal LLM (MLLM) face challenges of finding needle\nin the haystack problem i.e., either longer context length or substantial\nnumber of documents as search space. Late interaction mechanism over visual\nlanguage models has shown state of the art performance in retrieval-based\nvision augmented Q&A tasks. There are yet few challenges using it for RAG based\nmulti-modal Q&A. Firstly, many popular and widely adopted vector databases do\nnot support native multi-vector retrieval. Secondly, late interaction requires\ncomputation which inflates space footprint and can hinder enterprise adoption.\nLastly, the current state of late interaction mechanism does not leverage the\napproximate neighbor search indexing methods for large speed ups in retrieval\nprocess. This paper explores a pragmatic approach to make vision retrieval\nprocess scalable and efficient without compromising on performance quality. We\npropose multi-step custom implementation utilizing widely adopted hybrid search\n(metadata & embedding) and state of the art late interaction re-ranker to\nretrieve best matching pages. Finally, MLLM are prompted as reader to generate\nanswers from contextualized best matching pages. Through experiments, we\nobserve that the proposed design is scalable (significant speed up) and stable\n(without degrading performance quality), hence can be used as production\nsystems at enterprises.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u9ad8\u6548\u89c6\u89c9\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u641c\u7d22\u548c\u91cd\u6392\u5e8f\u673a\u5236\u89e3\u51b3\u4f20\u7edf\u4fe1\u606f\u63d0\u53d6\u7cfb\u7edf\u5728\u591a\u6a21\u6001\u95ee\u7b54\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u8bed\u8a00\u6a21\u578b\u5ffd\u89c6\u89c6\u89c9\u5143\u7d20\uff08\u5982\u8868\u683c\u3001\u56fe\u8868\uff09\uff0c\u800c\u591a\u6a21\u6001LLM\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u6216\u591a\u6587\u6863\u65f6\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u7d22\u901f\u5ea6\u3001\u7a7a\u95f4\u5360\u7528\u548c\u7d22\u5f15\u4f18\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u6df7\u5408\u641c\u7d22\uff08\u5143\u6570\u636e\u548c\u5d4c\u5165\uff09\u548c\u5148\u8fdb\u7684\u4ea4\u4e92\u91cd\u6392\u5e8f\u673a\u5236\uff0c\u7ed3\u5408MLLM\u751f\u6210\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u6027\u80fd\u7a33\u5b9a\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u5408\u4f01\u4e1a\u751f\u4ea7\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u95ee\u7b54\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u89c6\u89c9\u68c0\u7d22\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.12135", "pdf": "https://arxiv.org/pdf/2507.12135", "abs": "https://arxiv.org/abs/2507.12135", "authors": ["Junyu Lou", "Xiaorui Zhao", "Kexuan Shi", "Shuhang Gu"], "title": "Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Deep learning-based bilateral grid processing has emerged as a promising\nsolution for image enhancement, inherently encoding spatial and intensity\ninformation while enabling efficient full-resolution processing through slicing\noperations. However, existing approaches are limited to linear affine\ntransformations, hindering their ability to model complex color relationships.\nMeanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings,\ntraditional MLP-based methods employ globally shared parameters, which is hard\nto deal with localized variations. To overcome these dual challenges, we\npropose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM)\nframework. Our approach synergizes the spatial modeling of bilateral grids with\nthe non-linear capabilities of MLPs. Specifically, we generate bilateral grids\ncontaining MLP parameters, where each pixel dynamically retrieves its unique\ntransformation parameters and obtain a distinct MLP for color mapping based on\nspatial coordinates and intensity values. In addition, we propose a novel grid\ndecomposition strategy that categorizes MLP parameters into distinct types\nstored in separate subgrids. Multi-channel guidance maps are used to extract\ncategory-specific parameters from corresponding subgrids, ensuring effective\nutilization of color information during slicing while guiding precise parameter\ngeneration. Extensive experiments on public datasets demonstrate that our\nmethod outperforms state-of-the-art methods in performance while maintaining\nreal-time processing capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53cc\u8fb9\u7f51\u683c\u548cMLP\u7684BPAM\u6846\u67b6\uff0c\u7528\u4e8e\u56fe\u50cf\u589e\u5f3a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7ebf\u6027\u548c\u5168\u5c40\u53c2\u6570\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u53cc\u8fb9\u7f51\u683c\u65b9\u6cd5\u4ec5\u652f\u6301\u7ebf\u6027\u53d8\u6362\uff0c\u800c\u4f20\u7edfMLP\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5c40\u90e8\u53d8\u5316\uff0cBPAM\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u53cc\u8fb9\u7f51\u683c\u5b58\u50a8MLP\u53c2\u6570\uff0c\u6bcf\u4e2a\u50cf\u7d20\u52a8\u6001\u83b7\u53d6\u72ec\u7279\u53c2\u6570\uff0c\u5e76\u91c7\u7528\u7f51\u683c\u5206\u89e3\u7b56\u7565\u548c\u591a\u901a\u9053\u5f15\u5bfc\u56fe\u4f18\u5316\u53c2\u6570\u751f\u6210\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002", "conclusion": "BPAM\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u53cc\u8fb9\u7f51\u683c\u7684\u7a7a\u95f4\u5efa\u6a21\u548cMLP\u7684\u975e\u7ebf\u6027\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u589e\u5f3a\u6548\u679c\u3002"}}
{"id": "2507.12137", "pdf": "https://arxiv.org/pdf/2507.12137", "abs": "https://arxiv.org/abs/2507.12137", "authors": ["Jiawei Xu", "Kai Deng", "Zexin Fan", "Shenlong Wang", "Jin Xie", "Jian Yang"], "title": "AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Modeling and rendering dynamic urban driving scenes is crucial for\nself-driving simulation. Current high-quality methods typically rely on costly\nmanual object tracklet annotations, while self-supervised approaches fail to\ncapture dynamic object motions accurately and decompose scenes properly,\nresulting in rendering artifacts. We introduce AD-GS, a novel self-supervised\nframework for high-quality free-viewpoint rendering of driving scenes from a\nsingle log. At its core is a novel learnable motion model that integrates\nlocality-aware B-spline curves with global-aware trigonometric functions,\nenabling flexible yet precise dynamic object modeling. Rather than requiring\ncomprehensive semantic labeling, AD-GS automatically segments scenes into\nobjects and background with the simplified pseudo 2D segmentation, representing\nobjects using dynamic Gaussians and bidirectional temporal visibility masks.\nFurther, our model incorporates visibility reasoning and physically rigid\nregularization to enhance robustness. Extensive evaluations demonstrate that\nour annotation-free model significantly outperforms current state-of-the-art\nannotation-free methods and is competitive with annotation-dependent\napproaches.", "AI": {"tldr": "AD-GS\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u8d28\u91cf\u7684\u81ea\u7531\u89c6\u89d2\u9a7e\u9a76\u573a\u666f\u6e32\u67d3\uff0c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\uff0c\u901a\u8fc7\u52a8\u6001\u9ad8\u65af\u548c\u53cc\u5411\u65f6\u95f4\u53ef\u89c1\u6027\u63a9\u6a21\u5b9e\u73b0\u573a\u666f\u5206\u89e3\u548c\u52a8\u6001\u5bf9\u8c61\u5efa\u6a21\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u624b\u52a8\u6807\u6ce8\u6216\u81ea\u76d1\u7763\u65b9\u6cd5\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u52a8\u6001\u5bf9\u8c61\u8fd0\u52a8\uff0c\u5bfc\u81f4\u6e32\u67d3\u4f2a\u5f71\u3002", "method": "\u7ed3\u5408\u5c40\u90e8\u611f\u77e5B\u6837\u6761\u66f2\u7ebf\u548c\u5168\u5c40\u611f\u77e5\u4e09\u89d2\u51fd\u6570\u7684\u5b66\u4e60\u8fd0\u52a8\u6a21\u578b\uff0c\u7b80\u5316\u4f2a2D\u5206\u5272\u81ea\u52a8\u5206\u89e3\u573a\u666f\uff0c\u4f7f\u7528\u52a8\u6001\u9ad8\u65af\u548c\u53cc\u5411\u65f6\u95f4\u53ef\u89c1\u6027\u63a9\u6a21\u8868\u793a\u5bf9\u8c61\u3002", "result": "\u5728\u65e0\u6807\u6ce8\u65b9\u6cd5\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e0e\u4f9d\u8d56\u6807\u6ce8\u7684\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "AD-GS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u81ea\u76d1\u7763\u6e32\u67d3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9a7e\u9a76\u573a\u666f\u3002"}}
{"id": "2507.12138", "pdf": "https://arxiv.org/pdf/2507.12138", "abs": "https://arxiv.org/abs/2507.12138", "authors": ["Michal Heker", "Sefy Kararlitsky", "David Tolpin"], "title": "Neural Human Pose Prior", "categories": ["cs.CV", "cs.LG"], "comment": "Work in progress", "summary": "We introduce a principled, data-driven approach for modeling a neural prior\nover human body poses using normalizing flows. Unlike heuristic or\nlow-expressivity alternatives, our method leverages RealNVP to learn a flexible\ndensity over poses represented in the 6D rotation format. We address the\nchallenge of modeling distributions on the manifold of valid 6D rotations by\ninverting the Gram-Schmidt process during training, enabling stable learning\nwhile preserving downstream compatibility with rotation-based frameworks. Our\narchitecture and training pipeline are framework-agnostic and easily\nreproducible. We demonstrate the effectiveness of the learned prior through\nboth qualitative and quantitative evaluations, and we analyze its impact via\nablation studies. This work provides a sound probabilistic foundation for\nintegrating pose priors into human motion capture and reconstruction pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f52\u4e00\u5316\u6d41\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u4eba\u4f53\u59ff\u6001\u7684\u795e\u7ecf\u5148\u9a8c\uff0c\u901a\u8fc7RealNVP\u5b66\u4e606D\u65cb\u8f6c\u683c\u5f0f\u7684\u59ff\u6001\u5206\u5e03\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u542f\u53d1\u5f0f\u6216\u4f4e\u8868\u8fbe\u80fd\u529b\u65b9\u6cd5\u5728\u5efa\u6a216D\u65cb\u8f6c\u59ff\u6001\u5206\u5e03\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u7a33\u5b9a\u5b66\u4e60\u4e14\u517c\u5bb9\u65cb\u8f6c\u6846\u67b6\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528RealNVP\u5b66\u4e606D\u65cb\u8f6c\u59ff\u6001\u7684\u7075\u6d3b\u5bc6\u5ea6\u5206\u5e03\uff0c\u901a\u8fc7\u53cd\u8f6cGram-Schmidt\u8fc7\u7a0b\u89e3\u51b3\u6d41\u5f62\u5206\u5e03\u5efa\u6a21\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5148\u9a8c\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u5206\u6790\u4e86\u5176\u5f71\u54cd\u3002", "conclusion": "\u4e3a\u4eba\u4f53\u8fd0\u52a8\u6355\u6349\u548c\u91cd\u5efa\u4e2d\u59ff\u6001\u5148\u9a8c\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u6982\u7387\u57fa\u7840\u3002"}}
{"id": "2507.12157", "pdf": "https://arxiv.org/pdf/2507.12157", "abs": "https://arxiv.org/abs/2507.12157", "authors": ["Edwin Arkel Rios", "Fernando Mikael", "Oswin Gosal", "Femiloye Oyerinde", "Hao-Chun Liang", "Bo-Cheng Lai", "Min-Chun Hu"], "title": "Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation", "categories": ["cs.CV", "I.2; I.4"], "comment": "Main: 10 pages, 2 figures, 4 tables", "summary": "Fine-grained image recognition (FGIR) aims to distinguish visually similar\nsub-categories within a broader class, such as identifying bird species. While\nmost existing FGIR methods rely on backbones pretrained on large-scale datasets\nlike ImageNet, this dependence limits adaptability to resource-constrained\nenvironments and hinders the development of task-specific architectures\ntailored to the unique challenges of FGIR.\n  In this work, we challenge the conventional reliance on pretrained models by\ndemonstrating that high-performance FGIR systems can be trained entirely from\nscratch. We introduce a novel training framework, TGDA, that integrates\ndata-aware augmentation with weak supervision via a fine-grained-aware teacher\nmodel, implemented through knowledge distillation. This framework unlocks the\ndesign of task-specific and hardware-aware architectures, including LRNets for\nlow-resolution FGIR and ViTFS, a family of Vision Transformers optimized for\nefficient inference.\n  Extensive experiments across three FGIR benchmarks over diverse settings\ninvolving low-resolution and high-resolution inputs show that our method\nconsistently matches or surpasses state-of-the-art pretrained counterparts. In\nparticular, in the low-resolution setting, LRNets trained with TGDA improve\naccuracy by up to 23\\% over prior methods while requiring up to 20.6x less\nparameters, lower FLOPs, and significantly less training data. Similarly,\nViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k\nwhile using 15.3x fewer trainable parameters and requiring orders of magnitudes\nless data. These results highlight TGDA's potential as an adaptable alternative\nto pretraining, paving the way for more efficient fine-grained vision systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u8bc6\u522b\uff08FGIR\uff09\u6846\u67b6TGDA\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u5e76\u5728\u4f4e\u5206\u8fa8\u7387\u548c\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u4e0b\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709FGIR\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u4efb\u52a1\u7279\u5b9a\u67b6\u6784\u7684\u5f00\u53d1\u3002", "method": "\u5f15\u5165TGDA\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u636e\u611f\u77e5\u589e\u5f3a\u548c\u5f31\u76d1\u7763\uff08\u901a\u8fc7\u7ec6\u7c92\u5ea6\u611f\u77e5\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\uff09\uff0c\u8bbe\u8ba1\u4e86\u4efb\u52a1\u7279\u5b9a\u548c\u786c\u4ef6\u611f\u77e5\u67b6\u6784\uff08\u5982LRNets\u548cViTFS\uff09\u3002", "result": "\u5728\u591a\u4e2aFGIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTGDA\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5c24\u5176\u5728\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u4e0b\uff0cLRNets\u63d0\u534723%\u51c6\u786e\u7387\u4e14\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u5927\u5e45\u51cf\u5c11\u3002", "conclusion": "TGDA\u5c55\u793a\u4e86\u4f5c\u4e3a\u9884\u8bad\u7ec3\u66ff\u4ee3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u66f4\u9ad8\u6548\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2507.12177", "pdf": "https://arxiv.org/pdf/2507.12177", "abs": "https://arxiv.org/abs/2507.12177", "authors": ["Zahid Ullah", "Dragan Pamucar", "Jihie Kim"], "title": "Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification", "categories": ["cs.CV"], "comment": null, "summary": "Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable\ntool for detecting tumors due to its capability to produce detailed images that\nreveal their presence. However, the accuracy of diagnosis can be compromised\nwhen human specialists evaluate these images. Factors such as fatigue, limited\nexpertise, and insufficient image detail can lead to errors. For example, small\ntumors might go unnoticed, or overlap with healthy brain regions could result\nin misidentification. To address these challenges and enhance diagnostic\nprecision, this study proposes a novel double ensembling framework, consisting\nof ensembled pre-trained deep learning (DL) models for feature extraction and\nensembled fine-tuned hyperparameter machine learning (ML) models to efficiently\nclassify brain tumors. Specifically, our method includes extensive\npreprocessing and augmentation, transfer learning concepts by utilizing various\npre-trained deep convolutional neural networks and vision transformer networks\nto extract deep features from brain MRI, and fine-tune hyperparameters of ML\nclassifiers. Our experiments utilized three different publicly available Kaggle\nMRI brain tumor datasets to evaluate the pre-trained DL feature extractor\nmodels, ML classifiers, and the effectiveness of an ensemble of deep features\nalong with an ensemble of ML classifiers for brain tumor classification. Our\nresults indicate that the proposed feature fusion and classifier fusion improve\nupon the state of the art, with hyperparameter fine-tuning providing a\nsignificant enhancement over the ensemble method. Additionally, we present an\nablation study to illustrate how each component contributes to accurate brain\ntumor classification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u8d85\u53c2\u6570\u4f18\u5316\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u8111\u80bf\u7624\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "motivation": "MRI\u8bca\u65ad\u8111\u80bf\u7624\u65f6\uff0c\u4eba\u4e3a\u56e0\u7d20\u53ef\u80fd\u5bfc\u81f4\u8bef\u8bca\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u63d0\u9ad8\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u7ed3\u5408\u8d85\u53c2\u6570\u4f18\u5316\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\u548c\u589e\u5f3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7279\u5f81\u878d\u5408\u548c\u5206\u7c7b\u5668\u878d\u5408\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8d85\u53c2\u6570\u4f18\u5316\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u53cc\u96c6\u6210\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86\u8111\u80bf\u7624\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.12188", "pdf": "https://arxiv.org/pdf/2507.12188", "abs": "https://arxiv.org/abs/2507.12188", "authors": ["Shuangli Du", "Siming Yan", "Zhenghao Shi", "Zhenzhen You", "Lu Sun"], "title": "Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Low-light images suffer from complex degradation, and existing enhancement\nmethods often encode all degradation factors within a single latent space. This\nleads to highly entangled features and strong black-box characteristics, making\nthe model prone to shortcut learning. To mitigate the above issues, this paper\nproposes a wavelet-based low-light stereo image enhancement method with feature\nspace decoupling. Our insight comes from the following findings: (1) Wavelet\ntransform enables the independent processing of low-frequency and\nhigh-frequency information. (2) Illumination adjustment can be achieved by\nadjusting the low-frequency component of a low-light image, extracted through\nmulti-level wavelet decomposition. Thus, by using wavelet transform the feature\nspace is decomposed into a low-frequency branch for illumination adjustment and\nmultiple high-frequency branches for texture enhancement. Additionally, stereo\nlow-light image enhancement can extract useful cues from another view to\nimprove enhancement. To this end, we propose a novel high-frequency guided\ncross-view interaction module (HF-CIM) that operates within high-frequency\nbranches rather than across the entire feature space, effectively extracting\nvaluable image details from the other view. Furthermore, to enhance the\nhigh-frequency information, a detail and texture enhancement module (DTEM) is\nproposed based on cross-attention mechanism. The model is trained on a dataset\nconsisting of images with uniform illumination and images with non-uniform\nillumination. Experimental results on both real and synthetic images indicate\nthat our algorithm offers significant advantages in light adjustment while\neffectively recovering high-frequency information. The code and dataset are\npublicly available at: https://github.com/Cherisherr/WDCI-Net.git.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u4f4e\u5149\u7acb\u4f53\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u89e3\u8026\u5206\u522b\u5904\u7406\u4f4e\u9891\u548c\u9ad8\u9891\u4fe1\u606f\uff0c\u63d0\u5347\u5149\u7167\u8c03\u6574\u548c\u7eb9\u7406\u6062\u590d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u9000\u5316\u56e0\u7d20\u7f16\u7801\u5728\u5355\u4e00\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5bfc\u81f4\u7279\u5f81\u9ad8\u5ea6\u7ea0\u7f20\u548c\u9ed1\u76d2\u7279\u6027\uff0c\u5bb9\u6613\u9677\u5165\u6377\u5f84\u5b66\u4e60\u3002", "method": "\u4f7f\u7528\u5c0f\u6ce2\u53d8\u6362\u5206\u89e3\u7279\u5f81\u7a7a\u95f4\u4e3a\u4f4e\u9891\u5206\u652f\uff08\u5149\u7167\u8c03\u6574\uff09\u548c\u9ad8\u9891\u5206\u652f\uff08\u7eb9\u7406\u589e\u5f3a\uff09\uff0c\u5e76\u8bbe\u8ba1\u9ad8\u9891\u5f15\u5bfc\u7684\u8de8\u89c6\u89d2\u4ea4\u4e92\u6a21\u5757\uff08HF-CIM\uff09\u548c\u7ec6\u8282\u7eb9\u7406\u589e\u5f3a\u6a21\u5757\uff08DTEM\uff09\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u56fe\u50cf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7b97\u6cd5\u5728\u5149\u7167\u8c03\u6574\u548c\u9ad8\u9891\u4fe1\u606f\u6062\u590d\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u89e3\u8026\u548c\u8de8\u89c6\u89d2\u4ea4\u4e92\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12195", "pdf": "https://arxiv.org/pdf/2507.12195", "abs": "https://arxiv.org/abs/2507.12195", "authors": ["Arkaprabha Basu"], "title": "Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modern digitised approaches have dramatically changed the preservation and\nrestoration of cultural treasures, integrating computer scientists into\nmultidisciplinary projects with ease. Machine learning, deep learning, and\ncomputer vision techniques have revolutionised developing sectors like 3D\nreconstruction, picture inpainting,IoT-based methods, genetic algorithms, and\nimage processing with the integration of computer scientists into\nmultidisciplinary initiatives. We suggest three cutting-edge techniques in\nrecognition of the special qualities of Indian monuments, which are famous for\ntheir architectural skill and aesthetic appeal. First is the Fractal\nConvolution methodology, a segmentation method based on image processing that\nsuccessfully reveals subtle architectural patterns within these irreplaceable\ncultural buildings. The second is a revolutionary Self-Sensitive Tile Filling\n(SSTF) method created especially for West Bengal's mesmerising Bankura\nTerracotta Temples with a brand-new data augmentation method called MosaicSlice\non the third. Furthermore, we delve deeper into the Super Resolution strategy\nto upscale the images without losing significant amount of quality. Our methods\nallow for the development of seamless region-filling and highly detailed tiles\nwhile maintaining authenticity using a novel data augmentation strategy within\naffordable costs introducing automation. By providing effective solutions that\npreserve the delicate balance between tradition and innovation, this study\nimproves the subject and eventually ensures unrivalled efficiency and aesthetic\nexcellence in cultural heritage protection. The suggested approaches advance\nthe field into an era of unmatched efficiency and aesthetic quality while\ncarefully upholding the delicate equilibrium between tradition and innovation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e09\u79cd\u521b\u65b0\u6280\u672f\uff08Fractal Convolution\u3001SSTF\u548cSuper Resolution\uff09\u7528\u4e8e\u5370\u5ea6\u53e4\u8ff9\u7684\u4fdd\u62a4\u4e0e\u4fee\u590d\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u63d0\u5347\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u7684\u6548\u7387\u4e0e\u7f8e\u5b66\u8d28\u91cf\u3002", "motivation": "\u73b0\u4ee3\u6570\u5b57\u5316\u65b9\u6cd5\u4e3a\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u5e26\u6765\u53d8\u9769\uff0c\u4f46\u5370\u5ea6\u53e4\u8ff9\u7684\u7279\u6b8a\u6027\u9700\u8981\u9488\u5bf9\u6027\u6280\u672f\uff0c\u4ee5\u5e73\u8861\u4f20\u7edf\u4e0e\u521b\u65b0\u3002", "method": "1. Fractal Convolution\u7528\u4e8e\u56fe\u50cf\u5206\u5272\uff1b2. SSTF\u7ed3\u5408MosaicSlice\u6570\u636e\u589e\u5f3a\uff1b3. Super Resolution\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u533a\u57df\u586b\u5145\u548c\u7ec6\u8282\u4fee\u590d\uff0c\u6210\u672c\u53ef\u63a7\u4e14\u81ea\u52a8\u5316\u3002", "conclusion": "\u7814\u7a76\u63a8\u52a8\u4e86\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u9886\u57df\u7684\u53d1\u5c55\uff0c\u517c\u987e\u6548\u7387\u4e0e\u7f8e\u5b66\uff0c\u4e3a\u4f20\u7edf\u4e0e\u521b\u65b0\u63d0\u4f9b\u5e73\u8861\u65b9\u6848\u3002"}}
{"id": "2507.12201", "pdf": "https://arxiv.org/pdf/2507.12201", "abs": "https://arxiv.org/abs/2507.12201", "authors": ["Yiqi Tian", "Pengfei Jin", "Mingze Yuan", "Na Li", "Bo Zeng", "Quanzheng Li"], "title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models", "categories": ["cs.CV", "math.OC"], "comment": null, "summary": "Diffusion models have achieved state-of-the-art performance in generative\nmodeling, yet their sampling procedures remain vulnerable to hallucinations,\noften stemming from inaccuracies in score approximation. In this work, we\nreinterpret diffusion sampling through the lens of optimization and introduce\nRODS (Robust Optimization-inspired Diffusion Sampler), a novel method that\ndetects and corrects high-risk sampling steps using geometric cues from the\nloss landscape. RODS enforces smoother sampling trajectories and adaptively\nadjusts perturbations, reducing hallucinations without retraining and at\nminimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands\ndemonstrate that RODS improves both sampling fidelity and robustness, detecting\nover 70% of hallucinated samples and correcting more than 25%, all while\navoiding the introduction of new artifacts.", "AI": {"tldr": "RODS\u662f\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u6269\u6563\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u7ebf\u7d22\u68c0\u6d4b\u548c\u4fee\u6b63\u9ad8\u98ce\u9669\u91c7\u6837\u6b65\u9aa4\uff0c\u63d0\u9ad8\u751f\u6210\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\u5bb9\u6613\u56e0\u8bc4\u5206\u8fd1\u4f3c\u4e0d\u51c6\u786e\u800c\u4ea7\u751f\u5e7b\u89c9\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u4e14\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u89c6\u89d2\u91cd\u65b0\u89e3\u91ca\u6269\u6563\u91c7\u6837\uff0c\u5229\u7528\u635f\u5931\u666f\u89c2\u7684\u51e0\u4f55\u7ebf\u7d22\u68c0\u6d4b\u548c\u4fee\u6b63\u9ad8\u98ce\u9669\u6b65\u9aa4\uff0c\u5b9e\u73b0\u66f4\u5e73\u6ed1\u7684\u91c7\u6837\u8f68\u8ff9\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cRODS\u68c0\u6d4b\u523070%\u4ee5\u4e0a\u7684\u5e7b\u89c9\u6837\u672c\u5e76\u4fee\u6b63\u8d85\u8fc725%\uff0c\u540c\u65f6\u907f\u514d\u5f15\u5165\u65b0\u4f2a\u5f71\u3002", "conclusion": "RODS\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u9c81\u68d2\u6027\u548c\u4fdd\u771f\u5ea6\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002"}}
{"id": "2507.12232", "pdf": "https://arxiv.org/pdf/2507.12232", "abs": "https://arxiv.org/abs/2507.12232", "authors": ["Tao Chen", "Jingyi Zhang", "Decheng Liu", "Chunlei Peng"], "title": "MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies have utilized visual large language models (VLMs) to answer\nnot only \"Is this face a forgery?\" but also \"Why is the face a forgery?\" These\nstudies introduced forgery-related attributes, such as forgery location and\ntype, to construct deepfake VQA datasets and train VLMs, achieving high\naccuracy while providing human-understandable explanatory text descriptions.\nHowever, these methods still have limitations. For example, they do not fully\nleverage face quality-related attributes, which are often abnormal in forged\nfaces, and they lack effective training strategies for forgery-aware VLMs. In\nthis paper, we extend the VQA dataset to create DD-VQA+, which features a\nricher set of attributes and a more diverse range of samples. Furthermore, we\nintroduce a novel forgery detection framework, MGFFD-VLM, which integrates an\nAttribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual\nLarge Language Models (VLMs). Additionally, our framework incorporates\nMulti-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By\ntransforming classification and forgery segmentation results into prompts, our\nmethod not only improves forgery classification but also enhances\ninterpretability. To further boost detection performance, we design multiple\nforgery-related auxiliary losses. Experimental results demonstrate that our\napproach surpasses existing methods in both text-based forgery judgment and\nanalysis, achieving superior accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6MGFFD-VLM\uff0c\u901a\u8fc7\u6269\u5c55\u6570\u636e\u96c6\u548c\u6539\u8fdb\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f2a\u9020\u68c0\u6d4b\u548c\u89e3\u91ca\u6027\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u9762\u90e8\u8d28\u91cf\u76f8\u5173\u5c5e\u6027\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u9650\u5236\u4e86\u4f2a\u9020\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u89e3\u91ca\u6027\u3002", "method": "\u6269\u5c55\u4e86VQA\u6570\u636e\u96c6\u4e3aDD-VQA+\uff0c\u5f15\u5165Attribute-Driven Hybrid LoRA Strategy\u3001Multi-Granularity Prompt Learning\u548cForgery-Aware Training Strategy\uff0c\u5e76\u8bbe\u8ba1\u8f85\u52a9\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u4f2a\u9020\u5224\u65ad\u548c\u5206\u6790\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u66f4\u9ad8\u3002", "conclusion": "MGFFD-VLM\u6846\u67b6\u901a\u8fc7\u591a\u7c92\u5ea6\u63d0\u793a\u5b66\u4e60\u548c\u4f2a\u9020\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f2a\u9020\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.12236", "pdf": "https://arxiv.org/pdf/2507.12236", "abs": "https://arxiv.org/abs/2507.12236", "authors": ["Felix N\u00fctzel", "Mischa Dombrowski", "Bernhard Kainz"], "title": "Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models", "categories": ["cs.CV"], "comment": "20 pages, 6 figures. To appear in Proc. MIDL 2025 (PMLR)", "summary": "Phrase grounding, i.e., mapping natural language phrases to specific image\nregions, holds significant potential for disease localization in medical\nimaging through clinical reports. While current state-of-the-art methods rely\non discriminative, self-supervised contrastive models, we demonstrate that\ngenerative text-to-image diffusion models, leveraging cross-attention maps, can\nachieve superior zero-shot phrase grounding performance. Contrary to prior\nassumptions, we show that fine-tuning diffusion models with a frozen,\ndomain-specific language model, such as CXR-BERT, substantially outperforms\ndomain-agnostic counterparts. This setup achieves remarkable improvements, with\nmIoU scores doubling those of current discriminative methods. These findings\nhighlight the underexplored potential of generative models for phrase grounding\ntasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM),\na novel post-processing technique that aligns text and image biases to identify\nregions of high certainty. BBM refines cross-attention maps, achieving even\ngreater localization accuracy. Our results establish generative approaches as a\nmore effective paradigm for phrase grounding in the medical imaging domain,\npaving the way for more robust and interpretable applications in clinical\npractice. The source code and model weights are available at\nhttps://github.com/Felix-012/generate_to_ground.", "AI": {"tldr": "\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u77ed\u8bed\u5b9a\u4f4d\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5224\u522b\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u56fe\u548c\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u63a2\u7d22\u751f\u6210\u5f0f\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u77ed\u8bed\u5b9a\u4f4d\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u63d0\u5347\u75be\u75c5\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u548c\u8de8\u6ce8\u610f\u529b\u56fe\u8fdb\u884c\u96f6\u6837\u672c\u77ed\u8bed\u5b9a\u4f4d\uff0c\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\uff08\u5982CXR-BERT\uff09\u5fae\u8c03\uff0c\u5e76\u5f15\u5165Bimodal Bias Merging\uff08BBM\uff09\u540e\u5904\u7406\u6280\u672f\u4f18\u5316\u5b9a\u4f4d\u3002", "result": "\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cmIoU\u5206\u6570\u7ffb\u500d\uff0cBBM\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "\u751f\u6210\u5f0f\u6a21\u578b\u4e3a\u533b\u5b66\u56fe\u50cf\u77ed\u8bed\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8303\u5f0f\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.12245", "pdf": "https://arxiv.org/pdf/2507.12245", "abs": "https://arxiv.org/abs/2507.12245", "authors": ["Antonio Finocchiaro", "Giovanni Maria Farinella", "Antonino Furnari"], "title": "Calisthenics Skills Temporal Video Segmentation", "categories": ["cs.CV"], "comment": "9 pages, 6 figures, In Proceedings of the 19th International Joint\n  Conference on Computer Vision, Imaging and Computer Graphics Theory and\n  Applications - Volume 2", "summary": "Calisthenics is a fast-growing bodyweight discipline that consists of\ndifferent categories, one of which is focused on skills. Skills in calisthenics\nencompass both static and dynamic elements performed by athletes. The\nevaluation of static skills is based on their difficulty level and the duration\nof the hold. Automated tools able to recognize isometric skills from a video by\nsegmenting them to estimate their duration would be desirable to assist\nathletes in their training and judges during competitions. Although the video\nunderstanding literature on action recognition through body pose analysis is\nrich, no previous work has specifically addressed the problem of calisthenics\nskill temporal video segmentation. This study aims to provide an initial step\ntowards the implementation of automated tools within the field of Calisthenics.\nTo advance knowledge in this context, we propose a dataset of video footage of\nstatic calisthenics skills performed by athletes. Each video is annotated with\na temporal segmentation which determines the extent of each skill. We hence\nreport the results of a baseline approach to address the problem of skill\ntemporal segmentation on the proposed dataset. The results highlight the\nfeasibility of the proposed problem, while there is still room for improvement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u7528\u4e8e\u4ece\u89c6\u9891\u4e2d\u8bc6\u522b\u548c\u5206\u5272\u9759\u6001\u4f53\u64cd\u6280\u80fd\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u4f53\u64cd\u6280\u80fd\u65f6\u95f4\u5206\u5272\u7684\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8f85\u52a9\u8fd0\u52a8\u5458\u8bad\u7ec3\u548c\u6bd4\u8d5b\u8bc4\u5206\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u9759\u6001\u4f53\u64cd\u6280\u80fd\u7684\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5e76\u6807\u6ce8\u4e86\u65f6\u95f4\u5206\u5272\uff1b\u63d0\u51fa\u4e86\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u6280\u80fd\u65f6\u95f4\u5206\u5272\u3002", "result": "\u7ed3\u679c\u8868\u660e\u8be5\u95ee\u9898\u5177\u6709\u53ef\u884c\u6027\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f53\u64cd\u9886\u57df\u7684\u81ea\u52a8\u5316\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u4e86\u521d\u6b65\u57fa\u7840\u3002"}}
{"id": "2507.12248", "pdf": "https://arxiv.org/pdf/2507.12248", "abs": "https://arxiv.org/abs/2507.12248", "authors": ["Anida Nezovi\u0107", "Jalal Romano", "Nada Mari\u0107", "Medina Kapo", "Amila Akagi\u0107"], "title": "Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Deep learning has significantly advanced the field of medical image\nclassification, particularly with the adoption of Convolutional Neural Networks\n(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer\nunique advantages in model development and deployment. However, their\ncomparative performance in medical imaging tasks remains underexplored. This\nstudy presents a comprehensive analysis of CNN implementations across these\nframeworks, using the PathMNIST dataset as a benchmark. We evaluate training\nefficiency, classification accuracy and inference speed to assess their\nsuitability for real-world applications. Our findings highlight the trade-offs\nbetween computational speed and model accuracy, offering valuable insights for\nresearchers and practitioners in medical image analysis.", "AI": {"tldr": "\u6bd4\u8f83Keras\u3001PyTorch\u548cJAX\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4f7f\u7528PathMNIST\u6570\u636e\u96c6\u8bc4\u4f30\u8bad\u7ec3\u6548\u7387\u3001\u5206\u7c7b\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u63a2\u7a76\u4e0d\u540c\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7PathMNIST\u6570\u636e\u96c6\uff0c\u5bf9Keras\u3001PyTorch\u548cJAX\u5b9e\u73b0\u7684CNN\u8fdb\u884c\u8bad\u7ec3\u6548\u7387\u3001\u5206\u7c7b\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u7684\u5168\u9762\u5206\u6790\u3002", "result": "\u63ed\u793a\u4e86\u8ba1\u7b97\u901f\u5ea6\u4e0e\u6a21\u578b\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\uff0c\u5e2e\u52a9\u9009\u62e9\u9002\u5408\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2507.12269", "pdf": "https://arxiv.org/pdf/2507.12269", "abs": "https://arxiv.org/abs/2507.12269", "authors": ["Sybelle Goedicke-Fritz", "Michelle Bous", "Annika Engel", "Matthias Flotho", "Pascal Hirsch", "Hannah Wittig", "Dino Milanovic", "Dominik Mohr", "Mathias Kaspar", "Sogand Nemat", "Dorothea Kerner", "Arno B\u00fccker", "Andreas Keller", "Sascha Meyer", "Michael Zemlin", "Philipp Flotho"], "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "S.G.-F., M.B., and A.E. contributed equally to this work and share\n  first authorship. M.Z. and P.F. contributed equally to this work and share\n  senior authorship", "summary": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65e9\u671f\u652f\u6c14\u7ba1\u80ba\u53d1\u80b2\u4e0d\u826f\uff08BPD\uff09\u9884\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u51fa\u751f24\u5c0f\u65f6\u5185\u7684\u80f8\u7247\u8fdb\u884c\u975e\u4fb5\u5165\u6027\u9884\u540e\u8bc4\u4f30\u3002", "motivation": "BPD\u662f\u4e00\u79cd\u5f71\u54cd\u6781\u4f4e\u51fa\u751f\u4f53\u91cd\u5a74\u513f\u7684\u6162\u6027\u80ba\u90e8\u75be\u75c5\uff0c\u73b0\u6709\u9884\u9632\u63aa\u65bd\u5b58\u5728\u4e25\u91cd\u98ce\u9669\uff0c\u56e0\u6b64\u65e9\u671f\u9884\u6d4b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u7684ResNet-50\u6a21\u578b\uff0c\u7ed3\u5408\u6e10\u8fdb\u5c42\u51bb\u7ed3\u3001\u7ebf\u6027\u63a2\u6d4b\u548cCutMix\u589e\u5f3a\u6280\u672f\uff0c\u5bf9163\u540d\u6781\u4f4e\u51fa\u751f\u4f53\u91cd\u5a74\u513f\u7684\u80f8\u7247\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u9884\u6d4b\u4e2d\u91cd\u5ea6BPD\u65f6\uff0cAUROC\u4e3a0.78\uff0c\u5e73\u8861\u51c6\u786e\u7387\u4e3a0.69\uff0cF1\u5206\u6570\u4e3a0.67\uff0c\u663e\u8457\u4f18\u4e8eImageNet\u521d\u59cb\u5316\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9886\u57df\u7279\u5b9a\u7684\u9884\u8bad\u7ec3\u7ed3\u5408\u6e10\u8fdb\u51bb\u7ed3\u548c\u7ebf\u6027\u63a2\u6d4b\uff0c\u80fd\u591f\u4ece\u5e38\u89c4\u80f8\u7247\u4e2d\u51c6\u786e\u9884\u6d4bBPD\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u9002\u5408\u4e34\u5e8a\u90e8\u7f72\u3002"}}
{"id": "2507.12283", "pdf": "https://arxiv.org/pdf/2507.12283", "abs": "https://arxiv.org/abs/2507.12283", "authors": ["Zixuan Fu", "Yan Ren", "Finn Carter", "Chenyue Wang", "Ze Niu", "Dacheng Yu", "Emily Davis", "Bo Zhang"], "title": "FADE: Adversarial Concept Erasure in Flow Models", "categories": ["cs.CV"], "comment": "Camera Ready", "summary": "Diffusion models have demonstrated remarkable image generation capabilities,\nbut also pose risks in privacy and fairness by memorizing sensitive concepts or\nperpetuating biases. We propose a novel \\textbf{concept erasure} method for\ntext-to-image diffusion models, designed to remove specified concepts (e.g., a\nprivate individual or a harmful stereotype) from the model's generative\nrepertoire. Our method, termed \\textbf{FADE} (Fair Adversarial Diffusion\nErasure), combines a trajectory-aware fine-tuning strategy with an adversarial\nobjective to ensure the concept is reliably removed while preserving overall\nmodel fidelity. Theoretically, we prove a formal guarantee that our approach\nminimizes the mutual information between the erased concept and the model's\noutputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable\nDiffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,\nexplicit content, and style erasure tasks from MACE). FADE achieves\nstate-of-the-art concept removal performance, surpassing recent baselines like\nESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.\nNotably, FADE improves the harmonic mean of concept removal and fidelity by\n5--10\\% over the best prior method. We also conduct an ablation study to\nvalidate each component of FADE, confirming that our adversarial and\ntrajectory-preserving objectives each contribute to its superior performance.\nOur work sets a new standard for safe and fair generative modeling by\nunlearning specified concepts without retraining from scratch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFADE\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u5220\u9664\u6307\u5b9a\u6982\u5ff5\uff0c\u786e\u4fdd\u9690\u79c1\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e5f\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u548c\u504f\u89c1\u4f20\u64ad\u7684\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u6982\u5ff5\u5220\u9664\u65b9\u6cd5\u3002", "method": "FADE\u7ed3\u5408\u4e86\u8f68\u8ff9\u611f\u77e5\u7684\u5fae\u8c03\u7b56\u7565\u548c\u5bf9\u6297\u6027\u76ee\u6807\uff0c\u786e\u4fdd\u6982\u5ff5\u88ab\u53ef\u9760\u5220\u9664\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u4fdd\u771f\u5ea6\u3002", "result": "FADE\u5728\u6982\u5ff5\u5220\u9664\u6027\u80fd\u548c\u56fe\u50cf\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e865-10%\u7684\u8c03\u548c\u5e73\u5747\u503c\u3002", "conclusion": "FADE\u4e3a\u5b89\u5168\u548c\u516c\u5e73\u7684\u751f\u6210\u5efa\u6a21\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\u5373\u53ef\u5220\u9664\u6307\u5b9a\u6982\u5ff5\u3002"}}
{"id": "2507.12292", "pdf": "https://arxiv.org/pdf/2507.12292", "abs": "https://arxiv.org/abs/2507.12292", "authors": ["Antonio Finocchiaro", "Giovanni Maria Farinella", "Antonino Furnari"], "title": "Efficient Calisthenics Skills Classification through Foreground Instance Selection and Depth Estimation", "categories": ["cs.CV"], "comment": "13 pages, 4 figures, In International Conference on Image Analysis\n  and Processing", "summary": "Calisthenics skill classification is the computer vision task of inferring\nthe skill performed by an athlete from images, enabling automatic performance\nassessment and personalized analytics. Traditional methods for calisthenics\nskill recognition are based on pose estimation methods to determine the\nposition of skeletal data from images, which is later fed to a classification\nalgorithm to infer the performed skill. Despite the progress in human pose\nestimation algorithms, they still involve high computational costs, long\ninference times, and complex setups, which limit the applicability of such\napproaches in real-time applications or mobile devices. This work proposes a\ndirect approach to calisthenics skill recognition, which leverages depth\nestimation and athlete patch retrieval to avoid the computationally expensive\nhuman pose estimation module. Using Depth Anything V2 for depth estimation and\nYOLOv10 for athlete localization, we segment the subject from the background\nrather than relying on traditional pose estimation techniques. This strategy\nincreases efficiency, reduces inference time, and improves classification\naccuracy. Our approach significantly outperforms skeleton-based methods,\nachieving 38.3x faster inference with RGB image patches and improved\nclassification accuracy with depth patches (0.837 vs. 0.815). Beyond these\nperformance gains, the modular design of our pipeline allows for flexible\nreplacement of components, enabling future enhancements and adaptation to\nreal-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u4f30\u8ba1\u548c\u8fd0\u52a8\u5458\u533a\u57df\u68c0\u7d22\u7684\u5065\u7f8e\u64cd\u6280\u80fd\u5206\u7c7b\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u5206\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u59ff\u6001\u4f30\u8ba1\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u63a8\u7406\u65f6\u95f4\u957f\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u5e94\u7528\u548c\u79fb\u52a8\u8bbe\u5907\u7684\u9002\u7528\u6027\u3002", "method": "\u5229\u7528Depth Anything V2\u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1\u548cYOLOv10\u8fdb\u884c\u8fd0\u52a8\u5458\u5b9a\u4f4d\uff0c\u76f4\u63a5\u5206\u5272\u8fd0\u52a8\u5458\u533a\u57df\uff0c\u907f\u514d\u59ff\u6001\u4f30\u8ba1\u3002", "result": "\u65b9\u6cd5\u5728RGB\u56fe\u50cf\u5757\u4e0a\u63a8\u7406\u901f\u5ea6\u5feb38.3\u500d\uff0c\u6df1\u5ea6\u56fe\u50cf\u5757\u7684\u5206\u7c7b\u7cbe\u5ea6\u66f4\u9ad8\uff080.837 vs. 0.815\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u652f\u6301\u7075\u6d3b\u66ff\u6362\u7ec4\u4ef6\uff0c\u9002\u7528\u4e8e\u672a\u6765\u589e\u5f3a\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.12318", "pdf": "https://arxiv.org/pdf/2507.12318", "abs": "https://arxiv.org/abs/2507.12318", "authors": ["Samuel Lavoie", "Michael Noukhovitch", "Aaron Courville"], "title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "In submission, 22 pages, 7 tables, 12 figures", "summary": "We argue that diffusion models' success in modeling complex distributions is,\nfor the most part, coming from their input conditioning. This paper\ninvestigates the representation used to condition diffusion models from the\nperspective that ideal representations should improve sample fidelity, be easy\nto generate, and be compositional to allow out-of-training samples generation.\nWe introduce Discrete Latent Code (DLC), an image representation derived from\nSimplicial Embeddings trained with a self-supervised learning objective. DLCs\nare sequences of discrete tokens, as opposed to the standard continuous image\nembeddings. They are easy to generate and their compositionality enables\nsampling of novel images beyond the training distribution. Diffusion models\ntrained with DLCs have improved generation fidelity, establishing a new\nstate-of-the-art for unconditional image generation on ImageNet. Additionally,\nwe show that composing DLCs allows the image generator to produce\nout-of-distribution samples that coherently combine the semantics of images in\ndiverse ways. Finally, we showcase how DLCs can enable text-to-image generation\nby leveraging large-scale pretrained language models. We efficiently finetune a\ntext diffusion language model to generate DLCs that produce novel samples\noutside of the image generator training distribution.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u6269\u6563\u6a21\u578b\u6210\u529f\u7684\u5173\u952e\u5728\u4e8e\u8f93\u5165\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u79bb\u6563\u6f5c\u5728\u4ee3\u7801\uff08DLC\uff09\u4f5c\u4e3a\u6539\u8fdb\u8868\u793a\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u4e0e\u7ec4\u5408\u6027\u3002", "motivation": "\u7814\u7a76\u6269\u6563\u6a21\u578b\u8f93\u5165\u6761\u4ef6\u7684\u7406\u60f3\u8868\u793a\uff0c\u9700\u5177\u5907\u9ad8\u4fdd\u771f\u3001\u6613\u751f\u6210\u548c\u7ec4\u5408\u6027\uff0c\u4ee5\u652f\u6301\u8bad\u7ec3\u5916\u6837\u672c\u751f\u6210\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u79bb\u6563\u6f5c\u5728\u4ee3\u7801\uff08DLC\uff09\uff0c\u66ff\u4ee3\u8fde\u7eed\u5d4c\u5165\uff0c\u63d0\u5347\u6269\u6563\u6a21\u578b\u6027\u80fd\u3002", "result": "DLC\u663e\u8457\u63d0\u5347\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u521bImageNet\u65b0\u7eaa\u5f55\uff0c\u5e76\u652f\u6301\u8bad\u7ec3\u5916\u6837\u672c\u7ec4\u5408\u751f\u6210\u3002", "conclusion": "DLC\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u9ad8\u6548\u8868\u793a\uff0c\u652f\u6301\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u6269\u5c55\u6a21\u578b\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.12336", "pdf": "https://arxiv.org/pdf/2507.12336", "abs": "https://arxiv.org/abs/2507.12336", "authors": ["Subin Jeon", "In Cho", "Junyoung Hong", "Seon Joo Kim"], "title": "Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D\nkeypoints estimation that accurately predicts 3D keypoints from a single image.\nWhile previous methods rely on manual annotations or calibrated multi-view\nimages, both of which are expensive to collect, our method enables monocular 3D\nkeypoints estimation using only a collection of single-view images. To achieve\nthis, we leverage powerful geometric priors embedded in a pretrained multi-view\ndiffusion model. In our framework, this model generates multi-view images from\na single image, serving as a supervision signal to provide 3D geometric cues to\nour model. We also use the diffusion model as a powerful 2D multi-view feature\nextractor and construct 3D feature volumes from its intermediate\nrepresentations. This transforms implicit 3D priors learned by the diffusion\nmodel into explicit 3D features. Beyond accurate keypoints estimation, we\nfurther introduce a pipeline that enables manipulation of 3D objects generated\nby the diffusion model. Experimental results on diverse aspects and datasets,\nincluding Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain\ndatasets, highlight the effectiveness of our method in terms of accuracy,\ngeneralization, and its ability to enable manipulation of 3D objects generated\nby the diffusion model from a single image.", "AI": {"tldr": "KeyDiff3D\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u7684\u5355\u76ee3D\u5173\u952e\u70b9\u4f30\u8ba1\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u751f\u6210\u51e0\u4f55\u5148\u9a8c\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u9884\u6d4b3D\u5173\u952e\u70b9\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u624b\u52a8\u6807\u6ce8\u6216\u591a\u89c6\u89d2\u6821\u51c6\u56fe\u50cf\uff0c\u800cKeyDiff3D\u4ec5\u9700\u5355\u89c6\u89d2\u56fe\u50cf\uff0c\u964d\u4f4e\u4e86\u6570\u636e\u6536\u96c6\u6210\u672c\u3002", "method": "\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u89c6\u89d2\u56fe\u50cf\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u5e76\u4ece\u4e2d\u63d0\u53d62D\u591a\u89c6\u89d2\u7279\u5f81\u6784\u5efa3D\u7279\u5f81\u4f53\u79ef\uff0c\u5c06\u9690\u5f0f3D\u5148\u9a8c\u8f6c\u5316\u4e3a\u663e\u5f0f\u7279\u5f81\u3002", "result": "\u5728Human3.6M\u3001Stanford Dogs\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u53ca\u5bf93D\u5bf9\u8c61\u64cd\u63a7\u7684\u652f\u6301\u3002", "conclusion": "KeyDiff3D\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u65e0\u76d1\u77633D\u5173\u952e\u70b9\u4f30\u8ba1\u65b9\u6848\uff0c\u5e76\u6269\u5c55\u4e86\u6269\u6563\u6a21\u578b\u57283D\u5bf9\u8c61\u64cd\u63a7\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.12344", "pdf": "https://arxiv.org/pdf/2507.12344", "abs": "https://arxiv.org/abs/2507.12344", "authors": ["Ahmet O\u011fuz Salt\u0131k", "Max Voigt", "Sourav Modak", "Mike Beckworth", "Anthony Stein"], "title": "Improving Lightweight Weed Detection via Knowledge Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Weed detection is a critical component of precision agriculture, facilitating\ntargeted herbicide application and reducing environmental impact. However,\ndeploying accurate object detection models on resource-limited platforms\nremains challenging, particularly when differentiating visually similar weed\nspecies commonly encountered in plant phenotyping applications. In this work,\nwe investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative\nDistillation (MGD) to enhance the performance of lightweight models for\nreal-time smart spraying systems. Utilizing YOLO11x as the teacher model and\nYOLO11n as both reference and student, both CWD and MGD effectively transfer\nknowledge from the teacher to the student model. Our experiments, conducted on\na real-world dataset comprising sugar beet crops and four weed types (Cirsium,\nConvolvulus, Fallopia, and Echinochloa), consistently show increased AP50\nacross all classes. The distilled CWD student model achieves a notable\nimprovement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without\nincreasing model complexity. Additionally, we validate real-time deployment\nfeasibility by evaluating the student YOLO11n model on Jetson Orin Nano and\nRaspberry Pi 5 embedded devices, performing five independent runs to evaluate\nperformance stability across random seeds. These findings confirm CWD and MGD\nas an effective, efficient, and practical approach for improving deep\nlearning-based weed detection accuracy in precision agriculture and plant\nphenotyping scenarios.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u9053\u77e5\u8bc6\u84b8\u998f\uff08CWD\uff09\u548c\u63a9\u7801\u751f\u6210\u84b8\u998f\uff08MGD\uff09\u5728\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u9ad8\u6742\u8349\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u7cbe\u51c6\u519c\u4e1a\u4e2d\u6742\u8349\u68c0\u6d4b\u5bf9\u51cf\u5c11\u73af\u5883\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u90e8\u7f72\u9ad8\u7cbe\u5ea6\u6a21\u578b\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528YOLO11x\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0cYOLO11n\u4f5c\u4e3a\u5b66\u751f\u6a21\u578b\uff0c\u901a\u8fc7CWD\u548cMGD\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002", "result": "CWD\u548cMGD\u5206\u522b\u63d0\u5347mAP50 2.5%\u548c1.9%\uff0c\u4e14\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "CWD\u548cMGD\u662f\u63d0\u9ad8\u6742\u8349\u68c0\u6d4b\u51c6\u786e\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7cbe\u51c6\u519c\u4e1a\u548c\u690d\u7269\u8868\u578b\u5206\u6790\u3002"}}
{"id": "2507.12359", "pdf": "https://arxiv.org/pdf/2507.12359", "abs": "https://arxiv.org/abs/2507.12359", "authors": ["Nikolaos Giakoumoglou", "Tania Stathaki"], "title": "Cluster Contrast for Unsupervised Visual Representation Learning", "categories": ["cs.CV", "cs.AI"], "comment": "ICIP 2025", "summary": "We introduce Cluster Contrast (CueCo), a novel approach to unsupervised\nvisual representation learning that effectively combines the strengths of\ncontrastive learning and clustering methods. Inspired by recent advancements,\nCueCo is designed to simultaneously scatter and align feature representations\nwithin the feature space. This method utilizes two neural networks, a query and\na key, where the key network is updated through a slow-moving average of the\nquery outputs. CueCo employs a contrastive loss to push dissimilar features\napart, enhancing inter-class separation, and a clustering objective to pull\ntogether features of the same cluster, promoting intra-class compactness. Our\nmethod achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on\nCIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18\nbackbone. By integrating contrastive learning with clustering, CueCo sets a new\ndirection for advancing unsupervised visual representation learning.", "AI": {"tldr": "Cluster Contrast (CueCo) \u662f\u4e00\u79cd\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u805a\u7c7b\u65b9\u6cd5\u7684\u65e0\u76d1\u7763\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u5206\u6563\u548c\u5bf9\u9f50\u7279\u5f81\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u4e2d\u672a\u80fd\u5145\u5206\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u805a\u7c7b\u7684\u4f18\u52bf\uff0cCueCo\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u4e24\u8005\u63d0\u5347\u7279\u5f81\u8868\u793a\u7684\u8d28\u91cf\u3002", "method": "CueCo\u4f7f\u7528\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\uff08\u67e5\u8be2\u548c\u952e\uff09\uff0c\u952e\u7f51\u7edc\u901a\u8fc7\u67e5\u8be2\u8f93\u51fa\u7684\u6162\u901f\u79fb\u52a8\u5e73\u5747\u66f4\u65b0\u3002\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u548c\u805a\u7c7b\u76ee\u6807\u5206\u522b\u589e\u5f3a\u7c7b\u95f4\u5206\u79bb\u548c\u7c7b\u5185\u7d27\u51d1\u6027\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cImageNet-100\u4e0a\u5206\u522b\u8fbe\u523091.40%\u300168.56%\u548c78.65%\u7684top-1\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "CueCo\u901a\u8fc7\u6574\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u805a\u7c7b\uff0c\u4e3a\u65e0\u76d1\u7763\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.12382", "pdf": "https://arxiv.org/pdf/2507.12382", "abs": "https://arxiv.org/abs/2507.12382", "authors": ["Kaiwen Huang", "Yi Zhou", "Huazhu Fu", "Yizhe Zhang", "Chen Gong", "Tao Zhou"], "title": "Text-driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "10 pages; 2 figures; Have been accepted by MICCAI 2025", "summary": "Semi-supervised medical image segmentation is a crucial technique for\nalleviating the high cost of data annotation. When labeled data is limited,\ntextual information can provide additional context to enhance visual semantic\nunderstanding. However, research exploring the use of textual data to enhance\nvisual semantic embeddings in 3D medical imaging tasks remains scarce. In this\npaper, we propose a novel text-driven multiplanar visual interaction framework\nfor semi-supervised medical image segmentation (termed Text-SemiSeg), which\nconsists of three main modules: Text-enhanced Multiplanar Representation (TMR),\nCategory-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation\n(DCA). Specifically, TMR facilitates text-visual interaction through planar\nmapping, thereby enhancing the category awareness of visual features. CSA\nperforms cross-modal semantic alignment between the text features with\nintroduced learnable variables and the intermediate layer of visual features.\nDCA reduces the distribution discrepancy between labeled and unlabeled data\nthrough their interaction, thus improving the model's robustness. Finally,\nexperiments on three public datasets demonstrate that our model effectively\nenhances visual features with textual information and outperforms other\nmethods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aText-SemiSeg\u7684\u6587\u672c\u9a71\u52a8\u591a\u5e73\u9762\u89c6\u89c9\u4ea4\u4e92\u6846\u67b6\uff0c\u7528\u4e8e\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u901a\u8fc7\u6587\u672c\u589e\u5f3a\u89c6\u89c9\u8bed\u4e49\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5229\u7528\u6587\u672c\u4fe1\u606f\u589e\u5f3a\u89c6\u89c9\u8bed\u4e49\u7406\u89e3\uff0c\u586b\u88653D\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\u6587\u672c\u6570\u636e\u5229\u7528\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u6587\u672c\u589e\u5f3a\u591a\u5e73\u9762\u8868\u793a\uff08TMR\uff09\u3001\u7c7b\u522b\u611f\u77e5\u8bed\u4e49\u5bf9\u9f50\uff08CSA\uff09\u548c\u52a8\u6001\u8ba4\u77e5\u589e\u5f3a\uff08DCA\uff09\uff0c\u5206\u522b\u5b9e\u73b0\u6587\u672c-\u89c6\u89c9\u4ea4\u4e92\u3001\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u548c\u51cf\u5c11\u6807\u8bb0\u4e0e\u672a\u6807\u8bb0\u6570\u636e\u7684\u5206\u5e03\u5dee\u5f02\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u6709\u6548\u5229\u7528\u6587\u672c\u4fe1\u606f\u589e\u5f3a\u89c6\u89c9\u7279\u5f81\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "Text-SemiSeg\u901a\u8fc7\u6587\u672c\u9a71\u52a8\u591a\u5e73\u9762\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6548\u679c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12396", "pdf": "https://arxiv.org/pdf/2507.12396", "abs": "https://arxiv.org/abs/2507.12396", "authors": ["Hayat Ullah", "Abbas Khan", "Arslan Munir", "Hari Kalva"], "title": "OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments", "categories": ["cs.CV"], "comment": "14 pages", "summary": "Realistic human surveillance datasets are crucial for training and evaluating\ncomputer vision models under real-world conditions, facilitating the\ndevelopment of robust algorithms for human and human-interacting object\ndetection in complex environments. These datasets need to offer diverse and\nchallenging data to enable a comprehensive assessment of model performance and\nthe creation of more reliable surveillance systems for public safety. To this\nend, we present two visual object detection benchmarks named OD-VIRAT Large and\nOD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance\nimagery. The video sequences in both benchmarks cover 10 different scenes of\nhuman surveillance recorded from significant height and distance. The proposed\nbenchmarks offer rich annotations of bounding boxes and categories, where\nOD-VIRAT Large has 8.7 million annotated instances in 599,996 images and\nOD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also\nfocuses on benchmarking state-of-the-art object detection architectures,\nincluding RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object\ndetection-specific variant of VIRAT dataset. To the best of our knowledge, it\nis the first work to examine the performance of these recently published\nstate-of-the-art object detection architectures on realistic surveillance\nimagery under challenging conditions such as complex backgrounds, occluded\nobjects, and small-scale objects. The proposed benchmarking and experimental\nsettings will help in providing insights concerning the performance of selected\nobject detection models and set the base for developing more efficient and\nrobust object detection architectures.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u89c6\u89c9\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6OD-VIRAT Large\u548cOD-VIRAT Tiny\uff0c\u7528\u4e8e\u8bc4\u4f30\u590d\u6742\u73af\u5883\u4e0b\u7684\u4eba\u4f53\u76d1\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u6d4b\u8bd5\u4e86\u591a\u79cd\u5148\u8fdb\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\u7684\u8868\u73b0\u3002", "motivation": "\u5f00\u53d1\u53ef\u9760\u7684\u4eba\u4f53\u76d1\u6d4b\u7cfb\u7edf\u9700\u8981\u591a\u6837\u5316\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e30\u5bcc\u6807\u6ce8\uff0c\u5e76\u5728\u5176\u4e0a\u6d4b\u8bd5\u4e86RETMDET\u3001YOLOX\u7b49\u5148\u8fdb\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\u3002", "result": "OD-VIRAT Large\u5305\u542b870\u4e07\u6807\u6ce8\u5b9e\u4f8b\uff0cOD-VIRAT Tiny\u5305\u542b28.9\u4e07\u6807\u6ce8\u5b9e\u4f8b\uff0c\u5b9e\u9a8c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.12414", "pdf": "https://arxiv.org/pdf/2507.12414", "abs": "https://arxiv.org/abs/2507.12414", "authors": ["Santosh Vasa", "Aditi Ramadwar", "Jnana Rama Krishna Darabattula", "Md Zafar Anwar", "Stanislaw Antol", "Andrei Vatavu", "Thomas Monninger", "Sihao Ding"], "title": "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Training of autonomous driving systems requires extensive datasets with\nprecise annotations to attain robust performance. Human annotations suffer from\nimperfections, and multiple iterations are often needed to produce high-quality\ndatasets. However, manually reviewing large datasets is laborious and\nexpensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)\nframework and investigate the utilization of Vision-Language Models (VLMs) to\nautomatically identify erroneous annotations in vision datasets, thereby\nenabling users to eliminate these errors and enhance data quality. We validate\nour approach using the KITTI and nuImages datasets, which contain object\ndetection benchmarks for autonomous driving. To test the effectiveness of\nAutoVDC, we create dataset variants with intentionally injected erroneous\nannotations and observe the error detection rate of our approach. Additionally,\nwe compare the detection rates using different VLMs and explore the impact of\nVLM fine-tuning on our pipeline. The results demonstrate our method's high\nperformance in error detection and data cleaning experiments, indicating its\npotential to significantly improve the reliability and accuracy of large-scale\nproduction datasets in autonomous driving.", "AI": {"tldr": "AutoVDC\u6846\u67b6\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u68c0\u6d4b\u89c6\u89c9\u6570\u636e\u96c6\u4e2d\u7684\u6807\u6ce8\u9519\u8bef\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u4eba\u5de5\u6807\u6ce8\u5b58\u5728\u7f3a\u9677\u4e14\u6210\u672c\u9ad8\uff0c\u9700\u81ea\u52a8\u5316\u65b9\u6cd5\u63d0\u5347\u6570\u636e\u96c6\u8d28\u91cf\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8bc6\u522b\u9519\u8bef\u6807\u6ce8\uff0c\u5e76\u5728KITTI\u548cnuImages\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u663e\u793aAutoVDC\u80fd\u9ad8\u6548\u68c0\u6d4b\u9519\u8bef\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u96c6\u53ef\u9760\u6027\u3002", "conclusion": "AutoVDC\u6709\u6f5c\u529b\u5927\u89c4\u6a21\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.12416", "pdf": "https://arxiv.org/pdf/2507.12416", "abs": "https://arxiv.org/abs/2507.12416", "authors": ["Jaehyun Kwak", "Ramahdani Muhammad Izaaz Inhar", "Se-Young Yun", "Sung-Ju Lee"], "title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Composed Image Retrieval (CIR) retrieves relevant images based on a reference\nimage and accompanying text describing desired modifications. However, existing\nCIR methods only focus on retrieving the target image and disregard the\nrelevance of other images. This limitation arises because most methods\nemploying contrastive learning-which treats the target image as positive and\nall other images in the batch as negatives-can inadvertently include false\nnegatives. This may result in retrieving irrelevant images, reducing user\nsatisfaction even when the target image is retrieved. To address this issue, we\npropose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which\noptimizes a reward model objective to reduce false negatives. Additionally, we\nintroduce a hard negative sampling strategy that selects images positioned\nbetween two steep drops in relevance scores following the target image, to\neffectively filter false negatives. In order to evaluate CIR models on their\nalignment with human satisfaction, we create Human-Preference FashionIQ\n(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond\ntarget retrieval. Extensive experiments demonstrate that QuRe achieves\nstate-of-the-art performance on FashionIQ and CIRR datasets while exhibiting\nthe strongest alignment with human preferences on the HP-FashionIQ dataset. The\nsource code is available at https://github.com/jackwaky/QuRe.", "AI": {"tldr": "QuRe\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u786c\u8d1f\u91c7\u6837\u51cf\u5c11\u5047\u8d1f\u4f8b\u7684\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u73b0\u6709CIR\u65b9\u6cd5\u4ec5\u5173\u6ce8\u76ee\u6807\u56fe\u50cf\u68c0\u7d22\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u56fe\u50cf\u7684\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u5047\u8d1f\u4f8b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5956\u52b1\u6a21\u578b\u76ee\u6807\u548c\u786c\u8d1f\u91c7\u6837\u7b56\u7565\uff0c\u4f18\u5316\u68c0\u7d22\u7ed3\u679c\u3002", "result": "\u5728FashionIQ\u548cCIRR\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u4e0e\u4eba\u7c7b\u504f\u597d\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "QuRe\u6709\u6548\u89e3\u51b3\u4e86\u5047\u8d1f\u4f8b\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u68c0\u7d22\u8d28\u91cf\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2507.12420", "pdf": "https://arxiv.org/pdf/2507.12420", "abs": "https://arxiv.org/abs/2507.12420", "authors": ["Haoyuan Liu", "Hiroshi Watanabe"], "title": "InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based IoU Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Bounding box regression (BBR) is fundamental to object detection, where the\nregression loss is crucial for accurate localization. Existing IoU-based losses\noften incorporate handcrafted geometric penalties to address IoU's\nnon-differentiability in non-overlapping cases and enhance BBR performance.\nHowever, these penalties are sensitive to box shape, size, and distribution,\noften leading to suboptimal optimization for small objects and undesired\nbehaviors such as bounding box enlargement due to misalignment with the IoU\nobjective. To address these limitations, we propose InterpIoU, a novel loss\nfunction that replaces handcrafted geometric penalties with a term based on the\nIoU between interpolated boxes and the target. By using interpolated boxes to\nbridge the gap between predictions and ground truth, InterpIoU provides\nmeaningful gradients in non-overlapping cases and inherently avoids the box\nenlargement issue caused by misaligned penalties. Simulation results further\nshow that IoU itself serves as an ideal regression target, while existing\ngeometric penalties are both unnecessary and suboptimal. Building on InterpIoU,\nwe introduce Dynamic InterpIoU, which dynamically adjusts interpolation\ncoefficients based on IoU values, enhancing adaptability to scenarios with\ndiverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC\nshow that our methods consistently outperform state-of-the-art IoU-based losses\nacross various detection frameworks, with particularly notable improvements in\nsmall object detection, confirming their effectiveness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570InterpIoU\uff0c\u901a\u8fc7\u63d2\u503c\u6846\u89e3\u51b3IoU\u5728\u975e\u91cd\u53e0\u60c5\u51b5\u4e0b\u7684\u4e0d\u53ef\u5fae\u95ee\u9898\uff0c\u5e76\u907f\u514d\u4f20\u7edf\u51e0\u4f55\u60e9\u7f5a\u7684\u5c40\u9650\u6027\u3002\u8fdb\u4e00\u6b65\u63d0\u51faDynamic InterpIoU\uff0c\u52a8\u6001\u8c03\u6574\u63d2\u503c\u7cfb\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eIoU\u7684\u635f\u5931\u51fd\u6570\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u51e0\u4f55\u60e9\u7f5a\uff0c\u5bf9\u6846\u7684\u5f62\u72b6\u3001\u5927\u5c0f\u548c\u5206\u5e03\u654f\u611f\uff0c\u5bfc\u81f4\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u4e0d\u4f73\u548c\u6846\u6269\u5927\u95ee\u9898\u3002", "method": "\u63d0\u51faInterpIoU\uff0c\u7528\u63d2\u503c\u6846\u4e0e\u76ee\u6807\u7684IoU\u66ff\u4ee3\u51e0\u4f55\u60e9\u7f5a\uff1b\u8fdb\u4e00\u6b65\u5f15\u5165Dynamic InterpIoU\uff0c\u52a8\u6001\u8c03\u6574\u63d2\u503c\u7cfb\u6570\u3002", "result": "\u5728COCO\u3001VisDrone\u548cPASCAL VOC\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709IoU\u635f\u5931\uff0c\u5c24\u5176\u5728\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "InterpIoU\u548cDynamic InterpIoU\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u51e0\u4f55\u60e9\u7f5a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.12426", "pdf": "https://arxiv.org/pdf/2507.12426", "abs": "https://arxiv.org/abs/2507.12426", "authors": ["Hayat Ullah", "Muhammad Ali Shafique", "Abbas Khan", "Arslan Munir"], "title": "DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition", "categories": ["cs.CV"], "comment": "17 pages", "summary": "The landscape of video recognition has evolved significantly, shifting from\ntraditional Convolutional Neural Networks (CNNs) to Transformer-based\narchitectures for improved accuracy. While 3D CNNs have been effective at\ncapturing spatiotemporal dynamics, recent Transformer models leverage\nself-attention to model long-range spatial and temporal dependencies. Despite\nachieving state-of-the-art performance on major benchmarks, Transformers remain\ncomputationally expensive, particularly with dense video data. To address this,\nwe propose a lightweight Video Focal Modulation Network, DVFL-Net, which\ndistills spatiotemporal knowledge from a large pre-trained teacher into a\ncompact nano student model, enabling efficient on-device deployment. DVFL-Net\nutilizes knowledge distillation and spatial-temporal feature modulation to\nsignificantly reduce computation while preserving high recognition performance.\nWe employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal\nfocal modulation to effectively transfer both local and global context from the\nVideo-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate\nDVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it\nagainst recent state-of-the-art methods in Human Action Recognition (HAR).\nAdditionally, we conduct a detailed ablation study analyzing the impact of\nforward KL divergence. The results confirm the superiority of DVFL-Net in\nachieving an optimal balance between performance and efficiency, demonstrating\nlower memory usage, reduced GFLOPs, and strong accuracy, making it a practical\nsolution for real-time HAR applications.", "AI": {"tldr": "DVFL-Net \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u9891\u8bc6\u522b\u7f51\u7edc\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u65f6\u7a7a\u7279\u5f81\u8c03\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "Transformer \u6a21\u578b\u5728\u89c6\u9891\u8bc6\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa DVFL-Net\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u548c\u65f6\u7a7a\u7126\u70b9\u8c03\u5236\uff0c\u4ece\u5927\u578b\u6559\u5e08\u6a21\u578b\u4e2d\u63d0\u53d6\u77e5\u8bc6\u5230\u5c0f\u578b\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u4f4e\uff0c\u6027\u80fd\u63a5\u8fd1\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DVFL-Net \u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u7406\u60f3\u5e73\u8861\uff0c\u9002\u5408\u5b9e\u65f6\u89c6\u9891\u8bc6\u522b\u4efb\u52a1\u3002"}}
{"id": "2507.12433", "pdf": "https://arxiv.org/pdf/2507.12433", "abs": "https://arxiv.org/abs/2507.12433", "authors": ["Fahimeh Orvati Nia", "Hai Lin"], "title": "Traffic-Aware Pedestrian Intention Prediction", "categories": ["cs.CV", "cs.SY", "eess.SY", "I.2.10; I.5.1"], "comment": "6 pages, 4 figures. Accepted to the American Control Conference (ACC)\n  2025", "summary": "Accurate pedestrian intention estimation is crucial for the safe navigation\nof autonomous vehicles (AVs) and hence attracts a lot of research attention.\nHowever, current models often fail to adequately consider dynamic traffic\nsignals and contextual scene information, which are critical for real-world\napplications. This paper presents a Traffic-Aware Spatio-Temporal Graph\nConvolutional Network (TA-STGCN) that integrates traffic signs and their states\n(Red, Yellow, Green) into pedestrian intention prediction. Our approach\nintroduces the integration of dynamic traffic signal states and bounding box\nsize as key features, allowing the model to capture both spatial and temporal\ndependencies in complex urban environments. The model surpasses existing\nmethods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy\ncompared to the baseline model on the PIE dataset, demonstrating its\neffectiveness in improving pedestrian intention prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u901a\u4fe1\u53f7\u611f\u77e5\u7684\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc\uff08TA-STGCN\uff09\uff0c\u7528\u4e8e\u63d0\u9ad8\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u4e864.75%\u3002", "motivation": "\u5f53\u524d\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u6a21\u578b\u672a\u80fd\u5145\u5206\u52a8\u6001\u4ea4\u901a\u4fe1\u53f7\u548c\u573a\u666f\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u8fd9\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u5bfc\u822a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faTA-STGCN\uff0c\u6574\u5408\u52a8\u6001\u4ea4\u901a\u4fe1\u53f7\u72b6\u6001\u548c\u8fb9\u754c\u6846\u5927\u5c0f\u4f5c\u4e3a\u5173\u952e\u7279\u5f81\uff0c\u6355\u6349\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728PIE\u6570\u636e\u96c6\u4e0a\uff0cTA-STGCN\u6bd4\u57fa\u7ebf\u6a21\u578b\u51c6\u786e\u7387\u63d0\u9ad8\u4e864.75%\u3002", "conclusion": "TA-STGCN\u901a\u8fc7\u6574\u5408\u52a8\u6001\u4ea4\u901a\u4fe1\u53f7\u548c\u573a\u666f\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.12441", "pdf": "https://arxiv.org/pdf/2507.12441", "abs": "https://arxiv.org/abs/2507.12441", "authors": ["Yen-Linh Vu", "Dinh-Thang Duong", "Truong-Binh Duong", "Anh-Khoi Nguyen", "Thanh-Huy Nguyen", "Le Thien Phuc Nguyen", "Jianhua Xing", "Xingjian Li", "Tianyang Wang", "Ulas Bagci", "Min Xu"], "title": "Describe Anything Model for Visual Question Answering on Text-rich Images", "categories": ["cs.CV", "cs.LG"], "comment": "11 pages, 5 figures. Accepted to VisionDocs @ ICCV 2025", "summary": "Recent progress has been made in region-aware vision-language modeling,\nparticularly with the emergence of the Describe Anything Model (DAM). DAM is\ncapable of generating detailed descriptions of any specific image areas or\nobjects without the need for additional localized image-text alignment\nsupervision. We hypothesize that such region-level descriptive capability is\nbeneficial for the task of Visual Question Answering (VQA), especially in\nchallenging scenarios involving images with dense text. In such settings, the\nfine-grained extraction of textual information is crucial to producing correct\nanswers. Motivated by this, we introduce DAM-QA, a framework with a tailored\nevaluation protocol, developed to investigate and harness the region-aware\ncapabilities from DAM for the text-rich VQA problem that requires reasoning\nover text-based information within images. DAM-QA incorporates a mechanism that\naggregates answers from multiple regional views of image content, enabling more\neffective identification of evidence that may be tied to text-related elements.\nExperiments on six VQA benchmarks show that our approach consistently\noutperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA\nalso achieves the best overall performance among region-aware models with fewer\nparameters, significantly narrowing the gap with strong generalist VLMs. These\nresults highlight the potential of DAM-like models for text-rich and broader\nVQA tasks when paired with efficient usage and integration strategies. Our code\nis publicly available at https://github.com/Linvyl/DAM-QA.git.", "AI": {"tldr": "DAM-QA\u5229\u7528\u533a\u57df\u611f\u77e5\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578bDAM\uff0c\u901a\u8fc7\u591a\u533a\u57df\u89c6\u56fe\u805a\u5408\u7b54\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5bc6\u96c6\u56fe\u50cf\u4e2d\u7684VQA\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u6587\u672c\u5bc6\u96c6\u56fe\u50cf\u4e2d\u7684VQA\u4efb\u52a1\uff0c\u9700\u8981\u7ec6\u7c92\u5ea6\u63d0\u53d6\u6587\u672c\u4fe1\u606f\uff0cDAM\u7684\u533a\u57df\u63cf\u8ff0\u80fd\u529b\u53ef\u80fd\u5bf9\u6b64\u6709\u76ca\u3002", "method": "\u63d0\u51faDAM-QA\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u533a\u57df\u89c6\u56fe\u805a\u5408\u7b54\u6848\uff0c\u4f18\u5316\u6587\u672c\u76f8\u5173\u8bc1\u636e\u7684\u8bc6\u522b\u3002", "result": "\u5728\u516d\u4e2aVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebfDAM\uff0cDocVQA\u4e0a\u63d0\u53477+\u5206\uff0c\u53c2\u6570\u66f4\u5c11\u4e14\u6027\u80fd\u63a5\u8fd1\u901a\u7528VLMs\u3002", "conclusion": "DAM-QA\u5c55\u793a\u4e86\u533a\u57df\u611f\u77e5\u6a21\u578b\u5728\u6587\u672c\u5bc6\u96c6VQA\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u9700\u9ad8\u6548\u4f7f\u7528\u548c\u96c6\u6210\u7b56\u7565\u3002"}}
{"id": "2507.12449", "pdf": "https://arxiv.org/pdf/2507.12449", "abs": "https://arxiv.org/abs/2507.12449", "authors": ["Van-Hoang-Anh Phan", "Chi-Tam Nguyen", "Doan-Trung Au", "Thanh-Danh Phan", "Minh-Thien Duong", "My-Ha Le"], "title": "Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios", "categories": ["cs.CV"], "comment": "7 pages, 6 figures, 4 tables, HSI 2025", "summary": "Obstacle avoidance is essential for ensuring the safety of autonomous\nvehicles. Accurate perception and motion planning are crucial to enabling\nvehicles to navigate complex environments while avoiding collisions. In this\npaper, we propose an efficient obstacle avoidance pipeline that leverages a\ncamera-only perception module and a Frenet-Pure Pursuit-based planning\nstrategy. By integrating advancements in computer vision, the system utilizes\nYOLOv11 for object detection and state-of-the-art monocular depth estimation\nmodels, such as Depth Anything V2, to estimate object distances. A comparative\nanalysis of these models provides valuable insights into their accuracy,\nefficiency, and robustness in real-world conditions. The system is evaluated in\ndiverse scenarios on a university campus, demonstrating its effectiveness in\nhandling various obstacles and enhancing autonomous navigation. The video\npresenting the results of the obstacle avoidance experiments is available at:\nhttps://www.youtube.com/watch?v=FoXiO5S_tA8", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6444\u50cf\u5934\u611f\u77e5\u548cFrenet-Pure Pursuit\u89c4\u5212\u7684\u969c\u788d\u7269\u907f\u969c\u7cfb\u7edf\uff0c\u7ed3\u5408YOLOv11\u548c\u76ee\u6807\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u5728\u6821\u56ed\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u6027\u9700\u8981\u51c6\u786e\u7684\u611f\u77e5\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u4ee5\u5728\u590d\u6742\u73af\u5883\u4e2d\u907f\u514d\u78b0\u649e\u3002", "method": "\u4f7f\u7528\u6444\u50cf\u5934\u611f\u77e5\u6a21\u5757\uff08YOLOv11\u548cDepth Anything V2\uff09\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u548c\u6df1\u5ea6\u4f30\u8ba1\uff0c\u7ed3\u5408Frenet-Pure Pursuit\u89c4\u5212\u7b56\u7565\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u6821\u56ed\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u969c\u788d\u7269\u5e76\u63d0\u5347\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u969c\u788d\u7269\u907f\u969c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u5bfc\u822a\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12455", "pdf": "https://arxiv.org/pdf/2507.12455", "abs": "https://arxiv.org/abs/2507.12455", "authors": ["Shangpin Peng", "Senqiao Yang", "Li Jiang", "Zhuotao Tian"], "title": "Mitigating Object Hallucinations via Sentence-Level Early Intervention", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have revolutionized cross-modal\nunderstanding but continue to struggle with hallucinations - fabricated content\ncontradicting visual inputs. Existing hallucination mitigation methods either\nincur prohibitive computational costs or introduce distribution mismatches\nbetween training data and model outputs. We identify a critical insight:\nhallucinations predominantly emerge at the early stages of text generation and\npropagate through subsequent outputs. To address this, we propose **SENTINEL**\n(**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain\npr**E**ference **L**earning), a framework that eliminates dependency on human\nannotations. Specifically, we first bootstrap high-quality in-domain preference\npairs by iteratively sampling model outputs, validating object existence\nthrough cross-checking with two open-vocabulary detectors, and classifying\nsentences into hallucinated/non-hallucinated categories. Subsequently, we use\ncontext-coherent positive samples and hallucinated negative samples to build\ncontext-aware preference data iteratively. Finally, we train models using a\ncontext-aware preference loss (C-DPO) that emphasizes discriminative learning\nat the sentence level where hallucinations initially manifest. Experimental\nresults show that SENTINEL can reduce hallucinations by over 90\\% compared to\nthe original model and outperforms the previous state-of-the-art method on both\nhallucination benchmarks and general capabilities benchmarks, demonstrating its\nsuperiority and generalization ability. The models, datasets, and code are\navailable at https://github.com/pspdada/SENTINEL.", "AI": {"tldr": "SENTINEL\u6846\u67b6\u901a\u8fc7\u53e5\u5b50\u7ea7\u65e9\u671f\u5e72\u9884\u548c\u57df\u5185\u504f\u597d\u5b66\u4e60\uff0c\u663e\u8457\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6a21\u6001\u7406\u89e3\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u5f15\u5165\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51faSENTINEL\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u91c7\u6837\u3001\u9a8c\u8bc1\u548c\u5206\u7c7b\u6784\u5efa\u9ad8\u8d28\u91cf\u504f\u597d\u6570\u636e\uff0c\u5e76\u8bad\u7ec3\u6a21\u578b\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u504f\u597d\u635f\u5931\uff08C-DPO\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSENTINEL\u80fd\u5c06\u5e7b\u89c9\u51cf\u5c1190%\u4ee5\u4e0a\uff0c\u5e76\u5728\u5e7b\u89c9\u548c\u901a\u7528\u80fd\u529b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SENTINEL\u5728\u51cf\u5c11\u5e7b\u89c9\u548c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.12461", "pdf": "https://arxiv.org/pdf/2507.12461", "abs": "https://arxiv.org/abs/2507.12461", "authors": ["Trong-Thang Pham", "Anh Nguyen", "Zhigang Deng", "Carol C. Wu", "Hien Van Nguyen", "Ngan Le"], "title": "Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis", "categories": ["cs.CV", "cs.AI"], "comment": "ACM MM 2025", "summary": "Radiologists rely on eye movements to navigate and interpret medical images.\nA trained radiologist possesses knowledge about the potential diseases that may\nbe present in the images and, when searching, follows a mental checklist to\nlocate them using their gaze. This is a key observation, yet existing models\nfail to capture the underlying intent behind each fixation. In this paper, we\nintroduce a deep learning-based approach, RadGazeIntent, designed to model this\nbehavior: having an intention to find something and actively searching for it.\nOur transformer-based architecture processes both the temporal and spatial\ndimensions of gaze data, transforming fine-grained fixation features into\ncoarse, meaningful representations of diagnostic intent to interpret\nradiologists' goals. To capture the nuances of radiologists' varied\nintention-driven behaviors, we process existing medical eye-tracking datasets\nto create three intention-labeled subsets: RadSeq (Systematic Sequential\nSearch), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid\nPattern). Experimental results demonstrate RadGazeIntent's ability to predict\nwhich findings radiologists are examining at specific moments, outperforming\nbaseline methods across all intention-labeled datasets.", "AI": {"tldr": "RadGazeIntent\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u65e8\u5728\u6355\u6349\u653e\u5c04\u79d1\u533b\u751f\u5728\u89e3\u8bfb\u533b\u5b66\u56fe\u50cf\u65f6\u7684\u610f\u56fe\u9a71\u52a8\u884c\u4e3a\uff0c\u901a\u8fc7\u5904\u7406\u773c\u52a8\u6570\u636e\u9884\u6d4b\u5176\u8bca\u65ad\u610f\u56fe\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u672a\u80fd\u6355\u6349\u653e\u5c04\u79d1\u533b\u751f\u5728\u56fe\u50cf\u89e3\u8bfb\u4e2d\u7684\u610f\u56fe\uff0c\u800c\u7406\u89e3\u5176\u610f\u56fe\u5bf9\u63d0\u5347\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u5904\u7406\u773c\u52a8\u6570\u636e\u7684\u65f6\u7a7a\u7ef4\u5ea6\uff0c\u751f\u6210\u8bca\u65ad\u610f\u56fe\u7684\u7c97\u7c92\u5ea6\u8868\u793a\uff0c\u5e76\u5229\u7528\u4e09\u4e2a\u610f\u56fe\u6807\u8bb0\u6570\u636e\u96c6\uff08RadSeq\u3001RadExplore\u3001RadHybrid\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "RadGazeIntent\u5728\u6240\u6709\u610f\u56fe\u6807\u8bb0\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u51c6\u786e\u9884\u6d4b\u653e\u5c04\u79d1\u533b\u751f\u5728\u7279\u5b9a\u65f6\u523b\u7684\u68c0\u67e5\u76ee\u6807\u3002", "conclusion": "RadGazeIntent\u6210\u529f\u5efa\u6a21\u4e86\u653e\u5c04\u79d1\u533b\u751f\u7684\u610f\u56fe\u9a71\u52a8\u884c\u4e3a\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u89e3\u8bfb\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.12462", "pdf": "https://arxiv.org/pdf/2507.12462", "abs": "https://arxiv.org/abs/2507.12462", "authors": ["Yuxi Xiao", "Jianyuan Wang", "Nan Xue", "Nikita Karaev", "Yuri Makarov", "Bingyi Kang", "Xing Zhu", "Hujun Bao", "Yujun Shen", "Xiaowei Zhou"], "title": "SpatialTrackerV2: 3D Point Tracking Made Easy", "categories": ["cs.CV"], "comment": "International Conference on Computer Vision, ICCV 2025. Huggingface\n  Demo: https://huggingface.co/spaces/Yuxihenry/SpatialTrackerV2, Code:\n  https://github.com/henry123-boy/SpaTrackerV2", "summary": "We present SpatialTrackerV2, a feed-forward 3D point tracking method for\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\ncomponents for 3D tracking, our approach unifies the intrinsic connections\nbetween point tracking, monocular depth, and camera pose estimation into a\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\nwith a fully differentiable and end-to-end architecture, allowing scalable\ntraining across a wide range of datasets, including synthetic sequences, posed\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\ndynamic 3D reconstruction approaches while running 50$\\times$ faster.", "AI": {"tldr": "SpatialTrackerV2\u662f\u4e00\u79cd\u57fa\u4e8e\u5355\u76ee\u89c6\u9891\u7684\u524d\u99883D\u70b9\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u51e0\u4f55\u3001\u76f8\u673a\u8fd0\u52a8\u548c\u7269\u4f53\u8fd0\u52a8\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u67093D\u8ddf\u8e2a\u65b9\u6cd5\u4f9d\u8d56\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u6027\u80fd\u53d7\u9650\uff0cSpatialTrackerV2\u65e8\u5728\u901a\u8fc7\u7aef\u5230\u7aef\u67b6\u6784\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5c063D\u8fd0\u52a8\u5206\u89e3\u4e3a\u573a\u666f\u51e0\u4f55\u3001\u76f8\u673a\u8fd0\u52a8\u548c\u7269\u4f53\u8fd0\u52a8\uff0c\u91c7\u7528\u53ef\u5fae\u5206\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u652f\u6301\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u3002", "result": "\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd530%\uff0c\u4e0e\u52a8\u60013D\u91cd\u5efa\u65b9\u6cd5\u7cbe\u5ea6\u76f8\u5f53\uff0c\u4f46\u901f\u5ea6\u5feb50\u500d\u3002", "conclusion": "SpatialTrackerV2\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u51e0\u4f55\u548c\u8fd0\u52a8\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u76843D\u70b9\u8ddf\u8e2a\u3002"}}
{"id": "2507.12463", "pdf": "https://arxiv.org/pdf/2507.12463", "abs": "https://arxiv.org/abs/2507.12463", "authors": ["Renjie Li", "Ruijie Ye", "Mingyang Wu", "Hao Frank Yang", "Zhiwen Fan", "Hezhen Hu", "Zhengzhong Tu"], "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behavior$\\unicode{x2014}$such as motion, trajectories, and\nintention$\\unicode{x2014}$a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose $\\textbf{MMHU}$, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasks$\\unicode{x2014}$ranging from motion prediction to motion\ngeneration and human behavior question answering$\\unicode{x2014}$thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMMHU\u7684\u5927\u89c4\u6a21\u4eba\u7c7b\u884c\u4e3a\u5206\u6790\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\u7406\u89e3\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u6ce8\u91ca\u548c\u591a\u4efb\u52a1\u8bc4\u4f30\u3002", "motivation": "\u7406\u89e3\u4eba\u7c7b\u884c\u4e3a\u5bf9\u5f00\u53d1\u5b89\u5168\u7684\u9a7e\u9a76\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faMMHU\u57fa\u51c6\uff0c\u5305\u542b57k\u4eba\u7c7b\u8fd0\u52a8\u7247\u6bb5\u548c1.73M\u5e27\u6570\u636e\uff0c\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u6807\u6ce8\u6d41\u7a0b\u751f\u6210\u884c\u4e3a\u63cf\u8ff0\u3002", "result": "\u6570\u636e\u96c6\u6db5\u76d6\u591a\u6837\u5316\u6765\u6e90\uff0c\u652f\u6301\u4ece\u8fd0\u52a8\u9884\u6d4b\u5230\u884c\u4e3a\u95ee\u7b54\u7684\u591a\u4efb\u52a1\u8bc4\u4f30\u3002", "conclusion": "MMHU\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\u7406\u89e3\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2507.12464", "pdf": "https://arxiv.org/pdf/2507.12464", "abs": "https://arxiv.org/abs/2507.12464", "authors": ["Muhammed Furkan Dasdelen", "Hyesu Lim", "Michele Buck", "Katharina S. G\u00f6tze", "Carsten Marr", "Steffen Schneider"], "title": "CytoSAE: Interpretable Cell Embeddings for Hematology", "categories": ["cs.CV", "cs.LG", "q-bio.QM"], "comment": "11 pages, 5 figures", "summary": "Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic\ninterpretability of transformer-based foundation models. Very recently, SAEs\nwere also adopted for the visual domain, enabling the discovery of visual\nconcepts and their patch-wise attribution to tokens in the transformer model.\nWhile a growing number of foundation models emerged for medical imaging, tools\nfor explaining their inferences are still lacking. In this work, we show the\napplicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder\nwhich is trained on over 40,000 peripheral blood single-cell images. CytoSAE\ngeneralizes to diverse and out-of-domain datasets, including bone marrow\ncytology, where it identifies morphologically relevant concepts which we\nvalidated with medical experts. Furthermore, we demonstrate scenarios in which\nCytoSAE can generate patient-specific and disease-specific concepts, enabling\nthe detection of pathognomonic cells and localized cellular abnormalities at\nthe patch level. We quantified the effect of concepts on a patient-level AML\nsubtype classification task and show that CytoSAE concepts reach performance\ncomparable to the state-of-the-art, while offering explainability on the\nsub-cellular level. Source code and model weights are available at\nhttps://github.com/dynamical-inference/cytosae.", "AI": {"tldr": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u5c55\u793a\u4e86\u5176\u5728\u89e3\u91ca\u6a21\u578b\u63a8\u65ad\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u8840\u6db2\u5b66\u4e2d\u3002\u63d0\u51fa\u7684CytoSAE\u80fd\u8bc6\u522b\u5f62\u6001\u5b66\u76f8\u5173\u6982\u5ff5\uff0c\u5e76\u5728\u60a3\u8005\u548c\u75be\u75c5\u7279\u5b9a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u533b\u5b66\u5f71\u50cf\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u89e3\u91ca\u5176\u63a8\u65ad\u7684\u5de5\u5177\u4ecd\u7136\u7f3a\u4e4f\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22SAEs\u5728\u8840\u6db2\u5b66\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faCytoSAE\uff0c\u4e00\u79cd\u57284\u4e07\u591a\u4e2a\u5916\u5468\u8840\u5355\u7ec6\u80de\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u591a\u6837\u5316\u548c\u57df\u5916\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "CytoSAE\u80fd\u8bc6\u522b\u5f62\u6001\u5b66\u76f8\u5173\u6982\u5ff5\uff0c\u5e76\u5728AML\u4e9a\u578b\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e9a\u7ec6\u80de\u6c34\u5e73\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CytoSAE\u4e3a\u533b\u5b66\u5f71\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u5de5\u5177\uff0c\u80fd\u591f\u8bc6\u522b\u75c5\u7406\u7279\u5f81\u5e76\u5728\u4e34\u5e8a\u4efb\u52a1\u4e2d\u53d1\u6325\u4f5c\u7528\u3002"}}
{"id": "2507.12465", "pdf": "https://arxiv.org/pdf/2507.12465", "abs": "https://arxiv.org/abs/2507.12465", "authors": ["Ziang Cao", "Zhaoxi Chen", "Linag Pan", "Ziwei Liu"], "title": "PhysX: Physical-Grounded 3D Asset Generation", "categories": ["cs.CV"], "comment": "Project page: https://physx-3d.github.io/", "summary": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose \\textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose \\textbf{PhysXGen}, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPhysX\uff0c\u4e00\u79cd\u7269\u7406\u57fa\u7840\u76843D\u8d44\u4ea7\u751f\u6210\u65b9\u6cd5\uff0c\u5305\u62ec\u6570\u636e\u96c6PhysXNet\u548c\u751f\u6210\u6846\u67b6PhysXGen\uff0c\u586b\u8865\u4e86\u7269\u7406\u6807\u6ce83D\u6570\u636e\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u67093D\u751f\u6210\u6a21\u578b\u5ffd\u89c6\u7269\u7406\u5c5e\u6027\uff0c\u9650\u5236\u4e86\u5728\u4eff\u771f\u548c\u5177\u8eabAI\u7b49\u7269\u7406\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "1) \u6784\u5efaPhysXNet\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u7269\u7406\u5c5e\u6027\uff1b2) \u63d0\u51faPhysXGen\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u7ed3\u54083D\u7ed3\u6784\u4e0e\u7269\u7406\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PhysXGen\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PhysX\u4e3a\u751f\u6210\u7269\u7406AI\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5c06\u5f00\u6e90\u3002"}}
{"id": "2507.11551", "pdf": "https://arxiv.org/pdf/2507.11551", "abs": "https://arxiv.org/abs/2507.11551", "authors": ["Ekaterina Stansfield", "Jennifer A. Mitterer", "Abdulrahman Altahhan"], "title": "Landmark Detection for Medical Images using a General-purpose Segmentation Model", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "13 pages, 8 figures, 2 tables. Submitted to ICONIP 2025", "summary": "Radiographic images are a cornerstone of medical diagnostics in orthopaedics,\nwith anatomical landmark detection serving as a crucial intermediate step for\ninformation extraction. General-purpose foundational segmentation models, such\nas SAM (Segment Anything Model), do not support landmark segmentation out of\nthe box and require prompts to function. However, in medical imaging, the\nprompts for landmarks are highly specific. Since SAM has not been trained to\nrecognize such landmarks, it cannot generate accurate landmark segmentations\nfor diagnostic purposes. Even MedSAM, a medically adapted variant of SAM, has\nbeen trained to identify larger anatomical structures, such as organs and their\nparts, and lacks the fine-grained precision required for orthopaedic pelvic\nlandmarks. To address this limitation, we propose leveraging another\ngeneral-purpose, non-foundational model: YOLO. YOLO excels in object detection\nand can provide bounding boxes that serve as input prompts for SAM. While YOLO\nis efficient at detection, it is significantly outperformed by SAM in\nsegmenting complex structures. In combination, these two models form a reliable\npipeline capable of segmenting not only a small pilot set of eight anatomical\nlandmarks but also an expanded set of 72 landmarks and 16 regions with complex\noutlines, such as the femoral cortical bone and the pelvic inlet. By using\nYOLO-generated bounding boxes to guide SAM, we trained the hybrid model to\naccurately segment orthopaedic pelvic radiographs. Our results show that the\nproposed combination of YOLO and SAM yields excellent performance in detecting\nanatomical landmarks and intricate outlines in orthopaedic pelvic radiographs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408YOLO\u548cSAM\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u9aa8\u79d1\u9aa8\u76c6X\u5149\u7247\u4e2d\u7cbe\u786e\u5206\u5272\u89e3\u5256\u6807\u5fd7\u548c\u590d\u6742\u8f6e\u5ed3\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u5206\u5272\u6a21\u578b\uff08\u5982SAM\uff09\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u89e3\u5256\u6807\u5fd7\u5206\u5272\uff0c\u800c\u533b\u5b66\u4e13\u7528\u6a21\u578b\uff08\u5982MedSAM\uff09\u7f3a\u4e4f\u5bf9\u9aa8\u79d1\u9aa8\u76c6\u6807\u5fd7\u7684\u7cbe\u7ec6\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u5229\u7528YOLO\u68c0\u6d4b\u89e3\u5256\u6807\u5fd7\u5e76\u751f\u6210\u8fb9\u754c\u6846\u4f5c\u4e3aSAM\u7684\u8f93\u5165\u63d0\u793a\uff0c\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u6784\u5efa\u6df7\u5408\u6a21\u578b\u3002", "result": "\u6df7\u5408\u6a21\u578b\u5728\u5206\u52728\u4e2a\u89e3\u5256\u6807\u5fd7\u7684\u57fa\u7840\u4e0a\uff0c\u8fdb\u4e00\u6b65\u6269\u5c55\u523072\u4e2a\u6807\u5fd7\u548c16\u4e2a\u590d\u6742\u8f6e\u5ed3\u533a\u57df\uff0c\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "YOLO\u4e0eSAM\u7684\u7ed3\u5408\u4e3a\u9aa8\u79d1\u9aa8\u76c6X\u5149\u7247\u7684\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11557", "pdf": "https://arxiv.org/pdf/2507.11557", "abs": "https://arxiv.org/abs/2507.11557", "authors": ["Jiaxu Zheng", "Meiman He", "Xuhui Tang", "Xiong Wang", "Tuoyu Cao", "Tianyi Zeng", "Lichi Zhang", "Chenyu You"], "title": "3D Wavelet Latent Diffusion Model for Whole-Body MR-to-CT Modality Translation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Magnetic Resonance (MR) imaging plays an essential role in contemporary\nclinical diagnostics. It is increasingly integrated into advanced therapeutic\nworkflows, such as hybrid Positron Emission Tomography/Magnetic Resonance\n(PET/MR) imaging and MR-only radiation therapy. These integrated approaches are\ncritically dependent on accurate estimation of radiation attenuation, which is\ntypically facilitated by synthesizing Computed Tomography (CT) images from MR\nscans to generate attenuation maps. However, existing MR-to-CT synthesis\nmethods for whole-body imaging often suffer from poor spatial alignment between\nthe generated CT and input MR images, and insufficient image quality for\nreliable use in downstream clinical tasks. In this paper, we present a novel 3D\nWavelet Latent Diffusion Model (3D-WLDM) that addresses these limitations by\nperforming modality translation in a learned latent space. By incorporating a\nWavelet Residual Module into the encoder-decoder architecture, we enhance the\ncapture and reconstruction of fine-scale features across image and latent\nspaces. To preserve anatomical integrity during the diffusion process, we\ndisentangle structural and modality-specific characteristics and anchor the\nstructural component to prevent warping. We also introduce a Dual Skip\nConnection Attention mechanism within the diffusion model, enabling the\ngeneration of high-resolution CT images with improved representation of bony\nstructures and soft-tissue contrast.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b3D\u5c0f\u6ce2\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff083D-WLDM\uff09\uff0c\u7528\u4e8e\u4eceMR\u56fe\u50cf\u5408\u6210\u9ad8\u8d28\u91cfCT\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u5bf9\u9f50\u548c\u56fe\u50cf\u8d28\u91cf\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "MR\u6210\u50cf\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709MR-to-CT\u5408\u6210\u65b9\u6cd5\u5728\u7a7a\u95f4\u5bf9\u9f50\u548c\u56fe\u50cf\u8d28\u91cf\u4e0a\u5b58\u5728\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u5176\u5728\u6df7\u5408PET/MR\u6210\u50cf\u548cMR-only\u653e\u7597\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5728\u5c0f\u6ce2\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6a21\u6001\u8f6c\u6362\uff0c\u7ed3\u5408\u5c0f\u6ce2\u6b8b\u5dee\u6a21\u5757\u589e\u5f3a\u7279\u5f81\u6355\u83b7\uff0c\u5e76\u5f15\u5165\u53cc\u8df3\u8dc3\u8fde\u63a5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u751f\u6210\u9ad8\u5206\u8fa8\u7387CT\u56fe\u50cf\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u5177\u6709\u66f4\u597d\u9aa8\u9abc\u7ed3\u6784\u548c\u8f6f\u7ec4\u7ec7\u5bf9\u6bd4\u7684\u9ad8\u8d28\u91cfCT\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u7684\u5b8c\u6574\u6027\u3002", "conclusion": "3D-WLDM\u4e3aMR-to-CT\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63d0\u5347\u4e0b\u6e38\u4e34\u5e8a\u4efb\u52a1\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.11561", "pdf": "https://arxiv.org/pdf/2507.11561", "abs": "https://arxiv.org/abs/2507.11561", "authors": ["Lucas Erlacher", "Samuel Ruip\u00e9rez-Campillo", "Holger Michel", "Sven Wellmann", "Thomas M. Sutter", "Ece Ozkan", "Julia E. Vogt"], "title": "Predicting Pulmonary Hypertension in Newborns: A Multi-view VAE Approach", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Pulmonary hypertension (PH) in newborns is a critical condition characterized\nby elevated pressure in the pulmonary arteries, leading to right ventricular\nstrain and heart failure. While right heart catheterization (RHC) is the\ndiagnostic gold standard, echocardiography is preferred due to its non-invasive\nnature, safety, and accessibility. However, its accuracy highly depends on the\noperator, making PH assessment subjective. While automated detection methods\nhave been explored, most models focus on adults and rely on single-view\nechocardiographic frames, limiting their performance in diagnosing PH in\nnewborns. While multi-view echocardiography has shown promise in improving PH\nassessment, existing models struggle with generalizability. In this work, we\nemploy a multi-view variational autoencoder (VAE) for PH prediction using\nechocardiographic videos. By leveraging the VAE framework, our model captures\ncomplex latent representations, improving feature extraction and robustness. We\ncompare its performance against single-view and supervised learning approaches.\nOur results show improved generalization and classification accuracy,\nhighlighting the effectiveness of multi-view learning for robust PH assessment\nin newborns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u89c6\u89d2\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u65b0\u751f\u513f\u80ba\u52a8\u8109\u9ad8\u538b\uff08PH\uff09\u7684\u9884\u6d4b\uff0c\u63d0\u9ad8\u4e86\u7279\u5f81\u63d0\u53d6\u7684\u9c81\u68d2\u6027\u548c\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u65b0\u751f\u513f\u80ba\u52a8\u8109\u9ad8\u538b\uff08PH\uff09\u7684\u8bca\u65ad\u4f9d\u8d56\u8d85\u58f0\u5fc3\u52a8\u56fe\uff0c\u4f46\u5176\u51c6\u786e\u6027\u53d7\u64cd\u4f5c\u8005\u4e3b\u89c2\u5f71\u54cd\u3002\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u591a\u9488\u5bf9\u6210\u4eba\u4e14\u57fa\u4e8e\u5355\u89c6\u89d2\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u91c7\u7528\u591a\u89c6\u89d2\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u6846\u67b6\uff0c\u4ece\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u4e2d\u63d0\u53d6\u590d\u6742\u6f5c\u5728\u8868\u5f81\uff0c\u5e76\u4e0e\u5355\u89c6\u89d2\u548c\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u591a\u89c6\u89d2\u5b66\u4e60\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u548c\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u5355\u89c6\u89d2\u548c\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u591a\u89c6\u89d2\u5b66\u4e60\u4e3a\u65b0\u751f\u513fPH\u7684\u7a33\u5065\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.11569", "pdf": "https://arxiv.org/pdf/2507.11569", "abs": "https://arxiv.org/abs/2507.11569", "authors": ["Hanxue Gu", "Yaqian Chen", "Nicholas Konz", "Qihang Li", "Maciej A. Mazurowski"], "title": "Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "3 figures, 9 pages", "summary": "Foundation models, pre-trained on large image datasets and capable of\ncapturing rich feature representations, have recently shown potential for\nzero-shot image registration. However, their performance has mostly been tested\nin the context of rigid or less complex structures, such as the brain or\nabdominal organs, and it remains unclear whether these models can handle more\nchallenging, deformable anatomy. Breast MRI registration is particularly\ndifficult due to significant anatomical variation between patients, deformation\ncaused by patient positioning, and the presence of thin and complex internal\nstructure of fibroglandular tissue, where accurate alignment is crucial.\nWhether foundation model-based registration algorithms can address this level\nof complexity remains an open question. In this study, we provide a\ncomprehensive evaluation of foundation model-based registration algorithms for\nbreast MRI. We assess five pre-trained encoders, including DINO-v2, SAM,\nMedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that\ncapture variations in different years and dates, sequences, modalities, and\npatient disease status (lesion versus no lesion). Our results show that\nfoundation model-based algorithms such as SAM outperform traditional\nregistration baselines for overall breast alignment, especially under large\ndomain shifts, but struggle with capturing fine details of fibroglandular\ntissue. Interestingly, additional pre-training or fine-tuning on medical or\nbreast-specific images in MedSAM and SSLSAM, does not improve registration\nperformance and may even decrease it in some cases. Further work is needed to\nunderstand how domain-specific training influences registration and to explore\ntargeted strategies that improve both global alignment and fine structure\naccuracy. We also publicly release our code at\n\\href{https://github.com/mazurowski-lab/Foundation-based-reg}{Github}.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u56fe\u50cf\u914d\u51c6\u7b97\u6cd5\u5728\u4e73\u817aMRI\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0SAM\u7b49\u7b97\u6cd5\u5728\u5927\u57df\u504f\u79fb\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u5bf9\u7ea4\u7ef4\u817a\u4f53\u7ec4\u7ec7\u7684\u7ec6\u8282\u6355\u6349\u4e0d\u8db3\u3002", "motivation": "\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u53ef\u53d8\u5f62\u89e3\u5256\u7ed3\u6784\uff08\u5982\u4e73\u817aMRI\uff09\u4e2d\u7684\u914d\u51c6\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u8bc4\u4f30\u4e94\u79cd\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff08DINO-v2\u3001SAM\u3001MedSAM\u3001SSLSAM\u3001MedCLIP\uff09\u5728\u56db\u79cd\u4e73\u817a\u914d\u51c6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "SAM\u7b49\u7b97\u6cd5\u5728\u6574\u4f53\u4e73\u817a\u5bf9\u9f50\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u5bf9\u7ea4\u7ef4\u817a\u4f53\u7ec4\u7ec7\u7ec6\u8282\u5904\u7406\u4e0d\u4f73\uff1b\u533b\u5b66\u7279\u5b9a\u9884\u8bad\u7ec3\u672a\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u5bf9\u914d\u51c6\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u540c\u65f6\u63d0\u5347\u5168\u5c40\u5bf9\u9f50\u548c\u7ec6\u8282\u7cbe\u5ea6\u7684\u7b56\u7565\u3002"}}
{"id": "2507.11690", "pdf": "https://arxiv.org/pdf/2507.11690", "abs": "https://arxiv.org/abs/2507.11690", "authors": ["Amaya Dharmasiri", "William Yang", "Polina Kirichenko", "Lydia Liu", "Olga Russakovsky"], "title": "The Impact of Coreset Selection on Spurious Correlations and Group Robustness", "categories": ["cs.LG", "cs.CV"], "comment": "10 pages, 9 additional pages for Appendix", "summary": "Coreset selection methods have shown promise in reducing the training data\nsize while maintaining model performance for data-efficient machine learning.\nHowever, as many datasets suffer from biases that cause models to learn\nspurious correlations instead of causal features, it is important to understand\nwhether and how dataset reduction methods may perpetuate, amplify, or mitigate\nthese biases. In this work, we conduct the first comprehensive analysis of the\nimplications of data selection on the spurious bias levels of the selected\ncoresets and the robustness of downstream models trained on them. We use an\nextensive experimental setting spanning ten different spurious correlations\nbenchmarks, five score metrics to characterize sample importance/ difficulty,\nand five data selection policies across a broad range of coreset sizes.\nThereby, we unravel a series of nontrivial nuances in interactions between\nsample difficulty and bias alignment, as well as dataset bias and resultant\nmodel robustness. For example, we find that selecting coresets using\nembedding-based sample characterization scores runs a comparatively lower risk\nof inadvertently exacerbating bias than selecting using characterizations based\non learning dynamics. Most importantly, our analysis reveals that although some\ncoreset selection methods could achieve lower bias levels by prioritizing\ndifficult samples, they do not reliably guarantee downstream robustness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u5bf9\u6838\u5fc3\u96c6\u504f\u5dee\u6c34\u5e73\u53ca\u4e0b\u6e38\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u6837\u672c\u96be\u5ea6\u4e0e\u504f\u5dee\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u5e76\u53d1\u73b0\u67d0\u4e9b\u65b9\u6cd5\u53ef\u80fd\u964d\u4f4e\u504f\u5dee\u4f46\u4e0d\u4fdd\u8bc1\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u6570\u636e\u96c6\u7f29\u51cf\u65b9\u6cd5\u662f\u5426\u53ca\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u5b66\u4e60\u4e2d\u7684\u504f\u5dee\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u9ad8\u6548\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u504f\u5dee\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5341\u4e2a\u4e0d\u540c\u7684\u865a\u5047\u76f8\u5173\u6027\u57fa\u51c6\u3001\u4e94\u79cd\u8bc4\u5206\u6307\u6807\u548c\u4e94\u79cd\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u5206\u6790\u6838\u5fc3\u96c6\u9009\u62e9\u5bf9\u504f\u5dee\u548c\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u57fa\u4e8e\u5d4c\u5165\u7684\u6837\u672c\u8bc4\u5206\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u5b66\u4e60\u52a8\u6001\u7684\u65b9\u6cd5\u66f4\u4e0d\u6613\u52a0\u5267\u504f\u5dee\uff0c\u4e14\u67d0\u4e9b\u65b9\u6cd5\u867d\u80fd\u964d\u4f4e\u504f\u5dee\u4f46\u4e0d\u4fdd\u8bc1\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\u9700\u8c28\u614e\u8bbe\u8ba1\uff0c\u4ee5\u907f\u514d\u504f\u5dee\u653e\u5927\u5e76\u786e\u4fdd\u6a21\u578b\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.11711", "pdf": "https://arxiv.org/pdf/2507.11711", "abs": "https://arxiv.org/abs/2507.11711", "authors": ["Daniel Moreno-Cartagena", "Guillermo Cabrera-Vives", "Alejandra M. Mu\u00f1oz Arancibia", "Pavlos Protopapas", "Francisco F\u00f6rster", "M\u00e1rcio Catelan", "A. Bayo", "Pablo A. Est\u00e9vez", "P. S\u00e1nchez-S\u00e1ez", "Franz E. Bauer", "M. Pavez-Herrera", "L. Hern\u00e1ndez-Garc\u00eda", "Gonzalo Rojas"], "title": "Image-Based Multi-Survey Classification of Light Curves with a Pre-Trained Vision Transformer", "categories": ["astro-ph.IM", "cs.CV"], "comment": "Accepted at the 2025 Workshop on Machine Learning for Astrophysics at\n  the International Conference on Machine Learning (ICML)", "summary": "We explore the use of Swin Transformer V2, a pre-trained vision Transformer,\nfor photometric classification in a multi-survey setting by leveraging light\ncurves from the Zwicky Transient Facility (ZTF) and the Asteroid\nTerrestrial-impact Last Alert System (ATLAS). We evaluate different strategies\nfor integrating data from these surveys and find that a multi-survey\narchitecture which processes them jointly achieves the best performance. These\nresults highlight the importance of modeling survey-specific characteristics\nand cross-survey interactions, and provide guidance for building scalable\nclassifiers for future time-domain astronomy.", "AI": {"tldr": "Swin Transformer V2\u7528\u4e8e\u591a\u5de1\u5929\u6570\u636e\u7684\u5149\u5ea6\u5206\u7c7b\uff0c\u8054\u5408\u5904\u7406ZTF\u548cATLAS\u6570\u636e\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u89c6\u89c9Transformer\u5728\u591a\u5de1\u5929\u5149\u5ea6\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u4f7f\u7528Swin Transformer V2\uff0c\u8bc4\u4f30\u4e0d\u540c\u6570\u636e\u6574\u5408\u7b56\u7565\uff0c\u8054\u5408\u5904\u7406ZTF\u548cATLAS\u6570\u636e\u3002", "result": "\u591a\u5de1\u5929\u8054\u5408\u67b6\u6784\u8868\u73b0\u6700\u4f73\uff0c\u5f3a\u8c03\u4e86\u5efa\u6a21\u5de1\u5929\u7279\u6027\u548c\u8de8\u5de1\u5929\u4ea4\u4e92\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u4e3a\u672a\u6765\u65f6\u57df\u5929\u6587\u5b66\u7684\u53ef\u6269\u5c55\u5206\u7c7b\u5668\u6784\u5efa\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2507.11821", "pdf": "https://arxiv.org/pdf/2507.11821", "abs": "https://arxiv.org/abs/2507.11821", "authors": ["Pouya Shaeri", "Arash Karimi", "Ariane Middel"], "title": "MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": "Submitted to a computer science conference", "summary": "Neural networks are often benchmarked using standard datasets such as MNIST,\nFashionMNIST, or other variants of MNIST, which, while accessible, are limited\nto generic classes such as digits or clothing items. For researchers working on\ndomain-specific tasks, such as classifying trees, food items, or other\nreal-world objects, these data sets are insufficient and irrelevant.\nAdditionally, creating and publishing a custom dataset can be time consuming,\nlegally constrained, or beyond the scope of individual projects. We present\nMNIST-Gen, an automated, modular, and adaptive framework for generating\nMNIST-style image datasets tailored to user-specified categories using\nhierarchical semantic categorization. The system combines CLIP-based semantic\nunderstanding with reinforcement learning and human feedback to achieve\nintelligent categorization with minimal manual intervention. Our hierarchical\napproach supports complex category structures with semantic characteristics,\nenabling fine-grained subcategorization and multiple processing modes:\nindividual review for maximum control, smart batch processing for large\ndatasets, and fast batch processing for rapid creation. Inspired by category\ntheory, MNIST-Gen models each data transformation stage as a composable\nmorphism, enhancing clarity, modularity, and extensibility. As proof of\nconcept, we generate and benchmark two novel datasets-\\textit{Tree-MNIST} and\n\\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing\ntask-specific evaluation data while achieving 85\\% automatic categorization\naccuracy and 80\\% time savings compared to manual approaches.", "AI": {"tldr": "MNIST-Gen\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u3001\u6a21\u5757\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u7684MNIST\u98ce\u683c\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6570\u636e\u96c6\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edfMNIST\u6570\u636e\u96c6\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\u4e0d\u9002\u7528\uff0c\u4e14\u521b\u5efa\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8017\u65f6\u4e14\u6cd5\u5f8b\u53d7\u9650\u3002", "method": "\u7ed3\u5408CLIP\u8bed\u4e49\u7406\u89e3\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u4eba\u5de5\u53cd\u9988\uff0c\u91c7\u7528\u5c42\u6b21\u5316\u8bed\u4e49\u5206\u7c7b\u751f\u6210\u6570\u636e\u96c6\u3002", "result": "\u751f\u6210Tree-MNIST\u548cFood-MNIST\u6570\u636e\u96c6\uff0c\u81ea\u52a8\u5206\u7c7b\u51c6\u786e\u7387\u8fbe85%\uff0c\u8282\u770180%\u65f6\u95f4\u3002", "conclusion": "MNIST-Gen\u4e3a\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u96c6\u751f\u6210\u5de5\u5177\u3002"}}
{"id": "2507.11852", "pdf": "https://arxiv.org/pdf/2507.11852", "abs": "https://arxiv.org/abs/2507.11852", "authors": ["Mohammed Hassanin", "Mohammad Abu Alsheikh", "Carlos C. N. Kuhn", "Damith Herath", "Dinh Thai Hoang", "Ibrahim Radwan"], "title": "Towards Autonomous Riding: A Review of Perception, Planning, and Control in Intelligent Two-Wheelers", "categories": ["cs.RO", "cs.CV", "93C85", "F.2.2; I.2.7"], "comment": "17 pages", "summary": "The rapid adoption of micromobility solutions, particularly two-wheeled\nvehicles like e-scooters and e-bikes, has created an urgent need for reliable\nautonomous riding (AR) technologies. While autonomous driving (AD) systems have\nmatured significantly, AR presents unique challenges due to the inherent\ninstability of two-wheeled platforms, limited size, limited power, and\nunpredictable environments, which pose very serious concerns about road users'\nsafety. This review provides a comprehensive analysis of AR systems by\nsystematically examining their core components, perception, planning, and\ncontrol, through the lens of AD technologies. We identify critical gaps in\ncurrent AR research, including a lack of comprehensive perception systems for\nvarious AR tasks, limited industry and government support for such\ndevelopments, and insufficient attention from the research community. The\nreview analyses the gaps of AR from the perspective of AD to highlight\npromising research directions, such as multimodal sensor techniques for\nlightweight platforms and edge deep learning architectures. By synthesising\ninsights from AD research with the specific requirements of AR, this review\naims to accelerate the development of safe, efficient, and scalable autonomous\nriding systems for future urban mobility.", "AI": {"tldr": "\u7efc\u8ff0\u5206\u6790\u4e86\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u5982\u4f55\u5e94\u7528\u4e8e\u4e24\u8f6e\u5fae\u79fb\u52a8\u5de5\u5177\u7684\u81ea\u4e3b\u9a91\u884c\uff08AR\uff09\u7cfb\u7edf\uff0c\u6307\u51fa\u5f53\u524d\u7814\u7a76\u7684\u4e0d\u8db3\u5e76\u63d0\u51fa\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4e24\u8f6e\u5fae\u79fb\u52a8\u5de5\u5177\u7684\u5feb\u901f\u666e\u53ca\u9700\u8981\u53ef\u9760\u7684AR\u6280\u672f\uff0c\u4f46\u5176\u72ec\u7279\u6311\u6218\uff08\u5982\u4e0d\u7a33\u5b9a\u6027\u548c\u73af\u5883\u4e0d\u53ef\u9884\u6d4b\u6027\uff09\u5c1a\u672a\u5f97\u5230\u5145\u5206\u89e3\u51b3\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790AR\u7684\u6838\u5fc3\u7ec4\u4ef6\uff08\u611f\u77e5\u3001\u89c4\u5212\u3001\u63a7\u5236\uff09\uff0c\u501f\u9274\u81ea\u52a8\u9a7e\u9a76\uff08AD\uff09\u6280\u672f\uff0c\u8bc6\u522b\u7814\u7a76\u7f3a\u53e3\u3002", "result": "\u53d1\u73b0\u5f53\u524dAR\u7814\u7a76\u5728\u611f\u77e5\u7cfb\u7edf\u3001\u884c\u4e1a\u652f\u6301\u53ca\u7814\u7a76\u5173\u6ce8\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u591a\u6a21\u6001\u4f20\u611f\u5668\u548c\u8fb9\u7f18\u6df1\u5ea6\u5b66\u4e60\u7b49\u65b9\u5411\u3002", "conclusion": "\u7ed3\u5408AD\u6280\u672f\u4e0eAR\u9700\u6c42\uff0c\u6709\u671b\u52a0\u901f\u5f00\u53d1\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684AR\u7cfb\u7edf\uff0c\u63a8\u52a8\u672a\u6765\u57ce\u5e02\u51fa\u884c\u3002"}}
{"id": "2507.11853", "pdf": "https://arxiv.org/pdf/2507.11853", "abs": "https://arxiv.org/abs/2507.11853", "authors": ["J. Senthilnath", "Jayasanker Jayabalan", "Zhuoyi Lin", "Aye Phyu Phyu Aung", "Chen Hao", "Kaixin Xu", "Yeow Kheng Lim", "F. C. Wellstood"], "title": "A Spatial-Physics Informed Model for 3D Spiral Sample Scanned by SQUID Microscopy", "categories": ["physics.ins-det", "cs.CV"], "comment": "copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "The development of advanced packaging is essential in the semiconductor\nmanufacturing industry. However, non-destructive testing (NDT) of advanced\npackaging becomes increasingly challenging due to the depth and complexity of\nthe layers involved. In such a scenario, Magnetic field imaging (MFI) enables\nthe imaging of magnetic fields generated by currents. For MFI to be effective\nin NDT, the magnetic fields must be converted into current density. This\nconversion has typically relied solely on a Fast Fourier Transform (FFT) for\nmagnetic field inversion; however, the existing approach does not consider eddy\ncurrent effects or image misalignment in the test setup. In this paper, we\npresent a spatial-physics informed model (SPIM) designed for a 3D spiral sample\nscanned using Superconducting QUantum Interference Device (SQUID) microscopy.\nThe SPIM encompasses three key components: i) magnetic image enhancement by\naligning all the \"sharp\" wire field signals to mitigate the eddy current effect\nusing both in-phase (I-channel) and quadrature-phase (Q-channel) images; (ii)\nmagnetic image alignment that addresses skew effects caused by any misalignment\nof the scanning SQUID microscope relative to the wire segments; and (iii) an\ninversion method for converting magnetic fields to magnetic currents by\nintegrating the Biot-Savart Law with FFT. The results show that the SPIM\nimproves I-channel sharpness by 0.3% and reduces Q-channel sharpness by 25%.\nAlso, we were able to remove rotational and skew misalignments of 0.30 in a\nreal image. Overall, SPIM highlights the potential of combining spatial\nanalysis with physics-driven models in practical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u7269\u7406\u4fe1\u606f\u6a21\u578b\uff08SPIM\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u534a\u5bfc\u4f53\u5c01\u88c5\u4e2d\u7684\u975e\u7834\u574f\u6027\u6d4b\u8bd5\uff08NDT\uff09\uff0c\u901a\u8fc7\u6574\u5408\u78c1\u56fe\u50cf\u589e\u5f3a\u3001\u5bf9\u9f50\u548c\u53cd\u6f14\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u534a\u5bfc\u4f53\u5c01\u88c5\u5c42\u6df1\u5ea6\u548c\u590d\u6742\u6027\u589e\u52a0\uff0c\u4f20\u7edf\u7684\u78c1\u6210\u50cf\u65b9\u6cd5\uff08\u5982FFT\uff09\u672a\u8003\u8651\u6da1\u6d41\u6548\u5e94\u548c\u56fe\u50cf\u9519\u4f4d\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "SPIM\u5305\u542b\u4e09\u4e2a\u5173\u952e\u90e8\u5206\uff1a\u78c1\u56fe\u50cf\u589e\u5f3a\uff08\u5229\u7528I/Q\u901a\u9053\u51cf\u5c11\u6da1\u6d41\u6548\u5e94\uff09\u3001\u78c1\u56fe\u50cf\u5bf9\u9f50\uff08\u6821\u6b63\u626b\u63cf\u663e\u5fae\u955c\u7684\u9519\u4f4d\uff09\u4ee5\u53ca\u7ed3\u5408\u6bd5\u5965-\u8428\u4f10\u5c14\u5b9a\u5f8b\u548cFFT\u7684\u78c1\u573a\u53cd\u6f14\u65b9\u6cd5\u3002", "result": "SPIM\u4f7fI\u901a\u9053\u6e05\u6670\u5ea6\u63d0\u53470.3%\uff0cQ\u901a\u9053\u6e05\u6670\u5ea6\u964d\u4f4e25%\uff0c\u5e76\u6210\u529f\u6821\u6b63\u4e860.30\u7684\u65cb\u8f6c\u548c\u9519\u4f4d\u3002", "conclusion": "SPIM\u5c55\u793a\u4e86\u7a7a\u95f4\u5206\u6790\u4e0e\u7269\u7406\u9a71\u52a8\u6a21\u578b\u7ed3\u5408\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.11938", "pdf": "https://arxiv.org/pdf/2507.11938", "abs": "https://arxiv.org/abs/2507.11938", "authors": ["Hao Chen", "Takuya Kiyokawa", "Zhengtao Hu", "Weiwei Wan", "Kensuke Harada"], "title": "A Multi-Level Similarity Approach for Single-View Object Grasping: Matching, Planning, and Fine-Tuning", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by IEEE T-RO", "summary": "Grasping unknown objects from a single view has remained a challenging topic\nin robotics due to the uncertainty of partial observation. Recent advances in\nlarge-scale models have led to benchmark solutions such as GraspNet-1Billion.\nHowever, such learning-based approaches still face a critical limitation in\nperformance robustness for their sensitivity to sensing noise and environmental\nchanges. To address this bottleneck in achieving highly generalized grasping,\nwe abandon the traditional learning framework and introduce a new perspective:\nsimilarity matching, where similar known objects are utilized to guide the\ngrasping of unknown target objects. We newly propose a method that robustly\nachieves unknown-object grasping from a single viewpoint through three key\nsteps: 1) Leverage the visual features of the observed object to perform\nsimilarity matching with an existing database containing various object models,\nidentifying potential candidates with high similarity; 2) Use the candidate\nmodels with pre-existing grasping knowledge to plan imitative grasps for the\nunknown target object; 3) Optimize the grasp quality through a local\nfine-tuning process. To address the uncertainty caused by partial and noisy\nobservation, we propose a multi-level similarity matching framework that\nintegrates semantic, geometric, and dimensional features for comprehensive\nevaluation. Especially, we introduce a novel point cloud geometric descriptor,\nthe C-FPFH descriptor, which facilitates accurate similarity assessment between\npartial point clouds of observed objects and complete point clouds of database\nmodels. In addition, we incorporate the use of large language models, introduce\nthe semi-oriented bounding box, and develop a novel point cloud registration\napproach based on plane detection to enhance matching accuracy under\nsingle-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u5339\u914d\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5df2\u77e5\u7269\u4f53\u7684\u76f8\u4f3c\u6027\u6765\u6307\u5bfc\u672a\u77e5\u7269\u4f53\u7684\u6293\u53d6\uff0c\u89e3\u51b3\u4e86\u5355\u89c6\u89d2\u4e0b\u6293\u53d6\u672a\u77e5\u7269\u4f53\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u5b66\u4e60\u6846\u67b6\u5bf9\u611f\u77e5\u566a\u58f0\u548c\u73af\u5883\u53d8\u5316\u654f\u611f\uff0c\u6027\u80fd\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u6293\u53d6\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u89c6\u89c9\u7279\u5f81\u5339\u914d\u6570\u636e\u5e93\u4e2d\u7684\u5df2\u77e5\u7269\u4f53\uff0c\u5229\u7528\u5019\u9009\u7269\u4f53\u7684\u6293\u53d6\u77e5\u8bc6\u89c4\u5212\u6a21\u4eff\u6293\u53d6\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u5fae\u8c03\u4f18\u5316\u6293\u53d6\u8d28\u91cf\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u76f8\u4f3c\u6027\u5339\u914d\u6846\u67b6\u548cC-FPFH\u63cf\u8ff0\u7b26\uff0c\u7ed3\u5408\u8bed\u4e49\u3001\u51e0\u4f55\u548c\u7ef4\u5ea6\u7279\u5f81\uff0c\u63d0\u9ad8\u4e86\u5339\u914d\u51c6\u786e\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u5355\u89c6\u89d2\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u5bf9\u672a\u77e5\u7269\u4f53\u7684\u9c81\u68d2\u6293\u53d6\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u5b66\u4e60\u6846\u67b6\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.11943", "pdf": "https://arxiv.org/pdf/2507.11943", "abs": "https://arxiv.org/abs/2507.11943", "authors": ["Haiwei Lin", "Shoko Imaizumi", "Hitoshi Kiya"], "title": "Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": "3 pages, 3 figures, conference", "summary": "We propose a low-rank adaptation method for training privacy-preserving\nvision transformer (ViT) models that efficiently freezes pre-trained ViT model\nweights. In the proposed method, trainable rank decomposition matrices are\ninjected into each layer of the ViT architecture, and moreover, the patch\nembedding layer is not frozen, unlike in the case of the conventional low-rank\nadaptation methods. The proposed method allows us not only to reduce the number\nof trainable parameters but to also maintain almost the same accuracy as that\nof full-time tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u9690\u79c1\u4fdd\u62a4\u7684\u89c6\u89c9Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u51bb\u7ed3\u9884\u8bad\u7ec3\u6743\u91cd\u5e76\u6ce8\u5165\u53ef\u8bad\u7ec3\u7684\u4f4e\u79e9\u5206\u89e3\u77e9\u9635\uff0c\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5168\u65f6\u8c03\u4f18\u76f8\u8fd1\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\u5728\u51bb\u7ed3\u9884\u8bad\u7ec3\u6743\u91cd\u65f6\u901a\u5e38\u4e5f\u4f1a\u51bb\u7ed3\u8865\u4e01\u5d4c\u5165\u5c42\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u7075\u6d3b\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u63d0\u5347\u9690\u79c1\u4fdd\u62a4ViT\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u5728ViT\u67b6\u6784\u7684\u6bcf\u4e00\u5c42\u6ce8\u5165\u53ef\u8bad\u7ec3\u7684\u4f4e\u79e9\u5206\u89e3\u77e9\u9635\uff0c\u540c\u65f6\u4e0d\u51bb\u7ed3\u8865\u4e01\u5d4c\u5165\u5c42\uff0c\u4ee5\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u5e76\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5168\u65f6\u8c03\u4f18\u76f8\u8fd1\u7684\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4ViT\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6027\u80fd\u4f18\u52bf\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2507.11949", "pdf": "https://arxiv.org/pdf/2507.11949", "abs": "https://arxiv.org/abs/2507.11949", "authors": ["Shuyang Xu", "Zhiyang Dou", "Mingyi Shi", "Liang Pan", "Leo Ho", "Jingbo Wang", "Yuan Liu", "Cheng Lin", "Yuexin Ma", "Wenping Wang", "Taku Komura"], "title": "MOSPA: Human Motion Generation Driven by Spatial Audio", "categories": ["cs.GR", "cs.CV", "cs.RO"], "comment": null, "summary": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u97f3\u9891\u9a71\u52a8\u7684\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\uff08MOSPA\uff09\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u7efc\u5408\u6027\u7684\u7a7a\u95f4\u97f3\u9891\u9a71\u52a8\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u96c6\uff08SAM\uff09\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u5728\u7a7a\u95f4\u97f3\u9891\u5bf9\u4eba\u4f53\u8fd0\u52a8\u5f71\u54cd\u5efa\u6a21\u4e0a\u7684\u7a7a\u767d\u3002", "motivation": "\u865a\u62df\u4eba\u7c7b\u5982\u4f55\u52a8\u6001\u4e14\u771f\u5b9e\u5730\u54cd\u5e94\u591a\u6837\u5316\u7684\u542c\u89c9\u523a\u6fc0\u662f\u89d2\u8272\u52a8\u753b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4f46\u76ee\u524d\u76f8\u5173\u7814\u7a76\u8f83\u5c11\uff0c\u5c24\u5176\u662f\u7a7a\u95f4\u97f3\u9891\u4fe1\u53f7\u5bf9\u4eba\u4f53\u8fd0\u52a8\u7684\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u7b80\u5355\u4f46\u6709\u6548\u7684\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6846\u67b6\uff08MOSPA\uff09\uff0c\u901a\u8fc7\u6709\u6548\u7684\u878d\u5408\u673a\u5236\u6355\u6349\u8eab\u4f53\u8fd0\u52a8\u4e0e\u7a7a\u95f4\u97f3\u9891\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u53d1\u5e03\u4e86SAM\u6570\u636e\u96c6\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "MOSPA\u80fd\u591f\u6839\u636e\u4e0d\u540c\u7684\u7a7a\u95f4\u97f3\u9891\u8f93\u5165\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u4eba\u4f53\u8fd0\u52a8\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8be5\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u586b\u8865\u4e86\u7a7a\u95f4\u97f3\u9891\u9a71\u52a8\u4eba\u4f53\u8fd0\u52a8\u5efa\u6a21\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u5c06\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2507.11971", "pdf": "https://arxiv.org/pdf/2507.11971", "abs": "https://arxiv.org/abs/2507.11971", "authors": ["Tielong Wang", "Yuxuan Xiong", "Jinfan Liu", "Zhifan Zhang", "Ye Chen", "Yue Shi", "Bingbing Ni"], "title": "HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Current 3D representations like meshes, voxels, point clouds, and NeRF-based\nneural implicit fields exhibit significant limitations: they are often\ntask-specific, lacking universal applicability across reconstruction,\ngeneration, editing, and driving. While meshes offer high precision, their\ndense vertex data complicates editing; NeRFs deliver excellent rendering but\nsuffer from structural ambiguity, hindering animation and manipulation; all\nrepresentations inherently struggle with the trade-off between data complexity\nand fidelity. To overcome these issues, we introduce a novel 3D Hierarchical\nProxy Node representation. Its core innovation lies in representing an object's\nshape and texture via a sparse set of hierarchically organized\n(tree-structured) proxy nodes distributed on its surface and interior. Each\nnode stores local shape and texture information (implicitly encoded by a small\nMLP) within its neighborhood. Querying any 3D coordinate's properties involves\nefficient neural interpolation and lightweight decoding from relevant nearby\nand parent nodes. This framework yields a highly compact representation where\nnodes align with local semantics, enabling direct drag-and-edit manipulation,\nand offers scalable quality-complexity control. Extensive experiments across 3D\nreconstruction and editing demonstrate our method's expressive efficiency,\nhigh-fidelity rendering quality, and superior editability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u76843D\u5206\u5c42\u4ee3\u7406\u8282\u70b9\u8868\u793a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u8868\u793a\u65b9\u6cd5\u5728\u901a\u7528\u6027\u3001\u7f16\u8f91\u6027\u548c\u590d\u6742\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u8868\u793a\u65b9\u6cd5\uff08\u5982\u7f51\u683c\u3001\u4f53\u7d20\u3001\u70b9\u4e91\u548cNeRF\uff09\u5728\u901a\u7528\u6027\u3001\u7f16\u8f91\u6027\u548c\u590d\u6742\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u8de8\u4efb\u52a1\uff08\u91cd\u5efa\u3001\u751f\u6210\u3001\u7f16\u8f91\u548c\u9a71\u52a8\uff09\u7684\u666e\u9002\u6027\u3002", "method": "\u901a\u8fc7\u7a00\u758f\u7684\u5206\u5c42\u7ec4\u7ec7\uff08\u6811\u72b6\u7ed3\u6784\uff09\u4ee3\u7406\u8282\u70b9\u8868\u793a\u7269\u4f53\u7684\u5f62\u72b6\u548c\u7eb9\u7406\uff0c\u6bcf\u4e2a\u8282\u70b9\u5b58\u50a8\u5c40\u90e8\u5f62\u72b6\u548c\u7eb9\u7406\u4fe1\u606f\uff08\u7531\u5c0f\u578bMLP\u9690\u5f0f\u7f16\u7801\uff09\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u63d2\u503c\u548c\u8f7b\u91cf\u89e3\u7801\u5b9e\u73b0\u9ad8\u6548\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57283D\u91cd\u5efa\u548c\u7f16\u8f91\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u7684\u8868\u8fbe\u80fd\u529b\u3001\u9ad8\u4fdd\u771f\u6e32\u67d3\u8d28\u91cf\u548c\u5353\u8d8a\u7684\u53ef\u7f16\u8f91\u6027\u3002", "conclusion": "\u63d0\u51fa\u76843D\u5206\u5c42\u4ee3\u7406\u8282\u70b9\u8868\u793a\u65b9\u6cd5\u5728\u901a\u7528\u6027\u3001\u7f16\u8f91\u6027\u548c\u590d\u6742\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u663e\u8457\u5e73\u8861\uff0c\u4e3a3D\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12042", "pdf": "https://arxiv.org/pdf/2507.12042", "abs": "https://arxiv.org/abs/2507.12042", "authors": ["Kazuki Shimada", "Archontis Politis", "Iran R. Roman", "Parthasaarathy Sudarsanam", "David Diaz-Guerra", "Ruchi Pandey", "Kengo Uchida", "Yuichiro Koyama", "Naoya Takahashi", "Takashi Shibuya", "Shusuke Takahashi", "Tuomas Virtanen", "Yuki Mitsufuji"], "title": "Stereo Sound Event Localization and Detection with Onscreen/offscreen Classification", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS", "eess.IV"], "comment": "5 pages, 2 figures", "summary": "This paper presents the objective, dataset, baseline, and metrics of Task 3\nof the DCASE2025 Challenge on sound event localization and detection (SELD). In\nprevious editions, the challenge used four-channel audio formats of first-order\nAmbisonics (FOA) and microphone array. In contrast, this year's challenge\ninvestigates SELD with stereo audio data (termed stereo SELD). This change\nshifts the focus from more specialized 360{\\deg} audio and audiovisual scene\nanalysis to more commonplace audio and media scenarios with limited\nfield-of-view (FOV). Due to inherent angular ambiguities in stereo audio data,\nthe task focuses on direction-of-arrival (DOA) estimation in the azimuth plane\n(left-right axis) along with distance estimation. The challenge remains divided\ninto two tracks: audio-only and audiovisual, with the audiovisual track\nintroducing a new sub-task of onscreen/offscreen event classification\nnecessitated by the limited FOV. This challenge introduces the DCASE2025 Task3\nStereo SELD Dataset, whose stereo audio and perspective video clips are sampled\nand converted from the STARSS23 recordings. The baseline system is designed to\nprocess stereo audio and corresponding video frames as inputs. In addition to\nthe typical SELD event classification and localization, it integrates\nonscreen/offscreen classification for the audiovisual track. The evaluation\nmetrics have been modified to introduce an onscreen/offscreen accuracy metric,\nwhich assesses the models' ability to identify which sound sources are\nonscreen. In the experimental evaluation, the baseline system performs\nreasonably well with the stereo audio data.", "AI": {"tldr": "DCASE2025\u6311\u6218\u8d5b\u4efb\u52a13\u805a\u7126\u4e8e\u7acb\u4f53\u58f0\u6570\u636e\u7684\u58f0\u97f3\u4e8b\u4ef6\u5b9a\u4f4d\u4e0e\u68c0\u6d4b\uff08SELD\uff09\uff0c\u5f15\u5165\u65b0\u7684\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5e76\u8c03\u6574\u8bc4\u4f30\u6307\u6807\u4ee5\u9002\u5e94\u6709\u9650\u89c6\u91ce\u573a\u666f\u3002", "motivation": "\u7814\u7a76\u7acb\u4f53\u58f0\u6570\u636e\u5728SELD\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u4f20\u7edf\u591a\u901a\u9053\u97f3\u9891\u5728\u6709\u9650\u89c6\u91ce\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u7acb\u4f53\u58f0\u97f3\u9891\u548c\u89c6\u9891\u6570\u636e\uff0c\u57fa\u7ebf\u7cfb\u7edf\u7ed3\u5408\u4e8b\u4ef6\u5206\u7c7b\u3001\u5b9a\u4f4d\u53ca\u5c4f\u5e55\u5185\u5916\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u57fa\u7ebf\u7cfb\u7edf\u5728\u7acb\u4f53\u58f0\u97f3\u9891\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7acb\u4f53\u58f0SELD\u4e3a\u6709\u9650\u89c6\u91ce\u573a\u666f\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.12050", "pdf": "https://arxiv.org/pdf/2507.12050", "abs": "https://arxiv.org/abs/2507.12050", "authors": ["Sunpill Kim", "Seunghun Paik", "Chanwoo Hwang", "Dongsoo Kim", "Junbum Shin", "Jae Hong Seo"], "title": "IDFace: Face Template Protection for Efficient and Secure Identification", "categories": ["cs.CR", "cs.CV", "I.5.4; K.6.5; D.4.6; I.4.7"], "comment": "Accepted to ICCV 2025", "summary": "As face recognition systems (FRS) become more widely used, user privacy\nbecomes more important. A key privacy issue in FRS is protecting the user's\nface template, as the characteristics of the user's face image can be recovered\nfrom the template. Although recent advances in cryptographic tools such as\nhomomorphic encryption (HE) have provided opportunities for securing the FRS,\nHE cannot be used directly with FRS in an efficient plug-and-play manner. In\nparticular, although HE is functionally complete for arbitrary programs, it is\nbasically designed for algebraic operations on encrypted data of predetermined\nshape, such as a polynomial ring. Thus, a non-tailored combination of HE and\nthe system can yield very inefficient performance, and many previous HE-based\nface template protection methods are hundreds of times slower than plain\nsystems without protection. In this study, we propose IDFace, a new HE-based\nsecure and efficient face identification method with template protection.\nIDFace is designed on the basis of two novel techniques for efficient searching\non a (homomorphically encrypted) biometric database with an angular metric. The\nfirst technique is a template representation transformation that sharply\nreduces the unit cost for the matching test. The second is a space-efficient\nencoding that reduces wasted space from the encryption algorithm, thus saving\nthe number of operations on encrypted templates. Through experiments, we show\nthat IDFace can identify a face template from among a database of 1M encrypted\ntemplates in 126ms, showing only 2X overhead compared to the identification\nover plaintexts.", "AI": {"tldr": "IDFace\u662f\u4e00\u79cd\u57fa\u4e8e\u540c\u6001\u52a0\u5bc6\u7684\u9ad8\u6548\u5b89\u5168\u4eba\u8138\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u677f\u8868\u793a\u8f6c\u6362\u548c\u7a7a\u95f4\u9ad8\u6548\u7f16\u7801\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a0\u5bc6\u6a21\u677f\u7684\u5339\u914d\u6548\u7387\u3002", "motivation": "\u968f\u7740\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u4f20\u7edf\u540c\u6001\u52a0\u5bc6\u65b9\u6cd5\u5728\u4fdd\u62a4\u4eba\u8138\u6a21\u677f\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faIDFace\u65b9\u6cd5\uff0c\u91c7\u7528\u6a21\u677f\u8868\u793a\u8f6c\u6362\u964d\u4f4e\u5339\u914d\u6d4b\u8bd5\u6210\u672c\uff0c\u5e76\u901a\u8fc7\u7a7a\u95f4\u9ad8\u6548\u7f16\u7801\u51cf\u5c11\u52a0\u5bc6\u7b97\u6cd5\u7684\u7a7a\u95f4\u6d6a\u8d39\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIDFace\u80fd\u5728126\u6beb\u79d2\u5185\u4ece100\u4e07\u4e2a\u52a0\u5bc6\u6a21\u677f\u4e2d\u8bc6\u522b\u51fa\u4eba\u8138\u6a21\u677f\uff0c\u4ec5\u6bd4\u660e\u6587\u8bc6\u522b\u61622\u500d\u3002", "conclusion": "IDFace\u901a\u8fc7\u4f18\u5316\u52a0\u5bc6\u6a21\u677f\u7684\u8868\u793a\u548c\u7f16\u7801\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u9690\u79c1\u4fdd\u62a4\u4eba\u8138\u8bc6\u522b\u3002"}}
{"id": "2507.12132", "pdf": "https://arxiv.org/pdf/2507.12132", "abs": "https://arxiv.org/abs/2507.12132", "authors": ["Navid Hasanzadeh", "Shahrokh Valaee"], "title": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Wi-Fi Channel State Information (CSI) has gained increasing interest for\nremote sensing applications. Recent studies show that Doppler velocity\nprojections extracted from CSI can enable human activity recognition (HAR) that\nis robust to environmental changes and generalizes to new users. However,\ndespite these advances, generalizability still remains insufficient for\npractical deployment. Inspired by neural radiance fields (NeRF), which learn a\nvolumetric representation of a 3D scene from 2D images, this work proposes a\nnovel approach to reconstruct an informative 3D latent motion representation\nfrom one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The\nresulting latent representation is then used to construct a uniform Doppler\nradiance field (DoRF) of the motion, providing a comprehensive view of the\nperformed activity and improving the robustness to environmental variability.\nThe results show that the proposed approach noticeably enhances the\ngeneralization accuracy of Wi-Fi-based HAR, highlighting the strong potential\nof DoRFs for practical sensing applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWi-Fi CSI\u7684\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\u91cd\u5efa3D\u6f5c\u5728\u8fd0\u52a8\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u7684\u591a\u666e\u52d2\u8f90\u5c04\u573a\uff08DoRF\uff09\u63d0\u5347\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1Wi-Fi CSI\u5728\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\u65b9\u9762\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u4ecd\u4e0d\u8db3\u4ee5\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u53d7NeRF\u542f\u53d1\uff0c\u65e8\u5728\u901a\u8fc73D\u6f5c\u5728\u8fd0\u52a8\u8868\u793a\u63d0\u5347\u6cdb\u5316\u6027\u3002", "method": "\u4eceWi-Fi CSI\u4e2d\u63d0\u53d6\u4e00\u7ef4\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\uff0c\u91cd\u5efa3D\u6f5c\u5728\u8fd0\u52a8\u8868\u793a\uff0c\u5e76\u6784\u5efa\u7edf\u4e00\u7684\u591a\u666e\u52d2\u8f90\u5c04\u573a\uff08DoRF\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86Wi-Fi HAR\u7684\u6cdb\u5316\u51c6\u786e\u6027\u3002", "conclusion": "DoRF\u5728\u5b9e\u7528\u4f20\u611f\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.12145", "pdf": "https://arxiv.org/pdf/2507.12145", "abs": "https://arxiv.org/abs/2507.12145", "authors": ["Muhammad Azlan Qazi", "Alexandros Iosifidis", "Qi Zhang"], "title": "PRISM: Distributed Inference for Foundation Models at Edge", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.", "AI": {"tldr": "PRISM\u662f\u4e00\u79cd\u901a\u4fe1\u9ad8\u6548\u4e14\u8ba1\u7b97\u611f\u77e5\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5206\u5e03\u5f0f\u63a8\u7406Transformer\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u8fb9\u7f18\u90e8\u7f72\u9762\u4e34\u901a\u4fe1\u548c\u8ba1\u7b97\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7b56\u7565\u3002", "method": "\u91c7\u7528Segment Means\u8fd1\u4f3c\u4e2d\u95f4\u7279\u5f81\uff0c\u91cd\u6784\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u8bbe\u8ba1\u5206\u533a\u611f\u77e5\u7684\u56e0\u679c\u63a9\u7801\u3002", "result": "\u5728ViT\u3001BERT\u548cGPT-2\u4e0a\u9a8c\u8bc1\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c1199.2%\uff0c\u8ba1\u7b97\u51cf\u5c1151.24%\uff0c\u7cbe\u5ea6\u635f\u5931\u5c0f\u3002", "conclusion": "PRISM\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12297", "pdf": "https://arxiv.org/pdf/2507.12297", "abs": "https://arxiv.org/abs/2507.12297", "authors": ["Yuan-Chen Shu", "Zhiwei Lin", "Yongtao Wang"], "title": "RegCL: Continual Adaptation of Segment Anything Model via Model Merging", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "To address the performance limitations of the Segment Anything Model (SAM) in\nspecific domains, existing works primarily adopt adapter-based one-step\nadaptation paradigms. However, some of these methods are specific developed for\nspecific domains. If used on other domains may lead to performance degradation.\nThis issue of catastrophic forgetting severely limits the model's scalability.\nTo address this issue, this paper proposes RegCL, a novel non-replay continual\nlearning (CL) framework designed for efficient multi-domain knowledge\nintegration through model merging. Specifically, RegCL incorporates the model\nmerging algorithm into the continual learning paradigm by merging the\nparameters of SAM's adaptation modules (e.g., LoRA modules) trained on\ndifferent domains. The merging process is guided by weight optimization, which\nminimizes prediction discrepancies between the merged model and each of the\ndomain-specific models. RegCL effectively consolidates multi-domain knowledge\nwhile maintaining parameter efficiency, i.e., the model size remains constant\nregardless of the number of tasks, and no historical data storage is required.\nExperimental results demonstrate that RegCL achieves favorable continual\nlearning performance across multiple downstream datasets, validating its\neffectiveness in dynamic scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRegCL\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u89e3\u51b3SAM\u5728\u7279\u5b9a\u9886\u57df\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5b9e\u73b0\u591a\u9886\u57df\u77e5\u8bc6\u9ad8\u6548\u6574\u5408\u3002", "motivation": "\u73b0\u6709\u9002\u914d\u5668\u65b9\u6cd5\u5728\u8de8\u9886\u57df\u5e94\u7528\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u707e\u96be\u6027\u9057\u5fd8\u9650\u5236\u4e86\u6a21\u578b\u6269\u5c55\u6027\u3002", "method": "RegCL\u901a\u8fc7\u5408\u5e76\u4e0d\u540c\u9886\u57df\u7684LoRA\u6a21\u5757\u53c2\u6570\uff0c\u5229\u7528\u6743\u91cd\u4f18\u5316\u6700\u5c0f\u5316\u9884\u6d4b\u5dee\u5f02\uff0c\u5b9e\u73b0\u591a\u9886\u57df\u77e5\u8bc6\u6574\u5408\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRegCL\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u53c2\u6570\u9ad8\u6548\u4e14\u65e0\u9700\u5386\u53f2\u6570\u636e\u5b58\u50a8\u3002", "conclusion": "RegCL\u662f\u4e00\u79cd\u6709\u6548\u7684\u975e\u91cd\u653e\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u573a\u666f\u3002"}}
{"id": "2507.12305", "pdf": "https://arxiv.org/pdf/2507.12305", "abs": "https://arxiv.org/abs/2507.12305", "authors": ["M. Anwar Ma'sum", "Mahardhika Pratama", "Savitha Ramasamy", "Lin Liu", "Habibullah Habibullah", "Ryszard Kowalczyk"], "title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "ICCV 2025", "summary": "The data privacy constraint in online continual learning (OCL), where the\ndata can be seen only once, complicates the catastrophic forgetting problem in\nstreaming data. A common approach applied by the current SOTAs in OCL is with\nthe use of memory saving exemplars or features from previous classes to be\nreplayed in the current task. On the other hand, the prompt-based approach\nperforms excellently in continual learning but with the cost of a growing\nnumber of trainable parameters. The first approach may not be applicable in\npractice due to data openness policy, while the second approach has the issue\nof throughput associated with the streaming data. In this study, we propose a\nnovel prompt-based method for online continual learning that includes 4 main\ncomponents: (1) single light-weight prompt generator as a general knowledge,\n(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model\n(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our\nproposed method achieves significantly higher performance than the current\nSOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity\nanalysis shows that our method requires a relatively smaller number of\nparameters and achieves moderate training time, inference time, and throughput.\nFor further study, the source code of our method is available at\nhttps://github.com/anwarmaxsum/PROL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u548c\u53c2\u6570\u589e\u957f\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u4e2d\u6570\u636e\u9690\u79c1\u7ea6\u675f\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u5f00\u653e\u653f\u7b56\u548c\u541e\u5410\u91cf\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u5305\u62ec\u8f7b\u91cf\u7ea7\u63d0\u793a\u751f\u6210\u5668\u3001\u53ef\u8bad\u7ec3\u7684\u7f29\u653e\u5668\u548c\u79fb\u4f4d\u5668\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u6cdb\u5316\u4fdd\u6301\u548c\u786c\u8f6f\u66f4\u65b0\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53c2\u6570\u8f83\u5c11\u4e14\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u9002\u4e2d\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.12366", "pdf": "https://arxiv.org/pdf/2507.12366", "abs": "https://arxiv.org/abs/2507.12366", "authors": ["Yifei Zhou", "Xuchu Huang", "Chenyu Ni", "Min Zhou", "Zheyu Yan", "Xunzhao Yin", "Cheng Zhuo"], "title": "FactorHD: A Hyperdimensional Computing Model for Multi-Object Multi-Class Representation and Factorization", "categories": ["cs.SC", "cs.AI", "cs.CV"], "comment": "7 pages, 5 figures, 2 tables, to be published in the 62nd DAC (Design\n  Automation Conference) proceedings", "summary": "Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical\nanalysis and reasoning. Hyperdimensional Computing (HDC), a promising\nbrain-inspired computational model, is integral to neuro-symbolic AI. Various\nHDC models have been proposed to represent class-instance and class-class\nrelations, but when representing the more complex class-subclass relation,\nwhere multiple objects associate different levels of classes and subclasses,\nthey face challenges for factorization, a crucial task for neuro-symbolic AI\nsystems. In this article, we propose FactorHD, a novel HDC model capable of\nrepresenting and factorizing the complex class-subclass relation efficiently.\nFactorHD features a symbolic encoding method that embeds an extra memorization\nclause, preserving more information for multiple objects. In addition, it\nemploys an efficient factorization algorithm that selectively eliminates\nredundant classes by identifying the memorization clause of the target class.\nSuch model significantly enhances computing efficiency and accuracy in\nrepresenting and factorizing multiple objects with class-subclass relation,\novercoming limitations of existing HDC models such as \"superposition\ncatastrophe\" and \"the problem of 2\". Evaluations show that FactorHD achieves\napproximately 5667x speedup at a representation size of 10^9 compared to\nexisting HDC models. When integrated with the ResNet-18 neural network,\nFactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.", "AI": {"tldr": "FactorHD\u662f\u4e00\u79cd\u65b0\u578b\u8d85\u7ef4\u8ba1\u7b97\u6a21\u578b\uff0c\u80fd\u9ad8\u6548\u8868\u793a\u548c\u5206\u89e3\u590d\u6742\u7684\u7c7b-\u5b50\u7c7b\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u8d85\u7ef4\u8ba1\u7b97\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u7684\u7c7b-\u5b50\u7c7b\u5173\u7cfb\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5982\u5206\u89e3\u56f0\u96be\u3002", "method": "\u63d0\u51faFactorHD\u6a21\u578b\uff0c\u91c7\u7528\u7b26\u53f7\u7f16\u7801\u65b9\u6cd5\u548c\u9ad8\u6548\u5206\u89e3\u7b97\u6cd5\uff0c\u9009\u62e9\u6027\u6d88\u9664\u5197\u4f59\u7c7b\u3002", "result": "FactorHD\u5728\u8868\u793a\u548c\u5206\u89e3\u7c7b-\u5b50\u7c7b\u5173\u7cfb\u65f6\u6548\u7387\u63d0\u53475667\u500d\uff0c\u4e0eResNet-18\u7ed3\u5408\u5728Cifar-10\u6570\u636e\u96c6\u4e0a\u8fbe\u523092.48%\u7684\u5206\u89e3\u51c6\u786e\u7387\u3002", "conclusion": "FactorHD\u514b\u670d\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12417", "pdf": "https://arxiv.org/pdf/2507.12417", "abs": "https://arxiv.org/abs/2507.12417", "authors": ["Weichen Dai", "Yuxuan Huang", "Li Zhu", "Dongjun Liu", "Yu Zhang", "Qibin Zhao", "Andrzej Cichocki", "Fabio Babiloni", "Ke Li", "Jianyu Qiu", "Gangyong Jia", "Wanzeng Kong", "Qing Wu"], "title": "Spontaneous Spatial Cognition Emerges during Egocentric Video Viewing through Non-invasive BCI", "categories": ["q-bio.NC", "cs.CV", "eess.SP"], "comment": null, "summary": "Humans possess a remarkable capacity for spatial cognition, allowing for\nself-localization even in novel or unfamiliar environments. While hippocampal\nneurons encoding position and orientation are well documented, the large-scale\nneural dynamics supporting spatial representation, particularly during\nnaturalistic, passive experience, remain poorly understood. Here, we\ndemonstrate for the first time that non-invasive brain-computer interfaces\n(BCIs) based on electroencephalography (EEG) can decode spontaneous,\nfine-grained egocentric 6D pose, comprising three-dimensional position and\norientation, during passive viewing of egocentric video. Despite EEG's limited\nspatial resolution and high signal noise, we find that spatially coherent\nvisual input (i.e., continuous and structured motion) reliably evokes decodable\nspatial representations, aligning with participants' subjective sense of\nspatial engagement. Decoding performance further improves when visual input is\npresented at a frame rate of 100 ms per image, suggesting alignment with\nintrinsic neural temporal dynamics. Using gradient-based backpropagation\nthrough a neural decoding model, we identify distinct EEG channels contributing\nto position -- and orientation specific -- components, revealing a distributed\nyet complementary neural encoding scheme. These findings indicate that the\nbrain's spatial systems operate spontaneously and continuously, even under\npassive conditions, challenging traditional distinctions between active and\npassive spatial cognition. Our results offer a non-invasive window into the\nautomatic construction of egocentric spatial maps and advance our understanding\nof how the human mind transforms everyday sensory experience into structured\ninternal representations.", "AI": {"tldr": "EEG-based\u8111\u673a\u63a5\u53e3\u9996\u6b21\u89e3\u7801\u88ab\u52a8\u89c2\u770b\u89c6\u9891\u65f6\u76846D\u81ea\u6211\u5b9a\u4f4d\uff0c\u63ed\u793a\u5927\u8111\u7a7a\u95f4\u7cfb\u7edf\u5728\u88ab\u52a8\u6761\u4ef6\u4e0b\u81ea\u53d1\u8fd0\u4f5c\u3002", "motivation": "\u63a2\u7d22\u5927\u8111\u5728\u81ea\u7136\u88ab\u52a8\u4f53\u9a8c\u4e2d\u5982\u4f55\u652f\u6301\u7a7a\u95f4\u8868\u5f81\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u975e\u4fb5\u5165\u6027\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528EEG\u8111\u673a\u63a5\u53e3\u89e3\u7801\u88ab\u8bd5\u5728\u89c2\u770b\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u65f6\u76846D\u4f4d\u7f6e\u548c\u65b9\u5411\uff0c\u5206\u6790\u89c6\u89c9\u8f93\u5165\u5bf9\u89e3\u7801\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u8fde\u7eed\u7ed3\u6784\u5316\u89c6\u89c9\u8f93\u5165\u53ef\u89e3\u7801\u7a7a\u95f4\u8868\u5f81\uff0c100ms\u5e27\u7387\u63d0\u5347\u89e3\u7801\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u5206\u5e03\u5f0f\u795e\u7ecf\u7f16\u7801\u6a21\u5f0f\u3002", "conclusion": "\u5927\u8111\u7a7a\u95f4\u7cfb\u7edf\u5728\u88ab\u52a8\u6761\u4ef6\u4e0b\u81ea\u53d1\u8fd0\u4f5c\uff0c\u6311\u6218\u4e86\u4e3b\u52a8\u4e0e\u88ab\u52a8\u7a7a\u95f4\u8ba4\u77e5\u7684\u4f20\u7edf\u533a\u5206\u3002"}}
{"id": "2507.12427", "pdf": "https://arxiv.org/pdf/2507.12427", "abs": "https://arxiv.org/abs/2507.12427", "authors": ["Ashkan Shakarami", "Azade Farshad", "Yousef Yeganeh", "Lorenzo Nicole", "Peter Schuffler", "Stefano Ghidoni", "Nassir Navab"], "title": "Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "12 pages, 6 figures", "summary": "We propose UTS, a unit-based tissue segmentation framework for histopathology\nthat classifies each fixed-size 32 * 32 tile, rather than each pixel, as the\nsegmentation unit. This approach reduces annotation effort and improves\ncomputational efficiency without compromising accuracy. To implement this\napproach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits\nthe multi-level feature representation to capture both fine-grained morphology\nand global tissue context. Trained to segment breast tissue into three\ncategories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports\nclinically relevant tasks such as tumor-stroma quantification and surgical\nmargin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it\noutperforms U-Net variants and transformer-based baselines. Code and Dataset\nwill be available at GitHub.", "AI": {"tldr": "UTS\u662f\u4e00\u79cd\u57fa\u4e8e\u5355\u5143\u7684\u7ec4\u7ec7\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u7c7b\u56fa\u5b9a\u5927\u5c0f\u768432*32\u56fe\u5757\u800c\u975e\u5355\u4e2a\u50cf\u7d20\uff0c\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u50cf\u7d20\u7ea7\u5206\u5272\u65b9\u6cd5\u7684\u9ad8\u6807\u6ce8\u6210\u672c\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u591a\u7ea7\u89c6\u89c9\u53d8\u6362\u5668\uff08L-ViT\uff09\uff0c\u5229\u7528\u591a\u7ea7\u7279\u5f81\u8868\u793a\u6355\u6349\u7ec6\u7c92\u5ea6\u5f62\u6001\u548c\u5168\u5c40\u7ec4\u7ec7\u4e0a\u4e0b\u6587\u3002", "result": "\u5728386,371\u4e2a\u56fe\u5757\u4e0a\u8bc4\u4f30\uff0c\u4f18\u4e8eU-Net\u53d8\u4f53\u548c\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "UTS\u5728\u51cf\u5c11\u6807\u6ce8\u548c\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u4efb\u52a1\u5982\u80bf\u7624-\u95f4\u8d28\u5b9a\u91cf\u548c\u624b\u672f\u8fb9\u7f18\u8bc4\u4f30\u3002"}}
{"id": "2507.12440", "pdf": "https://arxiv.org/pdf/2507.12440", "abs": "https://arxiv.org/abs/2507.12440", "authors": ["Ruihan Yang", "Qinxi Yu", "Yecheng Wu", "Rui Yan", "Borui Li", "An-Chieh Cheng", "Xueyan Zou", "Yunhao Fang", "Hongxu Yin", "Sifei Liu", "Song Han", "Yao Lu", "Xiaolong Wang"], "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "More videos can be found on our website:\n  https://rchalyang.github.io/EgoVLA", "summary": "Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Isaac\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA", "AI": {"tldr": "\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\uff0c\u901a\u8fc7\u9006\u8fd0\u52a8\u5b66\u548c\u52a8\u4f5c\u91cd\u5b9a\u5411\u5c06\u4eba\u7c7b\u52a8\u4f5c\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u786c\u4ef6\u6570\u636e\u6536\u96c6\u89c4\u6a21\u53d7\u9650\u7684\u95ee\u9898\uff0c\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u7684\u4e30\u5bcc\u573a\u666f\u548c\u4efb\u52a1\u591a\u6837\u6027\u3002", "method": "\u8bad\u7ec3VLA\u6a21\u578b\u9884\u6d4b\u4eba\u7c7b\u624b\u8155\u548c\u624b\u90e8\u52a8\u4f5c\uff0c\u901a\u8fc7\u9006\u8fd0\u52a8\u5b66\u548c\u52a8\u4f5c\u91cd\u5b9a\u5411\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5e76\u7528\u5c11\u91cf\u673a\u5668\u4eba\u6f14\u793a\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5728Isaac Humanoid Manipulation Benchmark\u4e0a\u8bc4\u4f30\uff0cEgoVLA\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u4eba\u7c7b\u6570\u636e\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u53ef\u6709\u6548\u6269\u5c55\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u89c4\u6a21\uff0c\u63d0\u5347\u4efb\u52a1\u591a\u6837\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
